  AcademicGuard 用户、计费与支付实施方案 (Rev. 2.0)

  1. 业务逻辑核心定义

  1.1 定价策略
   * 基础单价：2元 RMB / 100 单词。
   * 计费精度：以 100 单词为 1 Unit，不足 100 词按 100 词计算（向上取整）。
   * 最低消费（门槛）：50元 RMB。
       * 逻辑：若计算金额 < 50元，则统一按 50元收取。
       * 适用场景：鼓励用户上传完整的 Academic Essay 进行全文逻辑分析，而非碎片化段落。
   * 计费公式：

   1     billable_units = math.ceil(clean_word_count / 100)
   2     calculated_price = billable_units * 2
   3     final_price = max(50, calculated_price)

  1.2 计费字数统计规则 (Cleaner)
   * 文件格式限制：仅支持 .txt (推荐) 和 .docx。不接受 PDF。
   * 排除内容：必须排除 References、Bibliography、Works Cited 及其之后的内容。
   * 用户引导：
       * 在上传区域提示：“建议上传 .txt 纯文本文件以获得最精准的计费识别。系统将自动剔除参考文献部分。”

  1.3 支付鉴权模式
   * 中央集权制：AcademicGuard 后端不负责复杂的支付状态自校验，而是向您的中央平台数据库/API 查询订单状态。
   * 流程：本地生成订单 -> 推送至平台 -> 用户在平台支付 -> 本地轮询/回调确认支付成功。

  ---

  2. 数据库设计 (PostgreSQL)

  虽然用户信息在中央平台，但本地业务数据库（AcademicGuard）需要保留业务快照和关联信息。

    1 -- 用户表 (本地缓存/映射，用于关联任务)
    2 CREATE TABLE users (
    3     id SERIAL PRIMARY KEY,
    4     platform_user_id VARCHAR(64) UNIQUE NOT NULL, -- 中央平台的ID
    5     phone VARCHAR(20),
    6     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    7 );
    8
    9 -- 任务/订单表 (核心表)
   10 CREATE TABLE tasks (
   11     task_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
   12     user_id INTEGER REFERENCES users(id),
   13
   14     -- 文件信息
   15     original_filename VARCHAR(255) NOT NULL,
   16     file_path VARCHAR(512) NOT NULL, -- 原始文件存储路径
   17     file_type VARCHAR(10) NOT NULL,  -- 'txt' or 'docx'
   18
   19     -- 计费信息
   20     raw_word_count INTEGER,          -- 原始字数
   21     clean_word_count INTEGER,        -- 剔除参考文献后的计费字数
   22     billable_price DECIMAL(10, 2),   -- 最终计算价格 (>=50.00)
   23
   24     -- 状态管理
   25     status VARCHAR(20) DEFAULT 'uploaded', -- uploaded, quoted, paying, processing, completed, expired
   26     payment_status VARCHAR(20) DEFAULT 'unpaid', -- unpaid, paid
   27     platform_order_id VARCHAR(64),   -- 关联中央平台的订单号
   28
   29     -- 时间戳
   30     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
   31     paid_at TIMESTAMP,
   32     expires_at TIMESTAMP -- 任务过期时间 (未支付24小时后)
   33 );

  ---

  3. 后端架构设计 (Python/FastAPI)

  为了保证代码整洁和可维护性，采用 Strategy Pattern (策略模式) 或 Interface (接口) 解耦。

  3.1 核心服务接口定义

    1 from abc import ABC, abstractmethod
    2 from pydantic import BaseModel
    3
    4 class OrderQuote(BaseModel):
    5     task_id: str
    6     clean_word_count: int
    7     price: float
    8     is_minimum_charge: bool # 是否触发了最低50元消费
    9
   10 class IPaymentProvider(ABC):
   11     """支付服务接口：对接中央平台"""
   12
   13     @abstractmethod
   14     async def create_platform_order(self, task_id: str, amount: float, user_id: str) -> str:
   15         """在中央平台创建订单，返回平台订单号"""
   16         pass
   17
   18     @abstractmethod
   19     async def check_payment_status(self, platform_order_id: str) -> bool:
   20         """向中央平台查询支付状态"""
   21         pass
   22
   23 class IWordCounter(ABC):
   24     """字数统计策略"""
   25
   26     @abstractmethod
   27     def count_billable_words(self, file_path: str, file_extension: str) -> int:
   28         """读取文件，剔除参考文献，返回计费字数"""
   29         pass

  3.2 字数统计与“清洗”逻辑 (Heuristic Logic)

  针对 .txt 和 .docx 的清洗逻辑：

    1 import re
    2
    3 class AcademicWordCounter(IWordCounter):
    4     # 常见的参考文献分割词
    5     REF_MARKERS = [
    6         r"^\s*references\s*$",
    7         r"^\s*bibliography\s*$",
    8         r"^\s*works cited\s*$",
    9         r"^\s*参考文献\s*$"
   10     ]
   11
   12     def _strip_references(self, text: str) -> str:
   13         lines = text.split('\n')
   14         clean_lines = []
   15         for line in lines:
   16             # 检查是否遇到参考文献标记（忽略大小写）
   17             if any(re.match(marker, line, re.IGNORECASE) for marker in self.REF_MARKERS):
   18                 break # 遇到标记直接截断后续所有内容
   19             clean_lines.append(line)
   20         return "\n".join(clean_lines)
   21
   22     def count_billable_words(self, content: str) -> int:
   23         cleaned_text = self._strip_references(content)
   24         # 简单的英文分词统计，实际可使用 nltk 或 spaCy
   25         words = re.findall(r'\w+', cleaned_text)
   26         return len(words)

  ---

  4. 用户交互流程 (UX Flow)

  阶段一：上传与登陆
   1. 上传：用户在首页上传 .txt 或 .docx。
   2. 创建任务：后端接收文件，生成 task_id，状态为 uploaded。
   3. 触发点：用户点击“Start Analysis”按钮。
   4. 登陆拦截：
       * 前端检查本地 Store 是否有 auth_token。
       * 无 Token -> 弹出/跳转登陆页 -> 用户使用手机号+验证码登陆（对接中央平台）-> 获取 Token -> 返回当前页面。

  阶段二：报价确认 (Quote Confirmation)
   5. 请求报价：前端带 Token 请求 GET /api/tasks/{task_id}/quote。
   6. 后端计算：
       * 调用 AcademicWordCounter 算出 clean_word_count。
       * 应用 max(50, count/100 * 2) 公式。
       * 更新数据库 billable_price。
   7. 前端展示弹窗：
      > Order Confirmation
      >    File*: my_essay.docx
      >    Detected Words*: 3,200 (References excluded)
      >    Rate*: ¥2.00 / 100 words
      >    Total Amount: ¥64.00*
      >
      > [ Cancel ]  [ Proceed to Pay ]

  阶段三：支付与执行
   8. 发起支付：用户点击 "Proceed to Pay"。
   9. 创建订单：后端调用 IPaymentProvider.create_platform_order，获得支付链接或二维码。
   10. 支付中：前端展示支付二维码（iframe 或跳转）。
   11. 状态同步：
       * 前端轮询 GET /api/tasks/{task_id}/status。
       * 后端在接收到轮询时，实时调用 IPaymentProvider.check_payment_status 向中央平台确认。
   12. 支付成功：
       * 中央平台返回 True。
       * 后端更新本地 tasks 表 payment_status = 'paid'。
       * 自动触发：后端 Flow Coordinator 开始调用 LLM 进行 Step 1 分析。
       * 前端收到状态变更，进入“Processing”进度条页面。

  ---

  5. 异常处理与清理策略

   1. 超时未支付：
       * 若任务创建 24小时后仍未变更为 paid，后端定时任务将标记其为 expired。
       * 前端不再允许对 expired 任务发起支付，提示“任务已失效，请重新上传”。
   2. 字数争议：
       * 如果在报价页，用户认为字数识别错误（例如把正文误判为 Reference 截断了）。
       * 提供一个 "Report Issue / Recalculate" 按钮，允许用户强制按“全文（不截断）”重新计费，或者转人工客服。
   3. 文件格式错误：
       * 上传非 txt/docx 文件时，前端直接拦截。
       * 上传损坏的 docx 时，后端报错，提示用户“文件解析失败，请另存为 .txt 后重试”。

  ---

  这个方案的优点
   1. 商业闭环：50元门槛保障了高价值服务的筛选。
   2. 开发效率：利用您已有的中央平台处理最复杂的鉴权和资金流，本地后端只负责业务逻辑。
   3. 纠纷减少：通过“报价确认页”和“推荐TXT”策略，把计费透明化，极大减少支付后的客诉。

强化方案：

本方案的核心改动在于：建立“清洗后文本”的唯一真实性（Single Source of
  Truth），确保“计费的对象”绝对等于“AI分析的对象”，并详细列出了可能导致资金损失的安全漏洞及防御措施。

  ---

  1. 核心架构原则：一致性锁 (Consistency Lock)

  为了绝对避免“计费与分析内容不符”的Bug，我们必须放弃“每次需要时重新读取文件”的做法，改为单向流水线。

  核心逻辑流
   1. Ingestion (摄入)：读取原始文件（.txt/.docx）。
   2. Cleaning (清洗)：一次性执行参考文献截断和清洗。
   3. Freezing (固化)：将清洗后的纯文本内容保存为 processed_content（可以是临时文件或内存对象），并计算其 Hash值（如SHA-256）。
   4. Downstream (下游)：
       * 计费模块：只读取 processed_content 统计字数算钱。
       * AI模块：只读取 processed_content 发送给 LLM。

  绝对禁止：在计费时读一遍原始文件，在分析时又去读一遍原始文件。这极易因为解析库版本更新或偶发读取错误导致两者不一致。

  ---

  2. 详细业务流程设计

  步骤 1：上传与预处理 (Upload & Preprocess)
   * 用户动作：上传文件。
   * 系统动作：
       1. 保存原始文件至 storage/raw/{task_id}.{ext}。
       2. 立即调用 TextCleaner 服务：
           * 提取文本。
           * 截断参考文献（识别 References 等关键词，丢弃后续内容）。
           * 去除不可见字符、乱码。
       3. 将清洗后的文本保存至 storage/clean/{task_id}.txt。
       4. 统计 clean 文件的字数，写入数据库。
       5. 计算并写入 clean 文件的 Content Hash (用于防篡改)。

  步骤 2：报价 (Quote)
   * 触发：用户点击“开始/获取报价”。
   * 逻辑：
       * 读取数据库中已锁定的 clean_word_count。
       * 应用公式：Billable Units = ceil(Count / 100) -> Price = max(50, Units * 2)。
       * 前端显示价格，不提供修改或申诉入口（硬性执行）。

  步骤 3：支付与锁定 (Pay & Lock)
   * 逻辑：
       * 调用中央平台支付接口。
       * 关键点：在发起支付请求时，检查任务状态。如果用户此时试图重新上传文件，必须强制废弃旧任务ID，生成新任务ID，防止“偷梁换柱”。

  步骤 4：执行分析 (Execution)
   * 触发：支付回调成功。
   * 逻辑：
       * Worker 读取 storage/clean/{task_id}.txt。
       * （可选安全校验）：Worker 再次计算该文件的 Hash，与数据库中的 Hash 比对，确保文件在支付后未被篡改。
       * 发送给 LLM 进行处理。

  ---

  3. 安全漏洞与资金止损分析 (Risk & Loss Prevention)

  这是您最需要关注的部分。在按 Token/API 调用的付费模式下，任何 Bug 都直接意味着现金损失。

  3.1 漏洞：偷梁换柱 (The Switcheroo)
   * 场景：用户上传了一个 100 词的文件 A (报价 50元)，获取了报价单。在点击“支付”的瞬间，利用并发脚本或浏览器多标签页，上传了一个 10万词的文件 B
     覆盖了当前任务的文件。支付 50元 后，系统读取了文件 B 进行分析。
   * 损失：收取 50元，消耗 2000元 的 LLM Token。
   * 防御：
       * 文件不可变原则：任务 (Task) 一旦创建并生成报价，对应的文件路径/内容就是只读的。
       * 新上传即新任务：如果用户要换文件，必须生成新的 task_id。旧任务废弃。

  3.2 漏洞：重放攻击 (Replay Attack)
   * 场景：用户支付了一次，然后通过抓包工具，拿到后端“开始分析”的 API 接口 (e.g., POST /api/analyze/start)，疯狂发送请求。
   * 损失：用户付一次钱，系统在后台跑了 10 次 LLM 分析，浪费 10 倍算力。
   * 防御：
       * 状态机幂等性：后端状态流转必须是单向的。
       * 代码逻辑：

   1         # 伪代码
   2         if task.status != 'paid_pending_analysis':
   3             return Error("Task is already processed or not paid")
   4         task.status = 'processing' # 立即加锁
   5         commit_db()
   6         start_analysis()

  3.3 漏洞：API 绕过 (Direct API Access)
   * 场景：黑客不经过支付流程，直接猜到了 task_id，并调用后续的查询进度或结果接口。
   * 防御：
       * 所有业务接口（分析、查询结果）必须严格校验 payment_status == 'paid'。
       * 不要信任前端传来的“支付成功”标记，必须查数据库。

  3.4 漏洞：格式炸弹 (Format Bomb)
   * 场景：用户上传一个特殊构造的 zip/docx 文件（解压后极大）或包含死循环字符的 txt。
   * 损失：服务器内存溢出，或者清洗脚本陷入死循环，卡死服务器导致其他付费用户无法服务。
   * 防御：
       * 上传限制：严格限制文件大小（如 Max 5MB，纯文本 5MB 已经非常大了）。
       * 解析超时：文本清洗函数设置 strict timeout (如 5秒)，超时直接报错任务失败。

  ---

  4. 数据库 Schema 更新 (PostgreSQL)

    1 CREATE TABLE tasks (
    2     task_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    3     user_id INTEGER NOT NULL, -- 关联本地users表
    4
    5     -- 文件与内容 (核心一致性保障)
    6     file_path_raw VARCHAR(512) NOT NULL,   -- 原始上传文件
    7     file_path_clean VARCHAR(512),          -- 清洗后的纯文本 (用于计费和分析的唯一源)
    8     content_hash VARCHAR(64),              -- clean文件的SHA256，防止篡改
    9
   10     -- 计费数据
   11     word_count_raw INTEGER,                -- 仅作记录
   12     word_count_billable INTEGER,           -- 计费字数 (Clean版)
   13     price_total DECIMAL(10, 2),            -- 最终订单金额
   14
   15     -- 状态机
   16     -- created -> quoted -> paying -> paid -> processing -> completed -> expired
   17     status VARCHAR(20) DEFAULT 'created',
   18
   19     -- 外部关联
   20     platform_order_id VARCHAR(64),
   21
   22     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
   23     processed_at TIMESTAMP
   24 );

  5. 开发建议 (Next Steps)

   1. 实现 `TextCleaner` 类：这是整个逻辑的心脏。请务必编写单元测试，测试包含 References, Bibliography 的各种大小写、换行组合，确保截断准确。
   2. 状态机测试：在开发阶段，模拟“支付后立即修改文件”、“未支付直接调用分析接口”等攻击行为，验证系统的防御能力。
   3. 日志记录：记录每一个关键节点的字数。例如：Log: Task 123 - Uploaded 5000 bytes - Cleaned to 450 words - Price
      50.00。一旦出现亏损（Token消耗异常高），可以通过日志排查是清洗逻辑出了问题，还是被攻击了。

  这个方案在保证商业逻辑闭环的同时，最大程度地规避了技术实现上可能出现的“甚至导致破产”级别的漏洞。