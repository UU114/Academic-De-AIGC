"""
============================================
DEPRECATED: Legacy Structure Analysis API
å·²åºŸå¼ƒï¼šæ—§ç‰ˆç»“æ„åˆ†æAPI
============================================

This module uses the OLD API structure and is superseded by:
- New 5-Layer Analysis API: /api/v1/analysis/document/*
- Located at: src/api/routes/analysis/document.py

Frontend should use:
- LayerStep1_1.tsx through LayerStep1_5.tsx
- documentLayerApi from analysisApi.ts

DO NOT DELETE - kept for backward compatibility
è¯·å‹¿åˆ é™¤ - ä¿ç•™ç”¨äºå‘åå…¼å®¹
============================================

Structure Analysis API Routes (Level 1 De-AIGC)
ç»“æ„åˆ†æ API è·¯ç”±ï¼ˆLevel 1 De-AIGCï¼‰

Phase 4: Document structure analysis and restructuring endpoints
ç¬¬4é˜¶æ®µï¼šæ–‡æ¡£ç»“æ„åˆ†æå’Œé‡ç»„ç«¯ç‚¹
"""

from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm.attributes import flag_modified
from typing import Optional
import logging
import re
import httpx

from src.api.schemas import (
    StructureAnalysisRequest,
    StructureAnalysisResponse,
    StructureStrategy,
    StructureOption,
    LogicDiagnosisResponse,
    DocumentStructureRequest,
    ParagraphInfo,
    StructureIssue,
    BreakPoint,
    FlowRelation,
    RiskArea,
    StructureModification,
    StructureChange,
    SmartStructureResponse,
    SmartStructureIssue,
    SectionInfo,
    SmartParagraphInfo,
    ExplicitConnector,
    LogicBreak,
    # Enhanced schemas (Level 1 Enhancement)
    PredictabilityAnalysisRequest,
    PredictabilityAnalysisResponse,
    ProgressionAnalysisResult,
    FunctionDistributionResult,
    ClosureAnalysisResult,
    LexicalEchoResult,
    DisruptionRestructureRequest,
    DisruptionRestructureResponse,
    DisruptionLevel,
    # 7-Indicator Risk Card schemas
    StructuralIndicatorResponse,
    StructuralRiskCardResponse,
    CrossReferenceResult,
    RiskCardRequest,
    # Single paragraph suggestion schemas
    ParagraphSuggestionRequest,
    ParagraphSuggestionResponse,
    # Detailed improvement suggestions schemas
    SectionSuggestion,
    DetailedImprovementSuggestions,
    # Merge modify schemas
    MergeModifyRequest,
    MergeModifyPromptResponse,
    MergeModifyApplyResponse,
)
from src.core.analyzer.structure import StructureAnalyzer
from src.core.analyzer.smart_structure import SmartStructureAnalyzer
from src.prompts.structure import DISRUPTION_LEVELS, DISRUPTION_STRATEGIES
from src.db.database import get_db
from src.db.models import Document, Session

logger = logging.getLogger(__name__)

router = APIRouter()

# Initialize analyzers
# åˆå§‹åŒ–åˆ†æå™¨
structure_analyzer = StructureAnalyzer()
smart_analyzer = SmartStructureAnalyzer()


@router.post("/", response_model=StructureAnalysisResponse)
async def analyze_structure(request: StructureAnalysisRequest):
    """
    Analyze document structure for AI patterns
    åˆ†ææ–‡æ¡£ç»“æ„çš„AIæ¨¡å¼

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureAnalysisResponse with analysis results åŒ…å«åˆ†æç»“æœçš„å“åº”
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Convert paragraph info to API format
        # å°†æ®µè½ä¿¡æ¯è½¬æ¢ä¸ºAPIæ ¼å¼
        paragraphs = [
            ParagraphInfo(
                index=p.index,
                first_sentence=p.first_sentence[:200] if len(p.first_sentence) > 200 else p.first_sentence,
                last_sentence=p.last_sentence[:200] if len(p.last_sentence) > 200 else p.last_sentence,
                word_count=p.word_count,
                sentence_count=p.sentence_count,
                has_topic_sentence=p.has_topic_sentence,
                has_summary_ending=p.has_summary_ending,
                connector_words=p.connector_words,
                function_type=p.function_type
            )
            for p in result.paragraphs
        ]

        # Convert issues
        # è½¬æ¢é—®é¢˜
        issues = [
            StructureIssue(
                type=i.type,
                description=i.description,
                description_zh=i.description_zh,
                severity=i.severity,
                affected_paragraphs=i.affected_paragraphs,
                suggestion=i.suggestion,
                suggestion_zh=i.suggestion_zh
            )
            for i in result.issues
        ]

        # Convert break points
        # è½¬æ¢æ–­ç‚¹
        break_points = [
            BreakPoint(
                position=bp.position,
                type=bp.type,
                description=bp.description,
                description_zh=bp.description_zh
            )
            for bp in result.break_points
        ]

        return StructureAnalysisResponse(
            total_paragraphs=result.total_paragraphs,
            total_sentences=result.total_sentences,
            total_words=result.total_words,
            avg_paragraph_length=result.avg_paragraph_length,
            paragraph_length_variance=result.paragraph_length_variance,
            structure_score=result.structure_score,
            risk_level=result.risk_level,
            paragraphs=paragraphs,
            issues=issues,
            break_points=break_points,
            core_thesis=result.core_thesis,
            key_arguments=result.key_arguments,
            has_linear_flow=result.has_linear_flow,
            has_repetitive_pattern=result.has_repetitive_pattern,
            has_uniform_length=result.has_uniform_length,
            has_predictable_order=result.has_predictable_order,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/with-suggestions", response_model=StructureAnalysisResponse)
async def analyze_structure_with_suggestions(request: StructureAnalysisRequest):
    """
    Analyze document structure and generate restructuring suggestions
    åˆ†ææ–‡æ¡£ç»“æ„å¹¶ç”Ÿæˆé‡ç»„å»ºè®®

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureAnalysisResponse with analysis and suggestions åŒ…å«åˆ†æå’Œå»ºè®®çš„å“åº”
    """
    try:
        # Perform base analysis
        # æ‰§è¡ŒåŸºç¡€åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Generate suggestions based on issues
        # æ ¹æ®é—®é¢˜ç”Ÿæˆå»ºè®®
        options = []

        # Only generate suggestions if there are issues
        # ä»…åœ¨å­˜åœ¨é—®é¢˜æ—¶ç”Ÿæˆå»ºè®®
        if result.issues:
            # Option 1: Optimize Connection (gentle approach)
            # é€‰é¡¹1ï¼šä¼˜åŒ–è¿æ¥ï¼ˆæ¸©å’Œæ–¹æ³•ï¼‰
            optimize_modifications = []
            for p in result.paragraphs:
                if p.has_topic_sentence or p.connector_words:
                    optimize_modifications.append(
                        StructureModification(
                            paragraph_index=p.index,
                            change_type="rewrite_opening",
                            original=p.first_sentence[:100],
                            modified=None,  # Would be filled by LLM
                            explanation_zh="ç§»é™¤æ˜¾å¼è¿æ¥è¯ï¼Œåˆ›å»ºéšå¼é€»è¾‘æµ"
                        )
                    )

            options.append(StructureOption(
                strategy=StructureStrategy.OPTIMIZE_CONNECTION,
                strategy_name_zh="ä¼˜åŒ–è¿æ¥",
                modifications=optimize_modifications[:5],  # Limit to top 5
                outline=[f"æ®µè½{i+1}: {p.function_type}" for i, p in enumerate(result.paragraphs)],
                predicted_improvement=15,
                explanation_zh="ä¿æŒæ®µè½é¡ºåºï¼Œä¼˜åŒ–æ®µè½ä¹‹é—´çš„è¡”æ¥æ–¹å¼"
            ))

            # Option 2: Deep Restructure (aggressive approach)
            # é€‰é¡¹2ï¼šæ·±åº¦é‡ç»„ï¼ˆæ¿€è¿›æ–¹æ³•ï¼‰
            if result.structure_score >= 40:
                # Suggest reordering based on function types
                # æ ¹æ®åŠŸèƒ½ç±»å‹å»ºè®®é‡æ–°æ’åº
                new_order = list(range(len(result.paragraphs)))

                # Example: Move a body paragraph to front as hook
                # ç¤ºä¾‹ï¼šå°†æ­£æ–‡æ®µè½ç§»åˆ°å‰é¢ä½œä¸ºé’©å­
                body_indices = [
                    i for i, p in enumerate(result.paragraphs)
                    if p.function_type in ["evidence", "body"]
                ]
                if body_indices and len(result.paragraphs) > 3:
                    # Move evidence paragraph after intro as hook
                    # å°†è¯æ®æ®µè½ç§»åˆ°å¼•è¨€åä½œä¸ºé’©å­
                    hook_idx = body_indices[0]
                    new_order = [0, hook_idx] + [
                        i for i in range(1, len(result.paragraphs))
                        if i != hook_idx
                    ]

                options.append(StructureOption(
                    strategy=StructureStrategy.DEEP_RESTRUCTURE,
                    strategy_name_zh="æ·±åº¦é‡ç»„",
                    new_order=new_order,
                    restructure_type="hook_cycle",
                    restructure_type_zh="é’©å­å¾ªç¯",
                    changes=[
                        StructureChange(
                            type="reorder",
                            affected_paragraphs=new_order,
                            description="Reorganize to break linear flow",
                            description_zh="é‡æ–°ç»„ç»‡ä»¥æ‰“ç ´çº¿æ€§æµç¨‹"
                        )
                    ],
                    outline=[f"æ–°ä½ç½®{i+1}: åŸæ®µè½{idx+1}" for i, idx in enumerate(new_order)],
                    predicted_improvement=25,
                    explanation_zh="é‡æ–°æ’åºæ®µè½ï¼Œæ‰“ç ´å¯é¢„æµ‹çš„ç»“æ„æ¨¡å¼"
                ))

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        paragraphs = [
            ParagraphInfo(
                index=p.index,
                first_sentence=p.first_sentence[:200] if len(p.first_sentence) > 200 else p.first_sentence,
                last_sentence=p.last_sentence[:200] if len(p.last_sentence) > 200 else p.last_sentence,
                word_count=p.word_count,
                sentence_count=p.sentence_count,
                has_topic_sentence=p.has_topic_sentence,
                has_summary_ending=p.has_summary_ending,
                connector_words=p.connector_words,
                function_type=p.function_type
            )
            for p in result.paragraphs
        ]

        issues = [
            StructureIssue(
                type=i.type,
                description=i.description,
                description_zh=i.description_zh,
                severity=i.severity,
                affected_paragraphs=i.affected_paragraphs,
                suggestion=i.suggestion,
                suggestion_zh=i.suggestion_zh
            )
            for i in result.issues
        ]

        break_points = [
            BreakPoint(
                position=bp.position,
                type=bp.type,
                description=bp.description,
                description_zh=bp.description_zh
            )
            for bp in result.break_points
        ]

        return StructureAnalysisResponse(
            total_paragraphs=result.total_paragraphs,
            total_sentences=result.total_sentences,
            total_words=result.total_words,
            avg_paragraph_length=result.avg_paragraph_length,
            paragraph_length_variance=result.paragraph_length_variance,
            structure_score=result.structure_score,
            risk_level=result.risk_level,
            paragraphs=paragraphs,
            issues=issues,
            break_points=break_points,
            core_thesis=result.core_thesis,
            key_arguments=result.key_arguments,
            has_linear_flow=result.has_linear_flow,
            has_repetitive_pattern=result.has_repetitive_pattern,
            has_uniform_length=result.has_uniform_length,
            has_predictable_order=result.has_predictable_order,
            options=options,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Structure analysis with suggestions error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/suggest/{strategy}", response_model=StructureOption)
async def get_structure_suggestion(
    strategy: StructureStrategy,
    request: StructureAnalysisRequest
):
    """
    Get suggestion for a specific restructuring strategy
    è·å–ç‰¹å®šé‡ç»„ç­–ç•¥çš„å»ºè®®

    Args:
        strategy: Restructuring strategy é‡ç»„ç­–ç•¥
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureOption with specific suggestion åŒ…å«ç‰¹å®šå»ºè®®çš„é€‰é¡¹
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        if strategy == StructureStrategy.OPTIMIZE_CONNECTION:
            # Generate optimize connection suggestion
            # ç”Ÿæˆä¼˜åŒ–è¿æ¥å»ºè®®
            modifications = []
            for p in result.paragraphs:
                if p.has_topic_sentence or p.connector_words:
                    modifications.append(
                        StructureModification(
                            paragraph_index=p.index,
                            change_type="rewrite_opening",
                            original=p.first_sentence[:100],
                            modified=None,
                            explanation_zh="å»ºè®®ç§»é™¤æ˜¾å¼è¿æ¥è¯"
                        )
                    )

            return StructureOption(
                strategy=StructureStrategy.OPTIMIZE_CONNECTION,
                strategy_name_zh="ä¼˜åŒ–è¿æ¥",
                modifications=modifications[:5],
                outline=[f"æ®µè½{i+1}: {p.function_type}" for i, p in enumerate(result.paragraphs)],
                predicted_improvement=15,
                explanation_zh="ä¿æŒé¡ºåºï¼Œä¼˜åŒ–è¡”æ¥"
            )

        elif strategy == StructureStrategy.DEEP_RESTRUCTURE:
            # Generate deep restructure suggestion
            # ç”Ÿæˆæ·±åº¦é‡ç»„å»ºè®®
            new_order = list(range(len(result.paragraphs)))

            # Suggest moving evidence to front
            # å»ºè®®å°†è¯æ®ç§»åˆ°å‰é¢
            body_indices = [
                i for i, p in enumerate(result.paragraphs)
                if p.function_type in ["evidence", "body"]
            ]
            if body_indices and len(result.paragraphs) > 3:
                hook_idx = body_indices[0]
                new_order = [0, hook_idx] + [
                    i for i in range(1, len(result.paragraphs))
                    if i != hook_idx
                ]

            return StructureOption(
                strategy=StructureStrategy.DEEP_RESTRUCTURE,
                strategy_name_zh="æ·±åº¦é‡ç»„",
                new_order=new_order,
                restructure_type="hook_cycle",
                restructure_type_zh="é’©å­å¾ªç¯",
                changes=[
                    StructureChange(
                        type="reorder",
                        affected_paragraphs=new_order,
                        description="Reorganize paragraph order",
                        description_zh="é‡æ–°ç»„ç»‡æ®µè½é¡ºåº"
                    )
                ],
                outline=[f"æ–°ä½ç½®{i+1}: åŸæ®µè½{idx+1}" for i, idx in enumerate(new_order)],
                predicted_improvement=25,
                explanation_zh="æ‰“ç ´çº¿æ€§ç»“æ„ï¼Œåˆ›å»ºé’©å­å¾ªç¯æ¨¡å¼"
            )

    except Exception as e:
        logger.error(f"Get structure suggestion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/diagnosis", response_model=LogicDiagnosisResponse)
async def get_logic_diagnosis(request: StructureAnalysisRequest):
    """
    Get logic diagnosis card for document
    è·å–æ–‡æ¡£çš„é€»è¾‘è¯Šæ–­å¡

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        LogicDiagnosisResponse with diagnosis card data åŒ…å«è¯Šæ–­å¡æ•°æ®çš„å“åº”
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Generate flow map
        # ç”Ÿæˆæµç¨‹å›¾
        flow_map = []
        for i in range(len(result.paragraphs) - 1):
            curr = result.paragraphs[i]
            next_para = result.paragraphs[i + 1]

            # Determine relationship
            # ç¡®å®šå…³ç³»
            if next_para.connector_words:
                relation = "continuation"
                symbol = "â†’"
            elif curr.function_type == "evidence" and next_para.function_type == "analysis":
                relation = "evidence"
                symbol = "â¤µ"
            elif curr.function_type == next_para.function_type:
                relation = "comparison"
                symbol = "â†”"
            else:
                relation = "continuation"
                symbol = "â†’"

            # Check for gaps
            # æ£€æŸ¥é—´éš™
            for bp in result.break_points:
                if bp.position == i + 1:
                    relation = "gap"
                    symbol = "âœ—"
                    break

            flow_map.append(FlowRelation(
                **{"from": i, "to": i + 1},
                relation=relation,
                symbol=symbol
            ))

        # Determine structure pattern
        # ç¡®å®šç»“æ„æ¨¡å¼
        if result.has_linear_flow:
            pattern = "linear"
            pattern_zh = "çº¿æ€§"
        elif result.has_repetitive_pattern:
            pattern = "parallel"
            pattern_zh = "å¹¶åˆ—"
        else:
            pattern = "nested"
            pattern_zh = "åµŒå¥—"

        # Generate risk areas
        # ç”Ÿæˆé£é™©åŒºåŸŸ
        risk_areas = []
        for p in result.paragraphs:
            risk_level = "low"
            reason = ""
            reason_zh = ""

            if p.has_topic_sentence and p.connector_words:
                risk_level = "high"
                reason = "Topic sentence with explicit connectors"
                reason_zh = "ä¸»é¢˜å¥é…åˆæ˜¾å¼è¿æ¥è¯"
            elif p.has_topic_sentence:
                risk_level = "medium"
                reason = "Topic sentence pattern detected"
                reason_zh = "æ£€æµ‹åˆ°ä¸»é¢˜å¥æ¨¡å¼"
            elif p.connector_words:
                risk_level = "medium"
                reason = "Explicit connectors detected"
                reason_zh = "æ£€æµ‹åˆ°æ˜¾å¼è¿æ¥è¯"

            if risk_level != "low":
                risk_areas.append(RiskArea(
                    paragraph=p.index,
                    risk_level=risk_level,
                    reason=reason,
                    reason_zh=reason_zh
                ))

        # Determine recommended strategy
        # ç¡®å®šæ¨èç­–ç•¥
        if result.structure_score >= 40:
            recommended = StructureStrategy.DEEP_RESTRUCTURE
            rec_reason = "High structure score requires significant reorganization"
            rec_reason_zh = "é«˜ç»“æ„åˆ†æ•°éœ€è¦æ˜¾è‘—é‡ç»„"
        else:
            recommended = StructureStrategy.OPTIMIZE_CONNECTION
            rec_reason = "Moderate issues can be fixed with connection optimization"
            rec_reason_zh = "ä¸­ç­‰é—®é¢˜å¯ä»¥é€šè¿‡ä¼˜åŒ–è¿æ¥ä¿®å¤"

        return LogicDiagnosisResponse(
            flow_map=flow_map,
            structure_pattern=pattern,
            structure_pattern_zh=pattern_zh,
            pattern_description=f"Document follows a {pattern} structure pattern",
            pattern_description_zh=f"æ–‡æ¡£éµå¾ª{pattern_zh}ç»“æ„æ¨¡å¼",
            risk_areas=risk_areas,
            recommended_strategy=recommended,
            recommendation_reason=rec_reason,
            recommendation_reason_zh=rec_reason_zh
        )

    except Exception as e:
        logger.error(f"Logic diagnosis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document", response_model=SmartStructureResponse)
async def analyze_document_structure(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Analyze structure of a document by ID using smart LLM analysis
    ä½¿ç”¨æ™ºèƒ½ LLM åˆ†ææŒ‰ ID åˆ†ææ–‡æ¡£ç»“æ„

    Features:
    - Filters non-paragraph content (titles, tables, figures)
    - Uses paper section numbering (1, 1.1, 2.3.1)
    - Generates meaningful content summaries
    - Labels each paragraph with position like "3.2(1)"
    - Caches results to avoid repeated LLM calls
    - ç¼“å­˜ç»“æœä»¥é¿å…é‡å¤çš„ LLM è°ƒç”¨

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        SmartStructureResponse with analysis results åŒ…å«åˆ†æç»“æœçš„å“åº”
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if we have cached analysis result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
        if document.structure_analysis_cache:
            logger.info(f"Using cached structure analysis for document {request.document_id}")
            result = document.structure_analysis_cache
        else:
            # Use smart LLM-based analysis
            # ä½¿ç”¨æ™ºèƒ½ LLM åˆ†æ
            logger.info(f"Starting smart structure analysis for document {request.document_id}")
            result = await smart_analyzer.analyze(document.original_text)
            logger.info(f"Smart analysis completed: {result.get('total_paragraphs', 0)} paragraphs found")

            # Cache the result in database
            # å°†ç»“æœç¼“å­˜åˆ°æ•°æ®åº“
            document.structure_analysis_cache = result
            flag_modified(document, 'structure_analysis_cache')
            await db.commit()
            logger.info(f"Cached structure analysis for document {request.document_id}")

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        sections = [
            SectionInfo(
                number=s.get("number", "?"),
                title=s.get("title", ""),
                paragraphs=[
                    SmartParagraphInfo(
                        position=p.get("position", "?"),
                        summary=p.get("summary", ""),
                        summary_zh=p.get("summary_zh", ""),
                        first_sentence=p.get("first_sentence", "")[:200],
                        last_sentence=p.get("last_sentence", "")[:200],
                        word_count=p.get("word_count", 0),
                        ai_risk=p.get("ai_risk", "unknown"),
                        ai_risk_reason=p.get("ai_risk_reason", ""),
                        # New fields for detailed rewrite suggestions
                        # æ–°å­—æ®µï¼šè¯¦ç»†ä¿®æ”¹å»ºè®®
                        rewrite_suggestion_zh=p.get("rewrite_suggestion_zh"),
                        rewrite_example=p.get("rewrite_example")
                    )
                    for p in s.get("paragraphs", [])
                ]
            )
            for s in result.get("sections", [])
        ]

        # Convert issues
        # è½¬æ¢é—®é¢˜
        # Handle case where LLM returns "All" instead of a list
        # å¤„ç†LLMè¿”å›"All"è€Œéåˆ—è¡¨çš„æƒ…å†µ
        def ensure_list(val):
            if isinstance(val, list):
                return val
            elif isinstance(val, str):
                return [val] if val else []
            return []

        issues = [
            SmartStructureIssue(
                type=i.get("type", "unknown"),
                description=i.get("description", ""),
                description_zh=i.get("description_zh", ""),
                severity=i.get("severity", "low"),
                affected_positions=ensure_list(i.get("affected_positions", []))
            )
            for i in result.get("issues", [])
        ]

        # Build compatible paragraphs list for existing frontend
        # æ„å»ºä¸ç°æœ‰å‰ç«¯å…¼å®¹çš„æ®µè½åˆ—è¡¨
        paragraphs = []
        idx = 0
        for section in sections:
            for p in section.paragraphs:
                paragraphs.append(ParagraphInfo(
                    index=idx,
                    first_sentence=p.first_sentence,
                    last_sentence=p.last_sentence,
                    word_count=p.word_count,
                    sentence_count=0,
                    has_topic_sentence=False,
                    has_summary_ending=False,
                    connector_words=[],
                    function_type="body",
                    position=p.position,
                    summary=p.summary,
                    summary_zh=p.summary_zh,
                    ai_risk=p.ai_risk,
                    ai_risk_reason=p.ai_risk_reason,
                    # New rewrite suggestion fields
                    # æ–°çš„ä¿®æ”¹å»ºè®®å­—æ®µ
                    rewrite_suggestion_zh=p.rewrite_suggestion_zh,
                    rewrite_example=p.rewrite_example
                ))
                idx += 1

        # Extract pattern flags from score breakdown
        # ä»åˆ†æ•°åˆ†è§£ä¸­æå–æ¨¡å¼æ ‡å¿—
        score_breakdown = result.get("score_breakdown", {})

        # Convert explicit connectors
        # è½¬æ¢æ˜¾æ€§è¿æ¥è¯
        explicit_connectors = [
            ExplicitConnector(
                word=c.get("word", ""),
                position=c.get("position", ""),
                location=c.get("location", "paragraph_start"),
                severity=c.get("severity", "high")
            )
            for c in result.get("explicit_connectors", [])
        ]

        # Convert logic breaks
        # è½¬æ¢é€»è¾‘æ–­è£‚ç‚¹
        # Note: lb.get("key", "") returns None if key exists with null value
        # Pydantic v2 treats explicit None differently - always convert to empty string
        # æ³¨æ„ï¼šå½“keyå­˜åœ¨ä½†å€¼ä¸ºnullæ—¶ï¼Œlb.get("key", "")è¿”å›None
        # Pydantic v2 å¯¹æ˜¾å¼ None å¤„ç†ä¸åŒ - å§‹ç»ˆè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        logic_breaks = [
            LogicBreak(
                from_position=lb.get("from_position") or "",
                to_position=lb.get("to_position") or "",
                transition_type=lb.get("transition_type") or "abrupt",
                issue=lb.get("issue") or "",
                issue_zh=lb.get("issue_zh") or "",
                suggestion=lb.get("suggestion") or "",  # Empty string if None/null
                suggestion_zh=lb.get("suggestion_zh") or ""  # Empty string if None/null
            )
            for lb in result.get("logic_breaks", [])
        ]

        # Parse detailed suggestions if available
        # è§£æè¯¦ç»†å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰
        detailed_suggestions = None
        raw_suggestions = result.get("detailed_suggestions")
        if raw_suggestions and isinstance(raw_suggestions, dict):
            section_suggestions = []
            for s in raw_suggestions.get("section_suggestions", []):
                section_suggestions.append(SectionSuggestion(
                    section_number=s.get("section_number", "?"),
                    section_title=s.get("section_title", ""),
                    severity=s.get("severity", "medium"),
                    suggestion_type=s.get("suggestion_type", "restructure"),
                    suggestion_zh=s.get("suggestion_zh", ""),
                    suggestion_en=s.get("suggestion_en", ""),
                    details=s.get("details", []),
                    affected_paragraphs=s.get("affected_paragraphs", [])
                ))
            detailed_suggestions = DetailedImprovementSuggestions(
                abstract_suggestions=raw_suggestions.get("abstract_suggestions", []),
                logic_suggestions=raw_suggestions.get("logic_suggestions", []),
                section_suggestions=section_suggestions,
                priority_order=raw_suggestions.get("priority_order", []),
                overall_assessment_zh=raw_suggestions.get("overall_assessment_zh", ""),
                overall_assessment_en=raw_suggestions.get("overall_assessment_en", "")
            )

        return SmartStructureResponse(
            sections=sections,
            total_paragraphs=result.get("total_paragraphs", len(paragraphs)),
            total_sections=result.get("total_sections", len(sections)),
            structure_score=result.get("structure_score", 0),
            risk_level=result.get("risk_level", "low"),
            issues=issues,
            score_breakdown=score_breakdown,
            recommendation=result.get("recommendation", ""),
            recommendation_zh=result.get("recommendation_zh", ""),
            detailed_suggestions=detailed_suggestions,
            explicit_connectors=explicit_connectors,
            logic_breaks=logic_breaks,
            paragraphs=paragraphs,
            has_linear_flow=score_breakdown.get("linear_flow", 0) > 0,
            has_repetitive_pattern=score_breakdown.get("repetitive_pattern", 0) > 0,
            has_uniform_length=score_breakdown.get("uniform_length", 0) > 0,
            has_predictable_order=score_breakdown.get("predictable_order", 0) > 0,
            options=[]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Step 1-1 and Step 1-2: Two-Phase Analysis Endpoints
# æ­¥éª¤ 1-1 å’Œ 1-2ï¼šä¸¤é˜¶æ®µåˆ†æç«¯ç‚¹
# =============================================================================

@router.post("/document/step1-1")
async def analyze_document_structure_step1(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Step 1-1: Analyze document STRUCTURE only (global patterns)
    æ­¥éª¤ 1-1ï¼šä»…åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆå…¨å±€æ¨¡å¼ï¼‰

    This endpoint performs the first phase of analysis:
    - Section structure identification
    - Paragraph identification
    - Global structural patterns (linear flow, symmetry, etc.)
    - Structure score calculation
    - Style/formality analysis with mismatch detection

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Structure analysis result ç»“æ„åˆ†æç»“æœ
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Get colloquialism_level from session if provided
        # å¦‚æœæä¾›äº† session_idï¼Œä» session è·å– colloquialism_level
        target_colloquialism = None
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session:
                target_colloquialism = session.colloquialism_level
                logger.info(f"Using colloquialism_level={target_colloquialism} from session {request.session_id}")

        # Check if we have cached Step 1-1 result with same colloquialism level
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ­¥éª¤ 1-1 ç»“æœï¼ˆä¸”å£è¯­åŒ–çº§åˆ«ç›¸åŒï¼‰
        cache_key = "step1_1_cache"
        if hasattr(document, 'structure_analysis_cache') and document.structure_analysis_cache:
            cached = document.structure_analysis_cache
            if cache_key in cached:
                # Check if cached result was analyzed with same colloquialism level
                # æ£€æŸ¥ç¼“å­˜ç»“æœæ˜¯å¦ä½¿ç”¨ç›¸åŒçš„å£è¯­åŒ–çº§åˆ«åˆ†æ
                cached_style = cached[cache_key].get("style_analysis", {})
                cached_target = cached_style.get("target_colloquialism")
                if cached_target == target_colloquialism or (cached_target is None and target_colloquialism is None):
                    logger.info(f"Using cached Step 1-1 result for document {request.document_id}")
                    return cached[cache_key]
                else:
                    logger.info(f"Cached result has different colloquialism level, re-analyzing")

        # Perform Step 1-1 analysis with colloquialism level
        # ä½¿ç”¨å£è¯­åŒ–çº§åˆ«æ‰§è¡Œæ­¥éª¤ 1-1 åˆ†æ
        logger.info(f"Starting Step 1-1 structure analysis for document {request.document_id} (target_colloquialism={target_colloquialism})")
        result = await smart_analyzer.analyze_structure(document.original_text, target_colloquialism=target_colloquialism)

        # Cache the result to SQLite
        # ç¼“å­˜ç»“æœåˆ° SQLite
        if not document.structure_analysis_cache:
            document.structure_analysis_cache = {}
        document.structure_analysis_cache[cache_key] = result
        flag_modified(document, 'structure_analysis_cache')
        await db.commit()
        logger.info(f"Step 1-1 cache saved to SQLite for document {request.document_id}")

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Step 1-1 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document/step1-2")
async def analyze_document_relationships_step2(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Step 1-2: Analyze paragraph RELATIONSHIPS (connections, transitions)
    æ­¥éª¤ 1-2ï¼šåˆ†ææ®µè½å…³ç³»ï¼ˆè¿æ¥ã€è¿‡æ¸¡ï¼‰

    This endpoint performs the second phase of analysis:
    - Explicit connector word detection
    - Logic break points between paragraphs
    - AI risk assessment for each paragraph
    - Rewrite suggestions

    If Step 1-1 is not completed (no cache), it will be automatically run first.

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Relationship analysis result å…³ç³»åˆ†æç»“æœ
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        step1_1_key = "step1_1_cache"
        step1_2_key = "step1_2_cache"

        # Check if Step 1-1 has been completed
        # æ£€æŸ¥æ­¥éª¤ 1-1 æ˜¯å¦å·²å®Œæˆ
        if not document.structure_analysis_cache or step1_1_key not in document.structure_analysis_cache:
            logger.info(f"Step 1-1 cache missing for document {request.document_id}, auto-running Step 1-1 analysis...")
            
            # Auto-run Step 1-1
            # è‡ªåŠ¨è¿è¡Œ Step 1-1
            
            # Get colloquialism_level from session if provided
            target_colloquialism = None
            if request.session_id:
                session = await db.get(Session, request.session_id)
                if session:
                    target_colloquialism = session.colloquialism_level
                    logger.info(f"Using colloquialism_level={target_colloquialism} from session {request.session_id}")

            # Perform Step 1-1 analysis
            step1_1_result = await smart_analyzer.analyze_structure(
                document.original_text, 
                target_colloquialism=target_colloquialism
            )

            # Initialize cache dict if needed
            if not document.structure_analysis_cache:
                document.structure_analysis_cache = {}
            
            # Cache Step 1-1 result
            document.structure_analysis_cache[step1_1_key] = step1_1_result
            flag_modified(document, 'structure_analysis_cache')
            # We don't commit here yet, we'll commit after Step 1-2 to save DB calls, 
            # or commit now to be safe. Committing now is safer.
            await db.commit()
            
            structure_result = step1_1_result
        else:
            # Get Step 1-1 result from cache
            # ä»ç¼“å­˜è·å–æ­¥éª¤ 1-1 ç»“æœ
            structure_result = document.structure_analysis_cache[step1_1_key]

        # Check if we have cached Step 1-2 result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ­¥éª¤ 1-2 ç»“æœ
        if step1_2_key in document.structure_analysis_cache:
            # If we just auto-ran Step 1-1, we probably want to re-run Step 1-2 too,
            # but usually if Step 1-1 was missing, Step 1-2 would be missing too.
            # If explicit re-analysis is needed, cache should be cleared via delete endpoint.
            logger.info(f"Using cached Step 1-2 result for document {request.document_id}")
            return document.structure_analysis_cache[step1_2_key]

        # Perform Step 1-2 analysis
        # æ‰§è¡Œæ­¥éª¤ 1-2 åˆ†æ
        logger.info(f"Starting Step 1-2 relationship analysis for document {request.document_id}")
        result = await smart_analyzer.analyze_relationships(
            document.original_text,
            structure_result
        )

        # Cache the result to SQLite
        # ç¼“å­˜ç»“æœåˆ° SQLite
        # Refresh document to ensure we have latest version if we committed above
        # (SQLAlchemy async session usually handles this, but being careful)
        if not document.structure_analysis_cache:
             document.structure_analysis_cache = {}
             
        document.structure_analysis_cache[step1_2_key] = result
        flag_modified(document, 'structure_analysis_cache')
        await db.commit()
        logger.info(f"Step 1-2 cache saved to SQLite for document {request.document_id}")

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Step 1-2 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/document/{document_id}/cache")
async def clear_analysis_cache(
    document_id: str,
    db: AsyncSession = Depends(get_db)
):
    """
    Clear analysis cache for a document (allows re-analysis)
    æ¸…é™¤æ–‡æ¡£çš„åˆ†æç¼“å­˜ï¼ˆå…è®¸é‡æ–°åˆ†æï¼‰

    Args:
        document_id: Document ID æ–‡æ¡£ID
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Success message æˆåŠŸæ¶ˆæ¯
    """
    try:
        document = await db.get(Document, document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        document.structure_analysis_cache = None
        flag_modified(document, 'structure_analysis_cache')
        await db.commit()
        logger.info(f"Cache cleared for document {document_id}")

        return {"message": "Cache cleared successfully", "document_id": document_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Clear cache error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/strategies")
async def get_structure_strategies():
    """
    Get available structure strategies
    è·å–å¯ç”¨çš„ç»“æ„ç­–ç•¥

    Returns:
        List of strategy descriptions ç­–ç•¥æè¿°åˆ—è¡¨
    """
    return {
        "strategies": [
            {
                "id": "optimize_connection",
                "name": "Optimize Connection",
                "name_zh": "ä¼˜åŒ–è¿æ¥",
                "description": "Improve paragraph connections without changing content order",
                "description_zh": "åœ¨ä¸æ”¹å˜å†…å®¹é¡ºåºçš„æƒ…å†µä¸‹æ”¹å–„æ®µè½è¿æ¥"
            },
            {
                "id": "deep_restructure",
                "name": "Deep Restructure",
                "name_zh": "æ·±åº¦é‡ç»„",
                "description": "Reorder and reorganize content for maximum naturalness",
                "description_zh": "é‡æ–°æ’åºå’Œç»„ç»‡å†…å®¹ä»¥è·å¾—æœ€å¤§è‡ªç„¶åº¦"
            }
        ]
    }


# =============================================================================
# Enhanced Structure Analysis Endpoints (Level 1 Enhancement)
# å¢å¼ºç»“æ„åˆ†æç«¯ç‚¹ï¼ˆLevel 1å¢å¼ºï¼‰
# =============================================================================

@router.post("/predictability", response_model=PredictabilityAnalysisResponse)
async def analyze_predictability(request: PredictabilityAnalysisRequest):
    """
    Analyze document structure predictability
    åˆ†ææ–‡æ¡£ç»“æ„é¢„æµ‹æ€§

    This endpoint provides detailed analysis of:
    - Progression type (monotonic vs non-monotonic)
    - Function distribution (uniform vs asymmetric)
    - Closure pattern (strong vs weak/open)
    - Lexical echo (explicit connectors vs semantic bridges)

    Args:
        request: Predictability analysis request é¢„æµ‹æ€§åˆ†æè¯·æ±‚

    Returns:
        PredictabilityAnalysisResponse with detailed analysis åŒ…å«è¯¦ç»†åˆ†æçš„å“åº”
    """
    try:
        # Perform full structure analysis
        # æ‰§è¡Œå®Œæ•´ç»“æ„åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=True
        )

        # Convert progression analysis
        # è½¬æ¢æ¨è¿›åˆ†æ
        progression = None
        if result.progression_analysis:
            progression = ProgressionAnalysisResult(
                progression_type=result.progression_analysis.progression_type,
                progression_type_zh=result.progression_analysis.progression_type_zh,
                forward_transitions=result.progression_analysis.forward_transitions,
                backward_references=result.progression_analysis.backward_references,
                conditional_statements=result.progression_analysis.conditional_statements,
                score=result.progression_analysis.score
            )

        # Convert function distribution
        # è½¬æ¢åŠŸèƒ½åˆ†å¸ƒ
        distribution = None
        if result.function_distribution:
            distribution = FunctionDistributionResult(
                distribution_type=result.function_distribution.distribution_type,
                distribution_type_zh=result.function_distribution.distribution_type_zh,
                function_counts=result.function_distribution.function_counts,
                depth_variance=result.function_distribution.depth_variance,
                longest_section_ratio=result.function_distribution.longest_section_ratio,
                score=result.function_distribution.score,
                asymmetry_opportunities=result.function_distribution.asymmetry_opportunities
            )

        # Convert closure analysis
        # è½¬æ¢é—­åˆåˆ†æ
        closure = None
        if result.closure_analysis:
            closure = ClosureAnalysisResult(
                closure_type=result.closure_analysis.closure_type,
                closure_type_zh=result.closure_analysis.closure_type_zh,
                has_formulaic_ending=result.closure_analysis.has_formulaic_ending,
                has_complete_resolution=result.closure_analysis.has_complete_resolution,
                open_questions=result.closure_analysis.open_questions,
                hedging_in_conclusion=result.closure_analysis.hedging_in_conclusion,
                score=result.closure_analysis.score,
                detected_patterns=result.closure_analysis.detected_patterns
            )

        # Convert lexical echo analysis
        # è½¬æ¢è¯æ±‡å›å£°åˆ†æ
        lexical_echo = None
        if result.lexical_echo_analysis:
            lexical_echo = LexicalEchoResult(
                total_transitions=result.lexical_echo_analysis.total_transitions,
                echo_transitions=result.lexical_echo_analysis.echo_transitions,
                explicit_connector_transitions=result.lexical_echo_analysis.explicit_connector_transitions,
                echo_ratio=result.lexical_echo_analysis.echo_ratio,
                score=result.lexical_echo_analysis.score,
                transition_details=result.lexical_echo_analysis.transition_details
            )

        # Determine recommended disruption level based on score
        # æ ¹æ®åˆ†æ•°ç¡®å®šæ¨èçš„æ‰°åŠ¨ç­‰çº§
        if result.structure_score >= 60:
            recommended_level = "strong"
            recommended_strategies = ["inversion", "conflict_injection", "weak_closure", "asymmetry"]
        elif result.structure_score >= 35:
            recommended_level = "medium"
            recommended_strategies = ["lexical_echo", "asymmetry", "local_reorder"]
        else:
            recommended_level = "light"
            recommended_strategies = ["rewrite_opening", "remove_connector", "lexical_echo"]

        return PredictabilityAnalysisResponse(
            total_score=result.structure_score,
            risk_level=result.risk_level,
            progression_analysis=progression,
            function_distribution=distribution,
            closure_analysis=closure,
            lexical_echo_analysis=lexical_echo,
            recommended_disruption_level=recommended_level,
            recommended_strategies=recommended_strategies,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Predictability analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/disruption-levels")
async def get_disruption_levels():
    """
    Get available disruption levels and their configurations
    è·å–å¯ç”¨çš„æ‰°åŠ¨ç­‰çº§åŠå…¶é…ç½®

    Returns:
        Dictionary of disruption levels æ‰°åŠ¨ç­‰çº§å­—å…¸
    """
    return {
        "levels": DISRUPTION_LEVELS,
        "strategies": DISRUPTION_STRATEGIES
    }


@router.get("/disruption-strategies")
async def get_disruption_strategies():
    """
    Get available disruption strategies
    è·å–å¯ç”¨çš„æ‰°åŠ¨ç­–ç•¥

    Returns:
        List of strategy descriptions ç­–ç•¥æè¿°åˆ—è¡¨
    """
    strategies = []
    for key, value in DISRUPTION_STRATEGIES.items():
        strategies.append({
            "id": key,
            "name": value["name"],
            "name_zh": value["name_zh"],
            "description": value["description"],
            "description_zh": value["description_zh"],
            "prompt_instruction": value["prompt_instruction"]
        })
    return {"strategies": strategies}


# =============================================================================
# 7-Indicator Structural Risk Card Endpoint
# 7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç«¯ç‚¹
# =============================================================================

@router.post("/risk-card", response_model=StructuralRiskCardResponse)
async def get_structural_risk_card(request: RiskCardRequest):
    """
    Get 7-indicator structural risk card for user visualization
    è·å–7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç”¨äºç”¨æˆ·å¯è§†åŒ–

    Returns a visual risk card showing:
    - 7 AI structural indicators with emoji and color
    - Whether each indicator is triggered
    - Overall risk level and summary

    The 7 indicators are:
    1. âš–ï¸ Perfect Symmetry (é€»è¾‘æ¨è¿›å¯¹ç§°) - â˜…â˜…â˜…
    2. ğŸ“Š Uniform Function (æ®µè½åŠŸèƒ½å‡åŒ€) - â˜…â˜…â˜†
    3. ğŸ”— Over-signaled Transitions (è¿æ¥è¯ä¾èµ–) - â˜…â˜…â˜…
    4. ğŸ“ Linear Enumeration (å•ä¸€çº¿æ€§æ¨è¿›) - â˜…â˜…â˜…
    5. ğŸ“ Rhythmic Regularity (æ®µè½èŠ‚å¥å‡è¡¡) - â˜…â˜…â˜†
    6. ğŸ”’ Over-conclusive Ending (ç»“å°¾è¿‡åº¦é—­åˆ) - â˜…â˜…â˜†
    7. ğŸ”„ No Cross-References (ç¼ºä¹å›æŒ‡ç»“æ„) - â˜…â˜…â˜†

    Args:
        request: Risk card analysis request é£é™©å¡ç‰‡åˆ†æè¯·æ±‚

    Returns:
        StructuralRiskCardResponse with 7 indicators åŒ…å«7ä¸ªæŒ‡å¾çš„å“åº”
    """
    try:
        # Perform full structure analysis
        # æ‰§è¡Œå®Œæ•´ç»“æ„åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=True
        )

        # Get the risk card from the result
        # ä»ç»“æœä¸­è·å–é£é™©å¡ç‰‡
        if not result.risk_card:
            raise HTTPException(status_code=500, detail="Failed to generate risk card")

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        indicators = [
            StructuralIndicatorResponse(
                id=ind.id,
                name=ind.name,
                name_zh=ind.name_zh,
                triggered=ind.triggered,
                risk_level=ind.risk_level,
                emoji=ind.emoji,
                color=ind.color,
                description=ind.description,
                description_zh=ind.description_zh,
                details=ind.details,
                details_zh=ind.details_zh
            )
            for ind in result.risk_card.indicators
        ]

        return StructuralRiskCardResponse(
            indicators=indicators,
            triggered_count=result.risk_card.triggered_count,
            overall_risk=result.risk_card.overall_risk,
            overall_risk_zh=result.risk_card.overall_risk_zh,
            summary=result.risk_card.summary,
            summary_zh=result.risk_card.summary_zh,
            total_score=result.risk_card.total_score
        )

    except Exception as e:
        logger.error(f"Risk card generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/indicator-config")
async def get_indicator_config():
    """
    Get the 7-indicator configuration for UI display
    è·å–7æŒ‡å¾é…ç½®ç”¨äºUIæ˜¾ç¤º

    Returns:
        Configuration for all 7 indicators with emoji, colors, and descriptions
    """
    from src.core.analyzer.structure import StructureAnalyzer

    return {
        "indicators": StructureAnalyzer.INDICATOR_CONFIG,
        "summary_text": "AIå†™ä½œçš„æœ€å¤§ç‰¹å¾ä¸æ˜¯è¯­æ³•å®Œç¾ï¼Œè€Œæ˜¯ç»“æ„å¤ªå®Œç¾ã€‚",
        "summary_text_en": "The biggest feature of AI writing is not perfect grammar, but perfect structure."
    }


# =============================================================================
# Single Paragraph Suggestion Endpoint
# å•ä¸ªæ®µè½å»ºè®®ç«¯ç‚¹
# =============================================================================

# Prompt for generating single paragraph rewrite suggestions
# ç”Ÿæˆå•ä¸ªæ®µè½æ”¹å†™å»ºè®®çš„æç¤ºè¯
PARAGRAPH_SUGGESTION_PROMPT = """You are an expert De-AIGC consultant. Analyze this paragraph and provide specific rewriting advice to remove AI-writing patterns.

## PARAGRAPH TEXT:
{paragraph_text}

## PARAGRAPH POSITION: {paragraph_position}

## KNOWN AI RISK: {ai_risk} - {ai_risk_reason}

## CONTEXT (if available):
{context_hint}

## YOUR TASK:
Generate a SPECIFIC, ACTIONABLE Chinese rewriting suggestion for this paragraph. Focus on:
1. Identifying specific AI writing patterns (explicit connectors, formulaic structure, etc.)
2. Providing concrete strategies to humanize the text
3. Giving an example of how to rewrite key sentences

## OUTPUT FORMAT (JSON):
{{
  "rewrite_suggestion_zh": "ã€é—®é¢˜è¯Šæ–­ã€‘æ®µé¦–ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯'Furthermore'ï¼Œå±äºå…¸å‹AIå†™ä½œç—•è¿¹ã€‚æ®µè½ç»“æ„é‡‡ç”¨'é—®é¢˜-åˆ†æ-ç»“è®º'å…¬å¼åŒ–æ¨¡å¼ã€‚\\nã€ä¿®æ”¹ç­–ç•¥ã€‘1. åˆ é™¤æ®µé¦–è¿æ¥è¯ï¼Œæ”¹ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥ä¸Šæ®µå…³é”®æ¦‚å¿µï¼›2. æ‰“æ•£å…¬å¼åŒ–ç»“æ„ï¼Œå°†ç»“è®ºæå‰æˆ–èå…¥è®ºè¿°ä¸­ã€‚\\nã€æ”¹å†™æç¤ºã€‘å¯å°†å¼€å¤´æ”¹ä¸ºç›´æ¥æ‰¿æ¥ä¸Šæ®µå†…å®¹ï¼Œå¦‚'åœŸå£¤ç›åˆ†ç´¯ç§¯çš„è¿™ä¸€è¶‹åŠ¿åœ¨...'",
  "rewrite_example": "The escalating trend of soil salinization poses new challenges to traditional agriculture. In the North China Plain, monitoring data from the past decade reveals...",
  "ai_risk": "high",
  "ai_risk_reason": "æ®µé¦–ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯'Furthermore'ï¼Œé‡‡ç”¨å…¬å¼åŒ–ç»“æ„"
}}

CRITICAL RULES:
- The rewrite_suggestion_zh MUST be in Chinese
- The rewrite_suggestion_zh MUST includeã€é—®é¢˜è¯Šæ–­ã€‘ã€ä¿®æ”¹ç­–ç•¥ã€‘ã€æ”¹å†™æç¤ºã€‘sections
- Quote specific text from the paragraph in the diagnosis
- Provide concrete examples, not generic advice
- The rewrite_example should be in ENGLISH showing a better version of the first 1-2 sentences
- The ai_risk_reason should be in CHINESE (ä¸­æ–‡æè¿°ï¼Œå¼•ç”¨åŸæ–‡æ—¶ä¿ç•™åŸè¯­è¨€)
"""


@router.post("/paragraph-suggestion", response_model=ParagraphSuggestionResponse)
async def get_paragraph_suggestion(request: ParagraphSuggestionRequest):
    """
    Get rewrite suggestion for a single paragraph
    è·å–å•ä¸ªæ®µè½çš„æ”¹å†™å»ºè®®

    This endpoint generates specific, actionable rewriting advice
    for a single paragraph using LLM analysis.
    æ­¤ç«¯ç‚¹ä½¿ç”¨ LLM åˆ†æä¸ºå•ä¸ªæ®µè½ç”Ÿæˆå…·ä½“å¯è¡Œçš„æ”¹å†™å»ºè®®ã€‚

    Args:
        request: Paragraph suggestion request æ®µè½å»ºè®®è¯·æ±‚

    Returns:
        ParagraphSuggestionResponse with rewrite suggestions åŒ…å«æ”¹å†™å»ºè®®çš„å“åº”
    """
    try:
        import httpx
        import json
        from src.config import get_settings

        settings = get_settings()

        # Build prompt
        # æ„å»ºæç¤ºè¯
        prompt = PARAGRAPH_SUGGESTION_PROMPT.format(
            paragraph_text=request.paragraph_text[:2000],  # Limit length
            paragraph_position=request.paragraph_position,
            ai_risk=request.ai_risk or "unknown",
            ai_risk_reason=request.ai_risk_reason or "Not yet analyzed",
            context_hint=request.context_hint or "No context provided"
        )

        # Call LLM API
        # è°ƒç”¨ LLM API
        response_text = await _call_llm_for_suggestion(prompt, settings)

        # Parse response
        # è§£æå“åº”
        result = _parse_suggestion_response(response_text)

        return ParagraphSuggestionResponse(
            paragraph_position=request.paragraph_position,
            rewrite_suggestion_zh=result.get("rewrite_suggestion_zh", "ã€é—®é¢˜è¯Šæ–­ã€‘åˆ†æå¤±è´¥\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·ç¨åé‡è¯•\nã€æ”¹å†™æç¤ºã€‘æ— "),
            rewrite_example=result.get("rewrite_example"),
            ai_risk=result.get("ai_risk", request.ai_risk or "unknown"),
            ai_risk_reason=result.get("ai_risk_reason", request.ai_risk_reason or "")
        )

    except Exception as e:
        logger.error(f"Paragraph suggestion error: {e}")
        # Return a fallback response instead of error
        # è¿”å›åå¤‡å“åº”è€Œä¸æ˜¯é”™è¯¯
        return ParagraphSuggestionResponse(
            paragraph_position=request.paragraph_position,
            rewrite_suggestion_zh=f"ã€é—®é¢˜è¯Šæ–­ã€‘åˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·ç¨åé‡è¯•\nã€æ”¹å†™æç¤ºã€‘å»ºè®®åˆ é™¤æ®µé¦–æ˜¾æ€§è¿æ¥è¯ï¼Œæ”¹ç”¨è¯­ä¹‰æ‰¿æ¥",
            rewrite_example=None,
            ai_risk=request.ai_risk or "unknown",
            ai_risk_reason=request.ai_risk_reason or ""
        )


async def _call_llm_for_suggestion(prompt: str, settings) -> str:
    """
    Call LLM API for paragraph suggestion
    è°ƒç”¨ LLM API è·å–æ®µè½å»ºè®®
    """
    import httpx

    # Use DashScope (Aliyun)
    # ä½¿ç”¨é˜¿é‡Œäº‘çµç§¯
    if settings.llm_provider == "dashscope" and settings.dashscope_api_key:
        async with httpx.AsyncClient(
            base_url=settings.dashscope_base_url,
            headers={
                "Authorization": f"Bearer {settings.dashscope_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.dashscope_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use Volcengine
    # ä½¿ç”¨ç«å±±å¼•æ“
    elif settings.llm_provider == "volcengine" and settings.volcengine_api_key:
        async with httpx.AsyncClient(
            base_url=settings.volcengine_base_url,
            headers={
                "Authorization": f"Bearer {settings.volcengine_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.volcengine_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use DeepSeek
    # ä½¿ç”¨ DeepSeek
    elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
        async with httpx.AsyncClient(
            base_url=settings.deepseek_base_url,
            headers={
                "Authorization": f"Bearer {settings.deepseek_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.llm_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use Gemini
    # ä½¿ç”¨ Gemini
    elif settings.llm_provider == "gemini" and settings.gemini_api_key:
        from google import genai
        client = genai.Client(api_key=settings.gemini_api_key)
        response = await client.aio.models.generate_content(
            model=settings.llm_model,
            contents=prompt,
            config={"max_output_tokens": 2048, "temperature": 0.2}
        )
        return response.text

    else:
        raise ValueError("No LLM API configured")


def _parse_suggestion_response(response: str) -> dict:
    """
    Parse LLM response to JSON
    è§£æ LLM å“åº”ä¸º JSON
    """
    import json

    # Clean response (remove markdown code blocks if present)
    # æ¸…ç†å“åº”
    response = response.strip()
    if response.startswith("```"):
        lines = response.split("\n")
        lines = [l for l in lines if not l.strip().startswith("```")]
        response = "\n".join(lines)
    response = response.strip()

    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # Try to extract JSON from response
        # å°è¯•ä»å“åº”ä¸­æå– JSON
        import re
        match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except:
                pass
        # Return default
        return {
            "rewrite_suggestion_zh": "ã€é—®é¢˜è¯Šæ–­ã€‘æ— æ³•è§£æåˆ†æç»“æœ\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·é‡è¯•\nã€æ”¹å†™æç¤ºã€‘å»ºè®®æ£€æŸ¥æ®µè½ç»“æ„",
            "rewrite_example": None,
            "ai_risk": "unknown",
            "ai_risk_reason": "åˆ†æå¤±è´¥"
        }


# =============================================================================
# Issue-Specific Suggestion Endpoint (Step 1-1 Click-to-Expand)
# é’ˆå¯¹ç‰¹å®šé—®é¢˜çš„å»ºè®®ç«¯ç‚¹ï¼ˆStep 1-1 ç‚¹å‡»å±•å¼€ï¼‰
# =============================================================================

@router.post("/issue-suggestion")
async def get_issue_suggestion(
    request: dict,
    db: AsyncSession = Depends(get_db)
):
    """
    Get detailed suggestion for a specific structure issue
    è·å–é’ˆå¯¹ç‰¹å®šç»“æ„é—®é¢˜çš„è¯¦ç»†å»ºè®®

    This endpoint uses LLM with comprehensive De-AIGC knowledge to generate:
    - Detailed diagnosis of the issue
    - Multiple modification strategies
    - A complete modification prompt for other AI tools
    - Priority tips and cautions

    Args:
        request: Issue suggestion request dict é—®é¢˜å»ºè®®è¯·æ±‚å­—å…¸
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Dict with detailed suggestions åŒ…å«è¯¦ç»†å»ºè®®çš„å­—å…¸
    """
    import json
    import httpx
    import re
    from src.config import get_settings
    from src.prompts.structure_deaigc import format_issue_prompt

    try:
        settings = get_settings()

        # Extract request fields
        # æå–è¯·æ±‚å­—æ®µ
        document_id = request.get("documentId", "")
        issue_type = request.get("issueType", "unknown")
        issue_description = request.get("issueDescription", "")
        issue_description_zh = request.get("issueDescriptionZh", "")
        severity = request.get("severity", "medium")
        affected_positions = request.get("affectedPositions", [])
        quick_mode = request.get("quickMode", False)

        # Get document for context
        # è·å–æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
        document = await db.get(Document, document_id) if document_id else None
        document_excerpt = ""
        total_sections = 0
        total_paragraphs = 0
        structure_score = 50
        risk_level = "medium"

        if document:
            document_excerpt = document.original_text[:3000] if document.original_text else ""
            # Get cached analysis if available
            # è·å–ç¼“å­˜çš„åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            if document.structure_analysis_cache:
                cache = document.structure_analysis_cache
                if "step1_1_cache" in cache:
                    step1_cache = cache["step1_1_cache"]
                    total_sections = step1_cache.get("totalSections", len(step1_cache.get("sections", [])))
                    total_paragraphs = step1_cache.get("totalParagraphs", 0)
                    structure_score = step1_cache.get("structureScore", 50)
                    risk_level = step1_cache.get("riskLevel", "medium")

        # Build prompt
        # æ„å»ºæç¤ºè¯
        prompt = format_issue_prompt(
            issue_type=issue_type,
            issue_description=issue_description,
            issue_description_zh=issue_description_zh,
            severity=severity,
            affected_positions=affected_positions,
            total_sections=total_sections,
            total_paragraphs=total_paragraphs,
            structure_score=structure_score,
            risk_level=risk_level,
            document_excerpt=document_excerpt,
            use_quick_mode=quick_mode
        )

        # Call LLM API
        # è°ƒç”¨ LLM API
        response_text = ""

        # Use DashScope (Aliyun)
        # ä½¿ç”¨é˜¿é‡Œäº‘çµç§¯
        if settings.llm_provider == "dashscope" and settings.dashscope_api_key:
            async with httpx.AsyncClient(
                base_url=settings.dashscope_base_url,
                headers={
                    "Authorization": f"Bearer {settings.dashscope_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=90.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.dashscope_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4096,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        # Use Volcengine
        # ä½¿ç”¨ç«å±±å¼•æ“
        elif settings.llm_provider == "volcengine" and settings.volcengine_api_key:
            async with httpx.AsyncClient(
                base_url=settings.volcengine_base_url,
                headers={
                    "Authorization": f"Bearer {settings.volcengine_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=90.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.volcengine_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4096,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        # Use DeepSeek
        # ä½¿ç”¨ DeepSeek
        elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
            async with httpx.AsyncClient(
                base_url=settings.deepseek_base_url,
                headers={
                    "Authorization": f"Bearer {settings.deepseek_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=90.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4096,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        # Use Gemini
        # ä½¿ç”¨ Gemini
        elif settings.llm_provider == "gemini" and settings.gemini_api_key:
            from google import genai
            client = genai.Client(api_key=settings.gemini_api_key)
            gen_response = await client.aio.models.generate_content(
                model=settings.llm_model,
                contents=prompt,
                config={"max_output_tokens": 4096, "temperature": 0.3}
            )
            response_text = gen_response.text

        else:
            raise ValueError("No LLM API configured")

        # Parse response
        # è§£æå“åº”
        response_text = response_text.strip()
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            lines = [l for l in lines if not l.strip().startswith("```")]
            response_text = "\n".join(lines)
        response_text = response_text.strip()

        result = {}
        try:
            result = json.loads(response_text)
        except json.JSONDecodeError:
            # Try to extract JSON
            # å°è¯•æå– JSON
            match = re.search(r'\{[\s\S]*\}', response_text)
            if match:
                try:
                    result = json.loads(match.group())
                except:
                    pass

        # Return response
        # è¿”å›å“åº”
        if quick_mode:
            return {
                "diagnosisZh": result.get("diagnosis_zh", "åˆ†æç»“æœè§£æå¤±è´¥"),
                "quickFixZh": result.get("quick_fix_zh", "å»ºè®®ç§»é™¤æ˜¾æ€§è¿æ¥è¯"),
                "detailedStrategyZh": result.get("detailed_strategy_zh", ""),
                "promptSnippet": result.get("prompt_snippet", ""),
                "estimatedImprovement": result.get("estimated_improvement", 10)
            }

        return {
            "diagnosisZh": result.get("diagnosis_zh", "åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"),
            "strategies": result.get("strategies", []),
            "modificationPrompt": result.get("modification_prompt", ""),
            "priorityTipsZh": result.get("priority_tips_zh", ""),
            "cautionZh": result.get("caution_zh", "è¯·ç¡®ä¿ä¿®æ”¹åä»ç¬¦åˆå­¦æœ¯è§„èŒƒ")
        }

    except Exception as e:
        logger.error(f"Issue suggestion error: {e}")
        return {
            "diagnosisZh": f"ã€åˆ†æå¤±è´¥ã€‘æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
            "strategies": [],
            "modificationPrompt": "",
            "priorityTipsZh": "å»ºè®®ï¼šåˆ é™¤æ®µé¦–æ˜¾æ€§è¿æ¥è¯ï¼Œä½¿ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥",
            "cautionZh": "è¯·ç¡®ä¿ä¿®æ”¹åä»ç¬¦åˆå­¦æœ¯è§„èŒƒ"
        }


# =============================================================================
# Merge Modify Endpoints (Step 1-1 Combined Issue Modification)
# åˆå¹¶ä¿®æ”¹ç«¯ç‚¹ï¼ˆStep 1-1 å¤šé—®é¢˜åˆå¹¶ä¿®æ”¹ï¼‰
# =============================================================================

# Prompt template for generating merge modification prompt
# ç”Ÿæˆåˆå¹¶ä¿®æ”¹æç¤ºè¯çš„æ¨¡æ¿
MERGE_MODIFY_PROMPT_TEMPLATE = """You are a professional academic writing editor. Generate a modification prompt that can be used to fix the following issues in a document.

## TARGET STYLE LEVEL: {colloquialism_level}/10
(0 = Most Academic/Formal, 10 = Most Casual/Conversational)
{style_description}

{previous_improvements}

{semantic_echo_context}

## ISSUES TO ADDRESS:
{issues_list}

**NOTE**: Multiple issues may refer to the same paragraph position (e.g., a connector issue, logic break, and high-risk flag on the same paragraph).
When generating the prompt, instruct to address related issues TOGETHER rather than separately.

## USER'S ADDITIONAL NOTES:
{user_notes}

## YOUR TASK:
Generate a comprehensive, ACTIONABLE prompt that another AI can use to modify the document.
The prompt should:
1. Address ALL selected issues
2. Maintain the target style level ({colloquialism_level}/10)
3. Preserve the original meaning and content
4. Be specific about what to change (remove connectors, restructure sentences, etc.)
5. **CRITICAL: The generated prompt MUST be written in English, regardless of document language**
6. IMPORTANT: Preserve all previous improvements from Step 1-1 (if any)
7. **For connector issues: Include the specific semantic echo replacements provided above**

## OUTPUT FORMAT (JSON):
{{
  "prompt": "Your detailed modification prompt here...",
  "prompt_zh": "ç®€è¦è¯´æ˜è¿™ä¸ªæç¤ºè¯çš„ä½œç”¨",
  "issues_summary_zh": "å·²é€‰é—®é¢˜æ‘˜è¦ï¼š...",
  "estimated_changes": 5
}}

CRITICAL: The prompt must be actionable and specific. Include examples of patterns to remove/change.
CRITICAL: The generated prompt MUST explicitly mention preserving previous improvements to avoid reverting changes.
CRITICAL: If semantic echo replacements are provided, the generated prompt MUST include these specific replacements.
"""

# Prompt template for direct modification (diff-based output to save tokens)
# ç›´æ¥ä¿®æ”¹çš„æç¤ºè¯æ¨¡æ¿ï¼ˆåŸºäºå·®å¼‚è¾“å‡ºä»¥èŠ‚çœ tokensï¼‰
MERGE_MODIFY_APPLY_TEMPLATE = """You are a professional academic writing editor specializing in De-AIGC (removing AI-writing patterns).

## [CRITICAL] LANGUAGE REQUIREMENT:
**Document Language: {doc_language}**
**All modifications MUST be entirely in {doc_language}.**

## DOCUMENT TO MODIFY:
{document_text}

## TARGET STYLE LEVEL: {colloquialism_level}/10
(0 = Most Academic/Formal, 10 = Most Casual/Conversational)
{style_description}

{previous_improvements}

{semantic_echo_context}

## ISSUES TO FIX:
{issues_list}

**NOTE**: Multiple issues may refer to the same paragraph position. Address them TOGETHER in a single modification.

## USER'S ADDITIONAL NOTES:
{user_notes}

## YOUR TASK:
Identify and fix the problematic sentences/paragraphs. For each fix:
1. **CRITICAL: Modifications MUST be entirely in {doc_language}**
2. Maintain the target style level ({colloquialism_level}/10)
3. Preserve the original meaning
4. Make natural-sounding changes that a human would write

## MODIFICATION GUIDELINES:
- Remove explicit connector words (Furthermore, Moreover, Additionally, etc.)
- Use semantic echo instead of connectors
- Break up formulaic sentence patterns
- Vary sentence length and structure
- Avoid AI-typical patterns like "First... Second... Third..."

## OUTPUT FORMAT (JSON) - ONLY OUTPUT THE CHANGES, NOT THE FULL DOCUMENT:
{{
  "modifications": [
    {{
      "original": "The exact original sentence or paragraph that needs modification...",
      "modified": "The modified version of that sentence or paragraph...",
      "reason": "Brief reason for the change"
    }},
    {{
      "original": "Another sentence to modify...",
      "modified": "Its modified version...",
      "reason": "Brief reason"
    }}
  ],
  "changes_summary_zh": "ä¿®æ”¹æ‘˜è¦ï¼š1. ...; 2. ...; 3. ...",
  "changes_count": 5,
  "issues_addressed": ["connector_overuse", "linear_flow"]
}}

CRITICAL RULES:
1. The "original" field MUST be an EXACT substring from the document (copy-paste, no paraphrasing)
2. Only output the parts that need modification, NOT the entire document
3. Each modification should be a complete sentence or small paragraph
4. All "modified" text MUST be in {doc_language} only
5. Keep modifications focused - don't rewrite large sections unnecessarily
"""

# Style level descriptions
# é£æ ¼çº§åˆ«æè¿°
STYLE_LEVEL_DESCRIPTIONS = {
    0: "Extremely formal academic writing. Use precise terminology, complex sentence structures, passive voice, and formal transitions.",
    1: "Very formal academic style. Maintain scholarly tone with occasional active voice.",
    2: "Formal academic writing. Standard academic conventions with clear, professional language.",
    3: "Academic with moderate formality. Clear and professional but not overly stiff.",
    4: "Semi-formal academic. Accessible academic writing with some conversational elements.",
    5: "Balanced style. Mix of academic precision and readable prose.",
    6: "Semi-casual professional. Clear, direct language with minimal jargon.",
    7: "Casual professional. Conversational but still professional.",
    8: "Casual writing. Friendly, conversational tone.",
    9: "Very casual. Informal, personal writing style.",
    10: "Most casual. Highly conversational, like talking to a friend."
}


@router.post("/merge-modify/prompt", response_model=MergeModifyPromptResponse)
async def generate_merge_modify_prompt(
    request: MergeModifyRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Generate a modification prompt for selected issues
    ä¸ºé€‰å®šçš„é—®é¢˜ç”Ÿæˆä¿®æ”¹æç¤ºè¯

    User can copy this prompt to use with other AI tools.
    ç”¨æˆ·å¯ä»¥å¤åˆ¶æ­¤æç¤ºè¯ç”¨äºå…¶ä»–AIå·¥å…·ã€‚

    Args:
        request: Merge modify request åˆå¹¶ä¿®æ”¹è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        MergeModifyPromptResponse with generated prompt åŒ…å«ç”Ÿæˆæç¤ºè¯çš„å“åº”
    """
    import json
    import httpx
    import re
    from src.config import get_settings

    try:
        settings = get_settings()

        # Get document to access Step 1-1 cache
        # è·å–æ–‡æ¡£ä»¥è®¿é—® Step 1-1 ç¼“å­˜
        document = await db.get(Document, request.document_id)

        # Detect document language for context building
        # æ£€æµ‹æ–‡æ¡£è¯­è¨€ä»¥æ„å»ºä¸Šä¸‹æ–‡
        doc_language = "en"  # Default to English
        if document and document.original_text:
            doc_language = _detect_document_language(document.original_text)

        # Get colloquialism level from session
        # ä»ä¼šè¯è·å–å£è¯­åŒ–çº§åˆ«
        colloquialism_level = 3  # Default to semi-formal
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session and session.colloquialism_level is not None:
                colloquialism_level = session.colloquialism_level

        style_description = STYLE_LEVEL_DESCRIPTIONS.get(colloquialism_level, STYLE_LEVEL_DESCRIPTIONS[3])

        # Build previous improvements context from Step 1-1 cache (in document's language)
        # ä» Step 1-1 ç¼“å­˜æ„å»ºä¹‹å‰çš„æ”¹è¿›ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ–‡æ¡£çš„è¯­è¨€ï¼‰
        previous_improvements = _build_previous_improvements_context(document, doc_language)

        # Build semantic echo context from Step 1-2 cache (in document's language)
        # ä» Step 1-2 ç¼“å­˜æ„å»ºè¯­ä¹‰å›å£°ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ–‡æ¡£çš„è¯­è¨€ï¼‰
        semantic_echo_context = _build_semantic_echo_context(document, doc_language)

        # Build issues list in document's language
        # ä½¿ç”¨æ–‡æ¡£è¯­è¨€æ„å»ºé—®é¢˜åˆ—è¡¨
        issues_list = ""
        for i, issue in enumerate(request.selected_issues, 1):
            # Select description based on document language
            # æ ¹æ®æ–‡æ¡£è¯­è¨€é€‰æ‹©æè¿°
            if doc_language == "zh":
                desc = issue.description_zh or issue.description
            else:
                desc = issue.description or issue.description_zh
            issues_list += f"{i}. [{issue.severity.upper()}] {desc}\n"
            if issue.affected_positions:
                if doc_language == "zh":
                    issues_list += f"   å½±å“ä½ç½®: {', '.join(issue.affected_positions)}\n"
                else:
                    issues_list += f"   Affected positions: {', '.join(issue.affected_positions)}\n"

        # Build prompt for LLM
        # æ„å»º LLM æç¤ºè¯
        prompt = MERGE_MODIFY_PROMPT_TEMPLATE.format(
            colloquialism_level=colloquialism_level,
            style_description=style_description,
            previous_improvements=previous_improvements,
            semantic_echo_context=semantic_echo_context,
            issues_list=issues_list,
            user_notes=request.user_notes or "No additional notes"
        )

        # Call LLM
        # è°ƒç”¨ LLM
        response_text = await _call_llm_for_merge_modify(prompt, settings, max_tokens=2048)

        # Parse response
        # è§£æå“åº”
        result = _parse_json_response(response_text)

        return MergeModifyPromptResponse(
            prompt=result.get("prompt", "ç”Ÿæˆæç¤ºè¯å¤±è´¥ï¼Œè¯·é‡è¯•"),
            prompt_zh=result.get("prompt_zh", "ä¿®æ”¹æç¤ºè¯"),
            issues_summary_zh=result.get("issues_summary_zh", f"å·²é€‰æ‹© {len(request.selected_issues)} ä¸ªé—®é¢˜"),
            colloquialism_level=colloquialism_level,
            estimated_changes=result.get("estimated_changes", len(request.selected_issues))
        )

    except Exception as e:
        logger.error(f"Generate merge modify prompt error: {e}")
        # Return a fallback prompt
        # è¿”å›åå¤‡æç¤ºè¯
        fallback_prompt = _generate_fallback_prompt(request.selected_issues, request.user_notes)
        return MergeModifyPromptResponse(
            prompt=fallback_prompt,
            prompt_zh="å·²ç”ŸæˆåŸºç¡€ä¿®æ”¹æç¤ºè¯",
            issues_summary_zh=f"å·²é€‰æ‹© {len(request.selected_issues)} ä¸ªé—®é¢˜",
            colloquialism_level=3,
            estimated_changes=len(request.selected_issues)
        )


@router.post("/merge-modify/apply", response_model=MergeModifyApplyResponse)
async def apply_merge_modify(
    request: MergeModifyRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Apply AI modification to document directly
    ç›´æ¥åº”ç”¨AIä¿®æ”¹åˆ°æ–‡æ¡£

    Args:
        request: Merge modify request åˆå¹¶ä¿®æ”¹è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        MergeModifyApplyResponse with modified document åŒ…å«ä¿®æ”¹åæ–‡æ¡£çš„å“åº”
    """
    import json
    import httpx
    import re
    from src.config import get_settings

    try:
        settings = get_settings()

        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Detect document language to ensure output language consistency
        # æ£€æµ‹æ–‡æ¡£è¯­è¨€ä»¥ç¡®ä¿è¾“å‡ºè¯­è¨€ä¸€è‡´æ€§
        doc_language = _detect_document_language(document.original_text)
        logger.info(f"Detected document language: {doc_language}")

        # Get colloquialism level from session
        # ä»ä¼šè¯è·å–å£è¯­åŒ–çº§åˆ«
        colloquialism_level = 3  # Default to semi-formal
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session and session.colloquialism_level is not None:
                colloquialism_level = session.colloquialism_level

        style_description = STYLE_LEVEL_DESCRIPTIONS.get(colloquialism_level, STYLE_LEVEL_DESCRIPTIONS[3])

        # Build previous improvements context from Step 1-1 cache (in document's language)
        # ä» Step 1-1 ç¼“å­˜æ„å»ºä¹‹å‰çš„æ”¹è¿›ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ–‡æ¡£çš„è¯­è¨€ï¼‰
        previous_improvements = _build_previous_improvements_context(document, doc_language)

        # Build semantic echo context from Step 1-2 cache (in document's language)
        # ä» Step 1-2 ç¼“å­˜æ„å»ºè¯­ä¹‰å›å£°ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ–‡æ¡£çš„è¯­è¨€ï¼‰
        semantic_echo_context = _build_semantic_echo_context(document, doc_language)

        # Build issues list in document's language
        # ä½¿ç”¨æ–‡æ¡£è¯­è¨€æ„å»ºé—®é¢˜åˆ—è¡¨
        issues_list = ""
        for i, issue in enumerate(request.selected_issues, 1):
            # Select description based on document language
            # æ ¹æ®æ–‡æ¡£è¯­è¨€é€‰æ‹©æè¿°
            if doc_language == "zh":
                desc = issue.description_zh or issue.description
            else:
                desc = issue.description or issue.description_zh
            issues_list += f"{i}. [{issue.severity.upper()}] {desc}\n"
            if issue.affected_positions:
                if doc_language == "zh":
                    issues_list += f"   å½±å“ä½ç½®: {', '.join(issue.affected_positions)}\n"
                else:
                    issues_list += f"   Affected: {', '.join(issue.affected_positions)}\n"

        # Build prompt for LLM
        # æ„å»º LLM æç¤ºè¯
        # Determine language instruction for prompt
        # ç¡®å®špromptä¸­çš„è¯­è¨€æŒ‡ä»¤
        if doc_language == "zh":
            language_instruction = "Chinese (ä¸­æ–‡)"
            user_notes_default = "æ— é™„åŠ è¯´æ˜"
        else:
            language_instruction = "English"
            user_notes_default = "No additional notes"

        # Input limit: 180000 chars â‰ˆ 30k words â‰ˆ 40k tokens (within 64K context window)
        # è¾“å…¥é™åˆ¶ï¼š180000å­—ç¬¦ â‰ˆ 30000å•è¯ â‰ˆ 40000 tokensï¼ˆåœ¨64Kä¸Šä¸‹æ–‡çª—å£å†…ï¼‰
        prompt = MERGE_MODIFY_APPLY_TEMPLATE.format(
            document_text=document.original_text[:180000],  # ~30k words, ~40k tokens input
            colloquialism_level=colloquialism_level,
            style_description=style_description,
            previous_improvements=previous_improvements,
            semantic_echo_context=semantic_echo_context,
            issues_list=issues_list,
            user_notes=request.user_notes or user_notes_default,
            doc_language=language_instruction
        )

        # Call LLM - now outputs diff only, so max_tokens can be smaller
        # è°ƒç”¨ LLM - ç°åœ¨åªè¾“å‡º diffï¼Œæ‰€ä»¥ max_tokens å¯ä»¥æ›´å°
        response_text = await _call_llm_for_merge_modify(prompt, settings, max_tokens=8192, timeout=180.0)

        # Parse response
        # è§£æå“åº”
        result = _parse_json_response(response_text)

        # Apply modifications (diff) to original document
        # å°†ä¿®æ”¹ï¼ˆå·®å¼‚ï¼‰åº”ç”¨åˆ°åŸæ–‡æ¡£
        modified_text = document.original_text
        modifications = result.get("modifications", [])
        applied_count = 0

        for mod in modifications:
            original = mod.get("original", "")
            modified = mod.get("modified", "")
            if original and modified and original in modified_text:
                modified_text = modified_text.replace(original, modified, 1)  # Replace first occurrence only
                applied_count += 1
                logger.debug(f"Applied modification: {original[:50]}... -> {modified[:50]}...")
            elif original and modified:
                # Try fuzzy match if exact match fails (handle minor whitespace differences)
                # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†ç»†å¾®çš„ç©ºç™½å·®å¼‚ï¼‰
                # Normalize whitespace for matching
                original_normalized = re.sub(r'\s+', ' ', original.strip())
                text_normalized = re.sub(r'\s+', ' ', modified_text)
                if original_normalized in text_normalized:
                    # Find original position in actual text
                    # åœ¨å®é™…æ–‡æœ¬ä¸­æ‰¾åˆ°åŸå§‹ä½ç½®
                    pattern = re.sub(r'\s+', r'\\s+', re.escape(original.strip()))
                    match = re.search(pattern, modified_text)
                    if match:
                        modified_text = modified_text[:match.start()] + modified + modified_text[match.end():]
                        applied_count += 1
                        logger.debug(f"Applied fuzzy modification: {original[:50]}...")
                else:
                    logger.warning(f"Could not find original text: {original[:100]}...")

        return MergeModifyApplyResponse(
            modified_text=modified_text,
            changes_summary_zh=result.get("changes_summary_zh", f"å·²åº”ç”¨ {applied_count} å¤„ä¿®æ”¹"),
            changes_count=applied_count,
            issues_addressed=result.get("issues_addressed", []),
            remaining_attempts=2,  # 3 total, 1 used
            colloquialism_level=colloquialism_level
        )

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_type = type(e).__name__
        error_msg = str(e) if str(e) else f"{error_type} (no message)"
        logger.error(f"Apply merge modify error [{error_type}]: {error_msg}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ä¿®æ”¹å¤±è´¥: {error_type} - {error_msg}")


async def _call_llm_for_merge_modify(prompt: str, settings, max_tokens: int = 4096, timeout: float = 90.0) -> str:
    """
    Call LLM API for merge modification
    è°ƒç”¨ LLM API è¿›è¡Œåˆå¹¶ä¿®æ”¹
    """
    import httpx

    logger.info(f"LLM call: provider={settings.llm_provider}, prompt_len={len(prompt)}, max_tokens={max_tokens}, timeout={timeout}")

    # Use DashScope (Aliyun)
    # ä½¿ç”¨é˜¿é‡Œäº‘çµç§¯
    if settings.llm_provider == "dashscope" and settings.dashscope_api_key:
        try:
            async with httpx.AsyncClient(
                base_url=settings.dashscope_base_url,
                headers={
                    "Authorization": f"Bearer {settings.dashscope_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=httpx.Timeout(timeout, connect=30.0),
                trust_env=False
            ) as client:
                logger.info(f"Calling DashScope API with model={settings.dashscope_model}")
                response = await client.post("/chat/completions", json={
                    "model": settings.dashscope_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                logger.info(f"DashScope response received, usage: {data.get('usage', {})}")
                return data["choices"][0]["message"]["content"]
        except httpx.TimeoutException as e:
            logger.error(f"DashScope API timeout after {timeout}s: {type(e).__name__}")
            raise TimeoutError(f"DashScope API è¯·æ±‚è¶…æ—¶ ({timeout}ç§’)")
        except httpx.HTTPStatusError as e:
            logger.error(f"DashScope API HTTP error: {e.response.status_code} - {e.response.text[:500]}")
            raise ValueError(f"DashScope API é”™è¯¯: HTTP {e.response.status_code}")
        except Exception as e:
            logger.error(f"DashScope API error: {type(e).__name__} - {e}")
            raise

    # Use Volcengine
    # ä½¿ç”¨ç«å±±å¼•æ“
    elif settings.llm_provider == "volcengine" and settings.volcengine_api_key:
        async with httpx.AsyncClient(
            base_url=settings.volcengine_base_url,
            headers={
                "Authorization": f"Bearer {settings.volcengine_api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.volcengine_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.3
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use DeepSeek
    # ä½¿ç”¨ DeepSeek
    elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
        async with httpx.AsyncClient(
            base_url=settings.deepseek_base_url,
            headers={
                "Authorization": f"Bearer {settings.deepseek_api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.llm_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.3
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use Gemini
    # ä½¿ç”¨ Gemini
    elif settings.llm_provider == "gemini" and settings.gemini_api_key:
        from google import genai
        client = genai.Client(api_key=settings.gemini_api_key)
        response = await client.aio.models.generate_content(
            model=settings.llm_model,
            contents=prompt,
            config={"max_output_tokens": max_tokens, "temperature": 0.3}
        )
        return response.text

    else:
        raise ValueError("No LLM API configured")


def _parse_json_response(response: str) -> dict:
    """
    Parse LLM response to JSON
    è§£æ LLM å“åº”ä¸º JSON
    """
    import json
    import re

    # Clean response
    # æ¸…ç†å“åº”
    response = response.strip()
    if response.startswith("```"):
        lines = response.split("\n")
        lines = [l for l in lines if not l.strip().startswith("```")]
        response = "\n".join(lines)
    response = response.strip()

    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # Try to extract JSON from response
        # å°è¯•ä»å“åº”ä¸­æå– JSON
        match = re.search(r'\{[\s\S]*\}', response)
        if match:
            try:
                return json.loads(match.group())
            except:
                pass
        return {}


def _detect_document_language(text: str) -> str:
    """
    Detect the primary language of a document.
    æ£€æµ‹æ–‡æ¡£çš„ä¸»è¦è¯­è¨€ã€‚

    Args:
        text: Document text to analyze

    Returns:
        "zh" for Chinese, "en" for English
    """
    if not text:
        return "en"

    # Count Chinese characters vs total characters
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦ä¸æ€»å­—ç¬¦çš„æ¯”ä¾‹
    chinese_count = 0
    total_count = 0

    for char in text:
        if '\u4e00' <= char <= '\u9fff':  # CJK Unified Ideographs
            chinese_count += 1
            total_count += 1
        elif char.isalpha():  # Latin alphabet
            total_count += 1

    if total_count == 0:
        return "en"

    # If more than 10% of alphabetic characters are Chinese, treat as Chinese
    # å¦‚æœè¶…è¿‡10%çš„å­—æ¯å­—ç¬¦æ˜¯ä¸­æ–‡ï¼Œåˆ™è§†ä¸ºä¸­æ–‡æ–‡æ¡£
    chinese_ratio = chinese_count / total_count
    return "zh" if chinese_ratio > 0.1 else "en"


def _build_previous_improvements_context(document, doc_language: str = "en") -> str:
    """
    Build context about previous improvements from Step 1-1 analysis.
    ä» Step 1-1 åˆ†æç»“æœæ„å»ºä¹‹å‰æ”¹è¿›çš„ä¸Šä¸‹æ–‡ã€‚

    This helps LLM understand what changes were already suggested/made
    so it doesn't revert them in subsequent modifications.
    è¿™å¸®åŠ© LLM äº†è§£å·²ç»å»ºè®®/å®Œæˆçš„æ”¹è¿›ï¼Œé¿å…åœ¨åç»­ä¿®æ”¹ä¸­æ’¤é”€å®ƒä»¬ã€‚

    Args:
        document: Document model with structure_analysis_cache
        doc_language: Document language ("en" or "zh")

    Returns:
        Formatted string describing previous improvements (in document's language)
    """
    if not document or not document.structure_analysis_cache:
        return ""

    cache = document.structure_analysis_cache
    improvements = []

    # Extract Step 1-1 issues that were identified
    # æå– Step 1-1 ä¸­è¯†åˆ«çš„é—®é¢˜
    step1_1_cache = cache.get("step1_1_cache", {})
    if step1_1_cache:
        structure_issues = step1_1_cache.get("structureIssues") or step1_1_cache.get("structure_issues", [])
        if structure_issues:
            for issue in structure_issues[:5]:  # Limit to top 5 to avoid prompt bloat
                # Select description based on document language
                # æ ¹æ®æ–‡æ¡£è¯­è¨€é€‰æ‹©æè¿°
                if doc_language == "zh":
                    desc = issue.get("descriptionZh") or issue.get("description_zh") or issue.get("description", "")
                else:
                    desc = issue.get("description") or issue.get("descriptionZh") or issue.get("description_zh", "")
                if desc:
                    improvements.append(f"- {desc}")

        # Include style analysis context
        # åŒ…å«é£æ ¼åˆ†æä¸Šä¸‹æ–‡
        style_analysis = step1_1_cache.get("styleAnalysis") or step1_1_cache.get("style_analysis", {})
        if style_analysis:
            if doc_language == "zh":
                style_name = style_analysis.get("styleNameZh") or style_analysis.get("style_name_zh", "")
                if style_name:
                    improvements.append(f"- æ–‡æ¡£åŸå§‹é£æ ¼: {style_name}")
            else:
                style_name = style_analysis.get("styleName") or style_analysis.get("style_name", "")
                if style_name:
                    improvements.append(f"- Document original style: {style_name}")

    # Extract Step 1-2 issues if available
    # æå– Step 1-2 çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
    step1_2_cache = cache.get("step1_2_cache", {})
    if step1_2_cache:
        # Include relationship issues context
        # åŒ…å«å…³ç³»é—®é¢˜ä¸Šä¸‹æ–‡
        relationship_issues = step1_2_cache.get("relationshipIssues") or step1_2_cache.get("relationship_issues", [])
        if relationship_issues:
            for issue in relationship_issues[:3]:  # Limit to top 3
                # Select description based on document language
                # æ ¹æ®æ–‡æ¡£è¯­è¨€é€‰æ‹©æè¿°
                if doc_language == "zh":
                    desc = issue.get("descriptionZh") or issue.get("description_zh") or issue.get("description", "")
                else:
                    desc = issue.get("description") or issue.get("descriptionZh") or issue.get("description_zh", "")
                if desc:
                    improvements.append(f"- {desc}")

    if not improvements:
        return ""

    # Build the context block based on document language
    # æ ¹æ®æ–‡æ¡£è¯­è¨€æ„å»ºä¸Šä¸‹æ–‡å—
    improvements_text = "\n".join(improvements)

    if doc_language == "zh":
        return f"""## [IMPORTANT] ä¹‹å‰åˆ†æçš„ä¸Šä¸‹æ–‡ï¼ˆå¿…é¡»ä¿ç•™ï¼‰:
åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­å·²å¯¹æ–‡æ¡£è¿›è¡Œäº†åˆ†æï¼Œè¯†åˆ«å‡ºä»¥ä¸‹é—®é¢˜/æ”¹è¿›ç‚¹ï¼š

{improvements_text}

**å…³é”®æŒ‡ä»¤:**
- å¿…é¡»ä¿ç•™å·²æ ¹æ®è¿™äº›é—®é¢˜æ‰€åšçš„æ”¹è¿›
- ä¸è¦å°†æ–‡æ¡£æ¢å¤åˆ°è¢«æ ‡è®°ä¸ºæœ‰é—®é¢˜çš„æ¨¡å¼
- ä»…å¯¹å½“å‰é—®é¢˜è¿›è¡Œæ–°çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒä¹‹å‰çš„æ›´æ”¹ä¸å˜
"""
    else:
        return f"""## [IMPORTANT] PREVIOUS ANALYSIS CONTEXT (MUST PRESERVE):
The document has been analyzed in previous steps. The following issues/improvements were identified:

{improvements_text}

**CRITICAL INSTRUCTION:**
- You MUST preserve any improvements already made based on these issues
- DO NOT revert the document to patterns that were flagged as problematic
- Only make NEW improvements for the current issues, while keeping previous changes intact
"""


def _build_semantic_echo_context(document, doc_language: str = "en") -> str:
    """
    Build semantic echo replacement context from Step 1-2 analysis.
    ä» Step 1-2 åˆ†æç»“æœæ„å»ºè¯­ä¹‰å›å£°æ›¿æ¢ä¸Šä¸‹æ–‡ã€‚

    This provides LLM with specific replacement examples for explicit connectors.
    è¿™ä¸º LLM æä¾›æ˜¾æ€§è¿æ¥è¯çš„å…·ä½“æ›¿æ¢ç¤ºä¾‹ã€‚

    Args:
        document: Document model with structure_analysis_cache
        doc_language: Document language ("en" or "zh")

    Returns:
        Formatted string with semantic echo replacements (in document's language)
    """
    if not document or not document.structure_analysis_cache:
        return ""

    cache = document.structure_analysis_cache
    replacements = []
    has_chinese_replacement = False  # Track if any cached replacement is in Chinese

    def _is_chinese_text(text: str) -> bool:
        """Check if text contains significant Chinese characters"""
        if not text:
            return False
        chinese_count = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        return chinese_count > len(text) * 0.1

    # Extract semantic echo replacements from Step 1-2 cache
    # ä» Step 1-2 ç¼“å­˜æå–è¯­ä¹‰å›å£°æ›¿æ¢
    step1_2_cache = cache.get("step1_2_cache", {})
    if step1_2_cache:
        # Get explicit connectors with replacements
        # è·å–å¸¦æœ‰æ›¿æ¢çš„æ˜¾æ€§è¿æ¥è¯
        explicit_connectors = step1_2_cache.get("explicit_connectors") or step1_2_cache.get("explicitConnectors", [])
        for conn in explicit_connectors:
            word = conn.get("word", "")
            position = conn.get("position", "")
            current_opening = conn.get("current_opening") or conn.get("currentOpening", "")
            replacement = conn.get("semantic_echo_replacement") or conn.get("semanticEchoReplacement", "")
            prev_concepts = conn.get("prev_key_concepts") or conn.get("prevKeyConcepts", [])
            # Select explanation based on language
            # æ ¹æ®è¯­è¨€é€‰æ‹©è¯´æ˜
            if doc_language == "zh":
                explanation = conn.get("replacement_explanation_zh") or conn.get("replacementExplanationZh", "")
            else:
                explanation = conn.get("replacement_explanation") or conn.get("replacementExplanation", "") or \
                              conn.get("replacement_explanation_zh") or conn.get("replacementExplanationZh", "")

            if current_opening and replacement:
                concepts_str = ", ".join(prev_concepts) if prev_concepts else "N/A"

                # Check if replacement language matches document language
                # æ£€æŸ¥æ›¿æ¢å†…å®¹çš„è¯­è¨€æ˜¯å¦ä¸æ–‡æ¡£è¯­è¨€åŒ¹é…
                replacement_is_chinese = _is_chinese_text(replacement)

                if doc_language == "zh":
                    replacements.append(f"""
### ä½ç½® {position}: "{word}"
- **åŸæ–‡**: {current_opening}
- **å‰æ®µå…³é”®æ¦‚å¿µ**: {concepts_str}
- **è¯­ä¹‰å›å£°æ›¿æ¢**: {replacement}
- **è¯´æ˜**: {explanation}""")
                else:
                    # If document is English but replacement is Chinese, do NOT include Chinese text
                    # å¦‚æœæ–‡æ¡£æ˜¯è‹±æ–‡ä½†æ›¿æ¢æ˜¯ä¸­æ–‡ï¼Œä¸åŒ…å«ä¸­æ–‡æ–‡æœ¬
                    if replacement_is_chinese:
                        has_chinese_replacement = True
                        # Only provide key concepts, let LLM generate English replacement
                        # åªæä¾›å…³é”®æ¦‚å¿µï¼Œè®©LLMç”Ÿæˆè‹±æ–‡æ›¿æ¢
                        replacements.append(f"""
### Position {position}: "{word}"
- **Original text to modify**: {current_opening}
- **Connector to REMOVE**: "{word}"
- **Key concepts from previous paragraph**: {concepts_str}
- **YOUR TASK**: Generate a NEW English sentence that:
  1. Removes the connector "{word}" completely
  2. Uses one of the key concepts [{concepts_str}] at the beginning to create natural flow
  3. Maintains the original meaning
  4. Output MUST be entirely in English""")
                    else:
                        replacements.append(f"""
### Position {position}: "{word}"
- **Original**: {current_opening}
- **Previous paragraph key concepts**: {concepts_str}
- **Semantic echo replacement**: {replacement}
- **Explanation**: {explanation}""")

    # Also check Step 1-1 for any connector issues with replacements
    # åŒæ—¶æ£€æŸ¥ Step 1-1 æ˜¯å¦æœ‰å¸¦æ›¿æ¢çš„è¿æ¥è¯é—®é¢˜
    step1_1_cache = cache.get("step1_1_cache", {})
    if step1_1_cache:
        structure_issues = step1_1_cache.get("structureIssues") or step1_1_cache.get("structure_issues", [])
        for issue in structure_issues:
            issue_type = issue.get("type", "")
            if issue_type == "explicit_connector":
                original = issue.get("originalText") or issue.get("original_text", "")
                replacement = issue.get("semanticEchoReplacement") or issue.get("semantic_echo_replacement", "")
                if original and replacement and len(replacements) < 10:  # Limit total
                    replacement_is_chinese = _is_chinese_text(replacement)
                    if doc_language == "zh":
                        replacements.append(f"""
### è¿æ¥è¯é—®é¢˜
- **åŸæ–‡**: {original}
- **è¯­ä¹‰å›å£°æ›¿æ¢**: {replacement}""")
                    else:
                        # If document is English but replacement is Chinese, do NOT include Chinese text
                        # å¦‚æœæ–‡æ¡£æ˜¯è‹±æ–‡ä½†æ›¿æ¢æ˜¯ä¸­æ–‡ï¼Œä¸åŒ…å«ä¸­æ–‡æ–‡æœ¬
                        if replacement_is_chinese:
                            has_chinese_replacement = True
                            # Extract connector word from original if possible (common connectors)
                            # å°è¯•ä»åŸæ–‡ä¸­æå–è¿æ¥è¯
                            connector_patterns = ["Furthermore", "Moreover", "Additionally", "However", "Therefore", "Thus", "Hence", "Consequently", "In addition", "On the other hand"]
                            detected_connector = ""
                            for pattern in connector_patterns:
                                if pattern.lower() in original.lower():
                                    detected_connector = pattern
                                    break

                            replacements.append(f"""
### Connector Issue (from structure analysis)
- **Original text to modify**: {original}
- **Connector to REMOVE**: "{detected_connector if detected_connector else 'explicit connector'}"
- **YOUR TASK**: Generate a NEW English sentence that:
  1. Removes any explicit connector (Furthermore, Moreover, Additionally, etc.) completely
  2. Starts with a reference to concepts from previous context to create natural flow
  3. Maintains the original meaning
  4. Output MUST be entirely in English - NO Chinese text allowed""")
                        else:
                            replacements.append(f"""
### Connector Issue
- **Original**: {original}
- **Semantic echo replacement**: {replacement}""")

    if not replacements:
        return ""

    # Build the context block based on document language
    # æ ¹æ®æ–‡æ¡£è¯­è¨€æ„å»ºä¸Šä¸‹æ–‡å—
    replacements_text = "\n".join(replacements)

    if doc_language == "zh":
        return f"""## ğŸ”„ è¯­ä¹‰å›å£°æ›¿æ¢ (å¿…é¡»ä½¿ç”¨):
ä»¥ä¸‹æ˜¯é’ˆå¯¹æ˜¾æ€§è¿æ¥è¯ç”Ÿæˆçš„å…·ä½“æ›¿æ¢æ–¹æ¡ˆã€‚**æ‚¨å¿…é¡»åœ¨ä¿®æ”¹åçš„æ–‡æœ¬ä¸­ä½¿ç”¨è¿™äº›æ›¿æ¢ã€‚**

{replacements_text}

**ä½¿ç”¨æ–¹æ³•:**
1. åœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°æ¯ä¸ªåŸæ–‡
2. ç”¨è¯­ä¹‰å›å£°æ›¿æ¢è¿›è¡Œæ›¿æ¢
3. æ›¿æ¢ä½¿ç”¨å‰ä¸€æ®µçš„å…³é”®æ¦‚å¿µæ¥åˆ›å»ºè‡ªç„¶çš„è¡”æ¥
4. ä¸è¦å†æ·»åŠ ä»»ä½•æ˜¾æ€§è¿æ¥è¯
"""
    else:
        # If there are Chinese replacements, add special instructions
        # å¦‚æœæœ‰ä¸­æ–‡æ›¿æ¢ï¼Œæ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤
        if has_chinese_replacement:
            return f"""## ğŸ”„ SEMANTIC ECHO REPLACEMENTS:
The following connector issues need to be addressed. Some cached replacements are in Chinese and MUST be translated/regenerated in English.
**CRITICAL: The output document MUST be entirely in English. Do NOT include any Chinese text.**

{replacements_text}

**HOW TO USE:**
1. Find each original text in the document
2. For English replacements: use them directly
3. For Chinese replacements: Generate a NEW English replacement using the key concepts
4. The replacement should use concepts from the previous paragraph to create natural flow
5. Do NOT add back any explicit connectors
"""
        else:
            return f"""## ğŸ”„ SEMANTIC ECHO REPLACEMENTS (MUST USE):
The following specific replacements have been generated for explicit connector words.
**YOU MUST use these exact replacements in the modified text.**

{replacements_text}

**HOW TO USE:**
1. Find each original text in the document
2. Replace it with the semantic echo replacement
3. The replacement uses key concepts from the previous paragraph to create natural flow
4. Do NOT add back any explicit connectors
"""


def _generate_fallback_prompt(selected_issues: list, user_notes: str = None) -> str:
    """
    Generate a fallback prompt when LLM fails
    å½“ LLM å¤±è´¥æ—¶ç”Ÿæˆåå¤‡æç¤ºè¯
    """
    issues_text = "\n".join([
        f"- {issue.description_zh}" for issue in selected_issues
    ])

    prompt = f"""è¯·ä¿®æ”¹ä»¥ä¸‹æ–‡æ¡£ï¼Œè§£å†³è¿™äº›é—®é¢˜ï¼š

{issues_text}

ä¿®æ”¹è¦æ±‚ï¼š
1. åˆ é™¤æ˜¾æ€§è¿æ¥è¯ï¼ˆå¦‚ï¼šFurthermore, Moreover, æ­¤å¤–, å¦å¤–ç­‰ï¼‰
2. ä½¿ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥ä¸Šä¸‹æ–‡
3. æ‰“ç ´å…¬å¼åŒ–çš„å¥å­ç»“æ„
4. ä¿æŒåŸæ–‡çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
5. è¾“å‡ºè¯­è¨€ä¸åŸæ–‡ä¿æŒä¸€è‡´

"""
    if user_notes:
        prompt += f"ç”¨æˆ·æ³¨æ„äº‹é¡¹ï¼š{user_notes}\n"

    return prompt


# =============================================================================
# Step 1-2 Two-Phase Enhancement: Paragraph Length Distribution
# Step 1-2 ä¸¤é˜¶æ®µå¢å¼ºï¼šæ®µè½é•¿åº¦åˆ†å¸ƒ
# =============================================================================

from src.api.schemas import (
    ParagraphLengthAnalysisRequest,
    ParagraphLengthAnalysisResponse,
    ParagraphLengthStrategyItem,
    ParagraphLengthInfo,
    ApplyParagraphStrategiesRequest,
    ApplyParagraphStrategiesResponse,
    SelectedStrategy,
)
from src.core.analyzer.smart_structure import (
    analyze_paragraph_length_distribution,
    analyze_paragraph_length_distribution_async
)


@router.post("/paragraph-length/analyze", response_model=ParagraphLengthAnalysisResponse)
async def analyze_paragraph_length(
    request: ParagraphLengthAnalysisRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Phase 1: Analyze paragraph length distribution and suggest strategies
    ç¬¬ä¸€é˜¶æ®µï¼šåˆ†ææ®µè½é•¿åº¦åˆ†å¸ƒå¹¶å»ºè®®ç­–ç•¥

    This endpoint analyzes the paragraph lengths in the document and suggests
    strategies (merge, expand, split) to improve the CV (coefficient of variation)
    for a more human-like distribution.
    æ­¤ç«¯ç‚¹åˆ†ææ–‡æ¡£ä¸­çš„æ®µè½é•¿åº¦ï¼Œå¹¶å»ºè®®ç­–ç•¥ï¼ˆåˆå¹¶ã€æ‰©å±•ã€æ‹†åˆ†ï¼‰
    ä»¥æ”¹å–„CVï¼ˆå˜å¼‚ç³»æ•°ï¼‰ï¼Œä½¿åˆ†å¸ƒæ›´åƒäººç±»å†™ä½œã€‚

    Returns strategies that the user can multi-select.
    If user selects "expand", they will need to provide new content.
    è¿”å›ç”¨æˆ·å¯ä»¥å¤šé€‰çš„ç­–ç•¥ã€‚
    å¦‚æœç”¨æˆ·é€‰æ‹©"æ‰©å±•"ï¼Œéœ€è¦æä¾›æ–°å†…å®¹ã€‚

    Args:
        request: Contains document_id åŒ…å«æ–‡æ¡£ID

    Returns:
        ParagraphLengthAnalysisResponse with statistics and strategies
        åŒ…å«ç»Ÿè®¡æ•°æ®å’Œç­–ç•¥çš„å“åº”
    """
    try:
        # Get document from database
        # ä»æ•°æ®åº“è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if we have cached Step 1-1 result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ Step 1-1 ç»“æœ
        step1_1_key = "step1_1_smart_structure"
        if not document.structure_analysis_cache or step1_1_key not in document.structure_analysis_cache:
            # Run Step 1-1 first
            # å…ˆè¿è¡Œ Step 1-1
            logger.info(f"Running Step 1-1 for document {request.document_id}")
            step1_1_result = await smart_analyzer.analyze(document.original_text)
            if not document.structure_analysis_cache:
                document.structure_analysis_cache = {}
            document.structure_analysis_cache[step1_1_key] = step1_1_result
            flag_modified(document, 'structure_analysis_cache')
            await db.commit()
        else:
            step1_1_result = document.structure_analysis_cache[step1_1_key]

        # Analyze paragraph length distribution using async LLM-based semantic analysis
        # ä½¿ç”¨å¼‚æ­¥LLMè¯­ä¹‰åˆ†æè¿›è¡Œæ®µè½é•¿åº¦åˆ†å¸ƒåˆ†æ
        analysis = await analyze_paragraph_length_distribution_async(step1_1_result)

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        paragraph_lengths = [
            ParagraphLengthInfo(
                position=p["position"],
                word_count=p["word_count"],
                section=str(p["section"]),
                summary=p.get("summary", ""),
                summary_zh=p.get("summary_zh", "")
            )
            for p in analysis.paragraph_lengths
        ]

        strategies = [
            ParagraphLengthStrategyItem(
                strategy_type=s.strategy_type,
                target_positions=s.target_positions,
                description=s.description,
                description_zh=s.description_zh,
                reason=s.reason,
                reason_zh=s.reason_zh,
                priority=s.priority,
                expand_suggestion=s.expand_suggestion,
                expand_suggestion_zh=s.expand_suggestion_zh,
                semantic_relation=s.semantic_relation,
                semantic_relation_zh=s.semantic_relation_zh,
                split_points=s.split_points,
                split_points_zh=s.split_points_zh
            )
            for s in analysis.strategies
        ]

        return ParagraphLengthAnalysisResponse(
            paragraph_lengths=paragraph_lengths,
            mean_length=analysis.mean_length,
            std_dev=analysis.std_dev,
            cv=analysis.cv,
            is_uniform=analysis.is_uniform,
            human_like_cv_target=analysis.human_like_cv_target,
            strategies=strategies,
            summary=analysis.summary,
            summary_zh=analysis.summary_zh
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Paragraph length analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/paragraph-length/apply", response_model=ApplyParagraphStrategiesResponse)
async def apply_paragraph_strategies(
    request: ApplyParagraphStrategiesRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Phase 2: Apply user-selected paragraph length strategies
    ç¬¬äºŒé˜¶æ®µï¼šåº”ç”¨ç”¨æˆ·é€‰æ‹©çš„æ®µè½é•¿åº¦ç­–ç•¥

    Applies the selected strategies (merge, expand, split) to modify the document.
    For expand strategies, uses the user-provided text.
    åº”ç”¨é€‰å®šçš„ç­–ç•¥ï¼ˆåˆå¹¶ã€æ‰©å±•ã€æ‹†åˆ†ï¼‰æ¥ä¿®æ”¹æ–‡æ¡£ã€‚
    å¯¹äºæ‰©å±•ç­–ç•¥ï¼Œä½¿ç”¨ç”¨æˆ·æä¾›çš„æ–‡æœ¬ã€‚

    Args:
        request: Contains document_id, session_id, and selected strategies

    Returns:
        ApplyParagraphStrategiesResponse with modified text
    """
    try:
        # Get document from database
        # ä»æ•°æ®åº“è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Get colloquialism level from session if available
        # å¦‚æœå¯ç”¨ï¼Œä»ä¼šè¯è·å–å£è¯­åŒ–ç¨‹åº¦
        colloquialism_level = 4  # default
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session:
                colloquialism_level = session.colloquialism_level

        # Get cached Step 1-1 result for paragraph structure
        # è·å–ç¼“å­˜çš„ Step 1-1 ç»“æœä»¥äº†è§£æ®µè½ç»“æ„
        step1_1_key = "step1_1_smart_structure"
        if not document.structure_analysis_cache or step1_1_key not in document.structure_analysis_cache:
            raise HTTPException(
                status_code=400,
                detail="Please run paragraph length analysis first (Phase 1)"
            )
        step1_1_result = document.structure_analysis_cache[step1_1_key]

        # Build modification prompt based on selected strategies
        # æ ¹æ®é€‰æ‹©çš„ç­–ç•¥æ„å»ºä¿®æ”¹æç¤ºè¯
        strategy_instructions = []
        for strategy in request.selected_strategies:
            if strategy.strategy_type == "merge":
                positions = " and ".join(strategy.target_positions)
                strategy_instructions.append(
                    f"MERGE paragraphs {positions} into a single paragraph. "
                    f"Combine their content naturally while maintaining logical flow."
                )
            elif strategy.strategy_type == "expand":
                positions = ", ".join(strategy.target_positions)
                if strategy.expand_text:
                    strategy_instructions.append(
                        f"EXPAND paragraph(s) {positions} by incorporating the following content: "
                        f"\"{strategy.expand_text}\". "
                        f"Integrate this content naturally into the existing paragraph(s)."
                    )
                else:
                    strategy_instructions.append(
                        f"EXPAND paragraph(s) {positions} with additional supporting details, "
                        f"examples, or data to increase their length."
                    )
            elif strategy.strategy_type == "split":
                positions = ", ".join(strategy.target_positions)
                strategy_instructions.append(
                    f"SPLIT paragraph(s) {positions} into two separate paragraphs. "
                    f"Find a natural breaking point and ensure each resulting paragraph has a clear focus."
                )

        if not strategy_instructions:
            return ApplyParagraphStrategiesResponse(
                modified_text=document.original_text,
                changes_summary_zh="æ²¡æœ‰é€‰æ‹©ä»»ä½•ç­–ç•¥",
                strategies_applied=0,
                new_cv=None
            )

        # Generate modification prompt
        # ç”Ÿæˆä¿®æ”¹æç¤ºè¯
        prompt = f"""You are a professional academic writing editor. Modify the following document according to the specified paragraph strategies.

## DOCUMENT:
{document.original_text}

## PARAGRAPH STRATEGIES TO APPLY:
{chr(10).join(f"{i+1}. {instr}" for i, instr in enumerate(strategy_instructions))}

## REQUIREMENTS:
1. Apply ALL the specified strategies
2. Maintain academic writing style (colloquialism level: {colloquialism_level}/10)
3. Preserve the original meaning and key information
4. Keep the document language consistent with the original
5. Output ONLY the modified document text, no explanations

## OUTPUT:
Return the complete modified document text."""

        # Call LLM to apply modifications
        # è°ƒç”¨LLMåº”ç”¨ä¿®æ”¹
        from src.config import get_settings
        settings = get_settings()
        response_text = ""

        # Use configured LLM provider
        # ä½¿ç”¨é…ç½®çš„LLMæä¾›è€…
        if settings.llm_provider == "openai" and settings.openai_api_key:
            async with httpx.AsyncClient(
                base_url=settings.openai_base_url,
                headers={
                    "Authorization": f"Bearer {settings.openai_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=120.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 8192,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        elif settings.llm_provider == "volcengine" and settings.volcengine_api_key:
            async with httpx.AsyncClient(
                base_url=settings.volcengine_base_url,
                headers={
                    "Authorization": f"Bearer {settings.volcengine_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=120.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.volcengine_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 8192,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
            async with httpx.AsyncClient(
                base_url=settings.deepseek_base_url,
                headers={
                    "Authorization": f"Bearer {settings.deepseek_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=120.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 8192,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        elif settings.llm_provider == "gemini" and settings.gemini_api_key:
            from google import genai
            client = genai.Client(api_key=settings.gemini_api_key)
            gen_response = await client.aio.models.generate_content(
                model=settings.llm_model,
                contents=prompt,
                config={"max_output_tokens": 8192, "temperature": 0.3}
            )
            response_text = gen_response.text

        else:
            raise ValueError("No LLM API configured")

        # Clean up response
        # æ¸…ç†å“åº”
        modified_text = response_text.strip()
        if modified_text.startswith("```"):
            lines = modified_text.split("\n")
            lines = [l for l in lines if not l.strip().startswith("```")]
            modified_text = "\n".join(lines).strip()

        # Calculate new CV (re-analyze the modified text)
        # è®¡ç®—æ–°çš„CVï¼ˆé‡æ–°åˆ†æä¿®æ”¹åçš„æ–‡æœ¬ï¼‰
        # For now, we skip re-analysis to save time. Frontend can trigger re-analysis if needed.
        # ç›®å‰è·³è¿‡é‡æ–°åˆ†æä»¥èŠ‚çœæ—¶é—´ã€‚å¦‚æœéœ€è¦ï¼Œå‰ç«¯å¯ä»¥è§¦å‘é‡æ–°åˆ†æã€‚

        # Generate summary
        # ç”Ÿæˆæ‘˜è¦
        strategy_types = [s.strategy_type for s in request.selected_strategies]
        changes_summary = f"å·²åº”ç”¨ {len(request.selected_strategies)} ä¸ªç­–ç•¥ï¼š"
        if "merge" in strategy_types:
            changes_summary += "åˆå¹¶æ®µè½ã€"
        if "expand" in strategy_types:
            changes_summary += "æ‰©å±•æ®µè½ã€"
        if "split" in strategy_types:
            changes_summary += "æ‹†åˆ†æ®µè½ã€"
        changes_summary = changes_summary.rstrip("ã€")

        return ApplyParagraphStrategiesResponse(
            modified_text=modified_text,
            changes_summary_zh=changes_summary,
            strategies_applied=len(request.selected_strategies),
            new_cv=None  # Would need re-analysis to calculate
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Apply paragraph strategies error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
