"""
Structure Analysis API Routes (Level 1 De-AIGC)
ç»“æ„åˆ†æ API è·¯ç”±ï¼ˆLevel 1 De-AIGCï¼‰

Phase 4: Document structure analysis and restructuring endpoints
ç¬¬4é˜¶æ®µï¼šæ–‡æ¡£ç»“æ„åˆ†æå’Œé‡ç»„ç«¯ç‚¹
"""

from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm.attributes import flag_modified
from typing import Optional
import logging

from src.api.schemas import (
    StructureAnalysisRequest,
    StructureAnalysisResponse,
    StructureStrategy,
    StructureOption,
    LogicDiagnosisResponse,
    DocumentStructureRequest,
    ParagraphInfo,
    StructureIssue,
    BreakPoint,
    FlowRelation,
    RiskArea,
    StructureModification,
    StructureChange,
    SmartStructureResponse,
    SmartStructureIssue,
    SectionInfo,
    SmartParagraphInfo,
    ExplicitConnector,
    LogicBreak,
    # Enhanced schemas (Level 1 Enhancement)
    PredictabilityAnalysisRequest,
    PredictabilityAnalysisResponse,
    ProgressionAnalysisResult,
    FunctionDistributionResult,
    ClosureAnalysisResult,
    LexicalEchoResult,
    DisruptionRestructureRequest,
    DisruptionRestructureResponse,
    DisruptionLevel,
    # 7-Indicator Risk Card schemas
    StructuralIndicatorResponse,
    StructuralRiskCardResponse,
    CrossReferenceResult,
    RiskCardRequest,
    # Single paragraph suggestion schemas
    ParagraphSuggestionRequest,
    ParagraphSuggestionResponse,
    # Detailed improvement suggestions schemas
    SectionSuggestion,
    DetailedImprovementSuggestions,
    # Merge modify schemas
    MergeModifyRequest,
    MergeModifyPromptResponse,
    MergeModifyApplyResponse,
)
from src.core.analyzer.structure import StructureAnalyzer
from src.core.analyzer.smart_structure import SmartStructureAnalyzer
from src.prompts.structure import DISRUPTION_LEVELS, DISRUPTION_STRATEGIES
from src.db.database import get_db
from src.db.models import Document, Session

logger = logging.getLogger(__name__)

router = APIRouter()

# Initialize analyzers
# åˆå§‹åŒ–åˆ†æå™¨
structure_analyzer = StructureAnalyzer()
smart_analyzer = SmartStructureAnalyzer()


@router.post("/", response_model=StructureAnalysisResponse)
async def analyze_structure(request: StructureAnalysisRequest):
    """
    Analyze document structure for AI patterns
    åˆ†ææ–‡æ¡£ç»“æ„çš„AIæ¨¡å¼

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureAnalysisResponse with analysis results åŒ…å«åˆ†æç»“æœçš„å“åº”
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Convert paragraph info to API format
        # å°†æ®µè½ä¿¡æ¯è½¬æ¢ä¸ºAPIæ ¼å¼
        paragraphs = [
            ParagraphInfo(
                index=p.index,
                first_sentence=p.first_sentence[:200] if len(p.first_sentence) > 200 else p.first_sentence,
                last_sentence=p.last_sentence[:200] if len(p.last_sentence) > 200 else p.last_sentence,
                word_count=p.word_count,
                sentence_count=p.sentence_count,
                has_topic_sentence=p.has_topic_sentence,
                has_summary_ending=p.has_summary_ending,
                connector_words=p.connector_words,
                function_type=p.function_type
            )
            for p in result.paragraphs
        ]

        # Convert issues
        # è½¬æ¢é—®é¢˜
        issues = [
            StructureIssue(
                type=i.type,
                description=i.description,
                description_zh=i.description_zh,
                severity=i.severity,
                affected_paragraphs=i.affected_paragraphs,
                suggestion=i.suggestion,
                suggestion_zh=i.suggestion_zh
            )
            for i in result.issues
        ]

        # Convert break points
        # è½¬æ¢æ–­ç‚¹
        break_points = [
            BreakPoint(
                position=bp.position,
                type=bp.type,
                description=bp.description,
                description_zh=bp.description_zh
            )
            for bp in result.break_points
        ]

        return StructureAnalysisResponse(
            total_paragraphs=result.total_paragraphs,
            total_sentences=result.total_sentences,
            total_words=result.total_words,
            avg_paragraph_length=result.avg_paragraph_length,
            paragraph_length_variance=result.paragraph_length_variance,
            structure_score=result.structure_score,
            risk_level=result.risk_level,
            paragraphs=paragraphs,
            issues=issues,
            break_points=break_points,
            core_thesis=result.core_thesis,
            key_arguments=result.key_arguments,
            has_linear_flow=result.has_linear_flow,
            has_repetitive_pattern=result.has_repetitive_pattern,
            has_uniform_length=result.has_uniform_length,
            has_predictable_order=result.has_predictable_order,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/with-suggestions", response_model=StructureAnalysisResponse)
async def analyze_structure_with_suggestions(request: StructureAnalysisRequest):
    """
    Analyze document structure and generate restructuring suggestions
    åˆ†ææ–‡æ¡£ç»“æ„å¹¶ç”Ÿæˆé‡ç»„å»ºè®®

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureAnalysisResponse with analysis and suggestions åŒ…å«åˆ†æå’Œå»ºè®®çš„å“åº”
    """
    try:
        # Perform base analysis
        # æ‰§è¡ŒåŸºç¡€åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Generate suggestions based on issues
        # æ ¹æ®é—®é¢˜ç”Ÿæˆå»ºè®®
        options = []

        # Only generate suggestions if there are issues
        # ä»…åœ¨å­˜åœ¨é—®é¢˜æ—¶ç”Ÿæˆå»ºè®®
        if result.issues:
            # Option 1: Optimize Connection (gentle approach)
            # é€‰é¡¹1ï¼šä¼˜åŒ–è¿æ¥ï¼ˆæ¸©å’Œæ–¹æ³•ï¼‰
            optimize_modifications = []
            for p in result.paragraphs:
                if p.has_topic_sentence or p.connector_words:
                    optimize_modifications.append(
                        StructureModification(
                            paragraph_index=p.index,
                            change_type="rewrite_opening",
                            original=p.first_sentence[:100],
                            modified=None,  # Would be filled by LLM
                            explanation_zh="ç§»é™¤æ˜¾å¼è¿æ¥è¯ï¼Œåˆ›å»ºéšå¼é€»è¾‘æµ"
                        )
                    )

            options.append(StructureOption(
                strategy=StructureStrategy.OPTIMIZE_CONNECTION,
                strategy_name_zh="ä¼˜åŒ–è¿æ¥",
                modifications=optimize_modifications[:5],  # Limit to top 5
                outline=[f"æ®µè½{i+1}: {p.function_type}" for i, p in enumerate(result.paragraphs)],
                predicted_improvement=15,
                explanation_zh="ä¿æŒæ®µè½é¡ºåºï¼Œä¼˜åŒ–æ®µè½ä¹‹é—´çš„è¡”æ¥æ–¹å¼"
            ))

            # Option 2: Deep Restructure (aggressive approach)
            # é€‰é¡¹2ï¼šæ·±åº¦é‡ç»„ï¼ˆæ¿€è¿›æ–¹æ³•ï¼‰
            if result.structure_score >= 40:
                # Suggest reordering based on function types
                # æ ¹æ®åŠŸèƒ½ç±»å‹å»ºè®®é‡æ–°æ’åº
                new_order = list(range(len(result.paragraphs)))

                # Example: Move a body paragraph to front as hook
                # ç¤ºä¾‹ï¼šå°†æ­£æ–‡æ®µè½ç§»åˆ°å‰é¢ä½œä¸ºé’©å­
                body_indices = [
                    i for i, p in enumerate(result.paragraphs)
                    if p.function_type in ["evidence", "body"]
                ]
                if body_indices and len(result.paragraphs) > 3:
                    # Move evidence paragraph after intro as hook
                    # å°†è¯æ®æ®µè½ç§»åˆ°å¼•è¨€åä½œä¸ºé’©å­
                    hook_idx = body_indices[0]
                    new_order = [0, hook_idx] + [
                        i for i in range(1, len(result.paragraphs))
                        if i != hook_idx
                    ]

                options.append(StructureOption(
                    strategy=StructureStrategy.DEEP_RESTRUCTURE,
                    strategy_name_zh="æ·±åº¦é‡ç»„",
                    new_order=new_order,
                    restructure_type="hook_cycle",
                    restructure_type_zh="é’©å­å¾ªç¯",
                    changes=[
                        StructureChange(
                            type="reorder",
                            affected_paragraphs=new_order,
                            description="Reorganize to break linear flow",
                            description_zh="é‡æ–°ç»„ç»‡ä»¥æ‰“ç ´çº¿æ€§æµç¨‹"
                        )
                    ],
                    outline=[f"æ–°ä½ç½®{i+1}: åŸæ®µè½{idx+1}" for i, idx in enumerate(new_order)],
                    predicted_improvement=25,
                    explanation_zh="é‡æ–°æ’åºæ®µè½ï¼Œæ‰“ç ´å¯é¢„æµ‹çš„ç»“æ„æ¨¡å¼"
                ))

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        paragraphs = [
            ParagraphInfo(
                index=p.index,
                first_sentence=p.first_sentence[:200] if len(p.first_sentence) > 200 else p.first_sentence,
                last_sentence=p.last_sentence[:200] if len(p.last_sentence) > 200 else p.last_sentence,
                word_count=p.word_count,
                sentence_count=p.sentence_count,
                has_topic_sentence=p.has_topic_sentence,
                has_summary_ending=p.has_summary_ending,
                connector_words=p.connector_words,
                function_type=p.function_type
            )
            for p in result.paragraphs
        ]

        issues = [
            StructureIssue(
                type=i.type,
                description=i.description,
                description_zh=i.description_zh,
                severity=i.severity,
                affected_paragraphs=i.affected_paragraphs,
                suggestion=i.suggestion,
                suggestion_zh=i.suggestion_zh
            )
            for i in result.issues
        ]

        break_points = [
            BreakPoint(
                position=bp.position,
                type=bp.type,
                description=bp.description,
                description_zh=bp.description_zh
            )
            for bp in result.break_points
        ]

        return StructureAnalysisResponse(
            total_paragraphs=result.total_paragraphs,
            total_sentences=result.total_sentences,
            total_words=result.total_words,
            avg_paragraph_length=result.avg_paragraph_length,
            paragraph_length_variance=result.paragraph_length_variance,
            structure_score=result.structure_score,
            risk_level=result.risk_level,
            paragraphs=paragraphs,
            issues=issues,
            break_points=break_points,
            core_thesis=result.core_thesis,
            key_arguments=result.key_arguments,
            has_linear_flow=result.has_linear_flow,
            has_repetitive_pattern=result.has_repetitive_pattern,
            has_uniform_length=result.has_uniform_length,
            has_predictable_order=result.has_predictable_order,
            options=options,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Structure analysis with suggestions error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/suggest/{strategy}", response_model=StructureOption)
async def get_structure_suggestion(
    strategy: StructureStrategy,
    request: StructureAnalysisRequest
):
    """
    Get suggestion for a specific restructuring strategy
    è·å–ç‰¹å®šé‡ç»„ç­–ç•¥çš„å»ºè®®

    Args:
        strategy: Restructuring strategy é‡ç»„ç­–ç•¥
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureOption with specific suggestion åŒ…å«ç‰¹å®šå»ºè®®çš„é€‰é¡¹
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        if strategy == StructureStrategy.OPTIMIZE_CONNECTION:
            # Generate optimize connection suggestion
            # ç”Ÿæˆä¼˜åŒ–è¿æ¥å»ºè®®
            modifications = []
            for p in result.paragraphs:
                if p.has_topic_sentence or p.connector_words:
                    modifications.append(
                        StructureModification(
                            paragraph_index=p.index,
                            change_type="rewrite_opening",
                            original=p.first_sentence[:100],
                            modified=None,
                            explanation_zh="å»ºè®®ç§»é™¤æ˜¾å¼è¿æ¥è¯"
                        )
                    )

            return StructureOption(
                strategy=StructureStrategy.OPTIMIZE_CONNECTION,
                strategy_name_zh="ä¼˜åŒ–è¿æ¥",
                modifications=modifications[:5],
                outline=[f"æ®µè½{i+1}: {p.function_type}" for i, p in enumerate(result.paragraphs)],
                predicted_improvement=15,
                explanation_zh="ä¿æŒé¡ºåºï¼Œä¼˜åŒ–è¡”æ¥"
            )

        elif strategy == StructureStrategy.DEEP_RESTRUCTURE:
            # Generate deep restructure suggestion
            # ç”Ÿæˆæ·±åº¦é‡ç»„å»ºè®®
            new_order = list(range(len(result.paragraphs)))

            # Suggest moving evidence to front
            # å»ºè®®å°†è¯æ®ç§»åˆ°å‰é¢
            body_indices = [
                i for i, p in enumerate(result.paragraphs)
                if p.function_type in ["evidence", "body"]
            ]
            if body_indices and len(result.paragraphs) > 3:
                hook_idx = body_indices[0]
                new_order = [0, hook_idx] + [
                    i for i in range(1, len(result.paragraphs))
                    if i != hook_idx
                ]

            return StructureOption(
                strategy=StructureStrategy.DEEP_RESTRUCTURE,
                strategy_name_zh="æ·±åº¦é‡ç»„",
                new_order=new_order,
                restructure_type="hook_cycle",
                restructure_type_zh="é’©å­å¾ªç¯",
                changes=[
                    StructureChange(
                        type="reorder",
                        affected_paragraphs=new_order,
                        description="Reorganize paragraph order",
                        description_zh="é‡æ–°ç»„ç»‡æ®µè½é¡ºåº"
                    )
                ],
                outline=[f"æ–°ä½ç½®{i+1}: åŸæ®µè½{idx+1}" for i, idx in enumerate(new_order)],
                predicted_improvement=25,
                explanation_zh="æ‰“ç ´çº¿æ€§ç»“æ„ï¼Œåˆ›å»ºé’©å­å¾ªç¯æ¨¡å¼"
            )

    except Exception as e:
        logger.error(f"Get structure suggestion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/diagnosis", response_model=LogicDiagnosisResponse)
async def get_logic_diagnosis(request: StructureAnalysisRequest):
    """
    Get logic diagnosis card for document
    è·å–æ–‡æ¡£çš„é€»è¾‘è¯Šæ–­å¡

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        LogicDiagnosisResponse with diagnosis card data åŒ…å«è¯Šæ–­å¡æ•°æ®çš„å“åº”
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Generate flow map
        # ç”Ÿæˆæµç¨‹å›¾
        flow_map = []
        for i in range(len(result.paragraphs) - 1):
            curr = result.paragraphs[i]
            next_para = result.paragraphs[i + 1]

            # Determine relationship
            # ç¡®å®šå…³ç³»
            if next_para.connector_words:
                relation = "continuation"
                symbol = "â†’"
            elif curr.function_type == "evidence" and next_para.function_type == "analysis":
                relation = "evidence"
                symbol = "â¤µ"
            elif curr.function_type == next_para.function_type:
                relation = "comparison"
                symbol = "â†”"
            else:
                relation = "continuation"
                symbol = "â†’"

            # Check for gaps
            # æ£€æŸ¥é—´éš™
            for bp in result.break_points:
                if bp.position == i + 1:
                    relation = "gap"
                    symbol = "âœ—"
                    break

            flow_map.append(FlowRelation(
                **{"from": i, "to": i + 1},
                relation=relation,
                symbol=symbol
            ))

        # Determine structure pattern
        # ç¡®å®šç»“æ„æ¨¡å¼
        if result.has_linear_flow:
            pattern = "linear"
            pattern_zh = "çº¿æ€§"
        elif result.has_repetitive_pattern:
            pattern = "parallel"
            pattern_zh = "å¹¶åˆ—"
        else:
            pattern = "nested"
            pattern_zh = "åµŒå¥—"

        # Generate risk areas
        # ç”Ÿæˆé£é™©åŒºåŸŸ
        risk_areas = []
        for p in result.paragraphs:
            risk_level = "low"
            reason = ""
            reason_zh = ""

            if p.has_topic_sentence and p.connector_words:
                risk_level = "high"
                reason = "Topic sentence with explicit connectors"
                reason_zh = "ä¸»é¢˜å¥é…åˆæ˜¾å¼è¿æ¥è¯"
            elif p.has_topic_sentence:
                risk_level = "medium"
                reason = "Topic sentence pattern detected"
                reason_zh = "æ£€æµ‹åˆ°ä¸»é¢˜å¥æ¨¡å¼"
            elif p.connector_words:
                risk_level = "medium"
                reason = "Explicit connectors detected"
                reason_zh = "æ£€æµ‹åˆ°æ˜¾å¼è¿æ¥è¯"

            if risk_level != "low":
                risk_areas.append(RiskArea(
                    paragraph=p.index,
                    risk_level=risk_level,
                    reason=reason,
                    reason_zh=reason_zh
                ))

        # Determine recommended strategy
        # ç¡®å®šæ¨èç­–ç•¥
        if result.structure_score >= 40:
            recommended = StructureStrategy.DEEP_RESTRUCTURE
            rec_reason = "High structure score requires significant reorganization"
            rec_reason_zh = "é«˜ç»“æ„åˆ†æ•°éœ€è¦æ˜¾è‘—é‡ç»„"
        else:
            recommended = StructureStrategy.OPTIMIZE_CONNECTION
            rec_reason = "Moderate issues can be fixed with connection optimization"
            rec_reason_zh = "ä¸­ç­‰é—®é¢˜å¯ä»¥é€šè¿‡ä¼˜åŒ–è¿æ¥ä¿®å¤"

        return LogicDiagnosisResponse(
            flow_map=flow_map,
            structure_pattern=pattern,
            structure_pattern_zh=pattern_zh,
            pattern_description=f"Document follows a {pattern} structure pattern",
            pattern_description_zh=f"æ–‡æ¡£éµå¾ª{pattern_zh}ç»“æ„æ¨¡å¼",
            risk_areas=risk_areas,
            recommended_strategy=recommended,
            recommendation_reason=rec_reason,
            recommendation_reason_zh=rec_reason_zh
        )

    except Exception as e:
        logger.error(f"Logic diagnosis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document", response_model=SmartStructureResponse)
async def analyze_document_structure(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Analyze structure of a document by ID using smart LLM analysis
    ä½¿ç”¨æ™ºèƒ½ LLM åˆ†ææŒ‰ ID åˆ†ææ–‡æ¡£ç»“æ„

    Features:
    - Filters non-paragraph content (titles, tables, figures)
    - Uses paper section numbering (1, 1.1, 2.3.1)
    - Generates meaningful content summaries
    - Labels each paragraph with position like "3.2(1)"
    - Caches results to avoid repeated LLM calls
    - ç¼“å­˜ç»“æœä»¥é¿å…é‡å¤çš„ LLM è°ƒç”¨

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        SmartStructureResponse with analysis results åŒ…å«åˆ†æç»“æœçš„å“åº”
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if we have cached analysis result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
        if document.structure_analysis_cache:
            logger.info(f"Using cached structure analysis for document {request.document_id}")
            result = document.structure_analysis_cache
        else:
            # Use smart LLM-based analysis
            # ä½¿ç”¨æ™ºèƒ½ LLM åˆ†æ
            logger.info(f"Starting smart structure analysis for document {request.document_id}")
            result = await smart_analyzer.analyze(document.original_text)
            logger.info(f"Smart analysis completed: {result.get('total_paragraphs', 0)} paragraphs found")

            # Cache the result in database
            # å°†ç»“æœç¼“å­˜åˆ°æ•°æ®åº“
            document.structure_analysis_cache = result
            flag_modified(document, 'structure_analysis_cache')
            await db.commit()
            logger.info(f"Cached structure analysis for document {request.document_id}")

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        sections = [
            SectionInfo(
                number=s.get("number", "?"),
                title=s.get("title", ""),
                paragraphs=[
                    SmartParagraphInfo(
                        position=p.get("position", "?"),
                        summary=p.get("summary", ""),
                        summary_zh=p.get("summary_zh", ""),
                        first_sentence=p.get("first_sentence", "")[:200],
                        last_sentence=p.get("last_sentence", "")[:200],
                        word_count=p.get("word_count", 0),
                        ai_risk=p.get("ai_risk", "unknown"),
                        ai_risk_reason=p.get("ai_risk_reason", ""),
                        # New fields for detailed rewrite suggestions
                        # æ–°å­—æ®µï¼šè¯¦ç»†ä¿®æ”¹å»ºè®®
                        rewrite_suggestion_zh=p.get("rewrite_suggestion_zh"),
                        rewrite_example=p.get("rewrite_example")
                    )
                    for p in s.get("paragraphs", [])
                ]
            )
            for s in result.get("sections", [])
        ]

        # Convert issues
        # è½¬æ¢é—®é¢˜
        # Handle case where LLM returns "All" instead of a list
        # å¤„ç†LLMè¿”å›"All"è€Œéåˆ—è¡¨çš„æƒ…å†µ
        def ensure_list(val):
            if isinstance(val, list):
                return val
            elif isinstance(val, str):
                return [val] if val else []
            return []

        issues = [
            SmartStructureIssue(
                type=i.get("type", "unknown"),
                description=i.get("description", ""),
                description_zh=i.get("description_zh", ""),
                severity=i.get("severity", "low"),
                affected_positions=ensure_list(i.get("affected_positions", []))
            )
            for i in result.get("issues", [])
        ]

        # Build compatible paragraphs list for existing frontend
        # æ„å»ºä¸ç°æœ‰å‰ç«¯å…¼å®¹çš„æ®µè½åˆ—è¡¨
        paragraphs = []
        idx = 0
        for section in sections:
            for p in section.paragraphs:
                paragraphs.append(ParagraphInfo(
                    index=idx,
                    first_sentence=p.first_sentence,
                    last_sentence=p.last_sentence,
                    word_count=p.word_count,
                    sentence_count=0,
                    has_topic_sentence=False,
                    has_summary_ending=False,
                    connector_words=[],
                    function_type="body",
                    position=p.position,
                    summary=p.summary,
                    summary_zh=p.summary_zh,
                    ai_risk=p.ai_risk,
                    ai_risk_reason=p.ai_risk_reason,
                    # New rewrite suggestion fields
                    # æ–°çš„ä¿®æ”¹å»ºè®®å­—æ®µ
                    rewrite_suggestion_zh=p.rewrite_suggestion_zh,
                    rewrite_example=p.rewrite_example
                ))
                idx += 1

        # Extract pattern flags from score breakdown
        # ä»åˆ†æ•°åˆ†è§£ä¸­æå–æ¨¡å¼æ ‡å¿—
        score_breakdown = result.get("score_breakdown", {})

        # Convert explicit connectors
        # è½¬æ¢æ˜¾æ€§è¿æ¥è¯
        explicit_connectors = [
            ExplicitConnector(
                word=c.get("word", ""),
                position=c.get("position", ""),
                location=c.get("location", "paragraph_start"),
                severity=c.get("severity", "high")
            )
            for c in result.get("explicit_connectors", [])
        ]

        # Convert logic breaks
        # è½¬æ¢é€»è¾‘æ–­è£‚ç‚¹
        # Note: lb.get("key", "") returns None if key exists with null value
        # Pydantic v2 treats explicit None differently - always convert to empty string
        # æ³¨æ„ï¼šå½“keyå­˜åœ¨ä½†å€¼ä¸ºnullæ—¶ï¼Œlb.get("key", "")è¿”å›None
        # Pydantic v2 å¯¹æ˜¾å¼ None å¤„ç†ä¸åŒ - å§‹ç»ˆè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        logic_breaks = [
            LogicBreak(
                from_position=lb.get("from_position") or "",
                to_position=lb.get("to_position") or "",
                transition_type=lb.get("transition_type") or "abrupt",
                issue=lb.get("issue") or "",
                issue_zh=lb.get("issue_zh") or "",
                suggestion=lb.get("suggestion") or "",  # Empty string if None/null
                suggestion_zh=lb.get("suggestion_zh") or ""  # Empty string if None/null
            )
            for lb in result.get("logic_breaks", [])
        ]

        # Parse detailed suggestions if available
        # è§£æè¯¦ç»†å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰
        detailed_suggestions = None
        raw_suggestions = result.get("detailed_suggestions")
        if raw_suggestions and isinstance(raw_suggestions, dict):
            section_suggestions = []
            for s in raw_suggestions.get("section_suggestions", []):
                section_suggestions.append(SectionSuggestion(
                    section_number=s.get("section_number", "?"),
                    section_title=s.get("section_title", ""),
                    severity=s.get("severity", "medium"),
                    suggestion_type=s.get("suggestion_type", "restructure"),
                    suggestion_zh=s.get("suggestion_zh", ""),
                    suggestion_en=s.get("suggestion_en", ""),
                    details=s.get("details", []),
                    affected_paragraphs=s.get("affected_paragraphs", [])
                ))
            detailed_suggestions = DetailedImprovementSuggestions(
                abstract_suggestions=raw_suggestions.get("abstract_suggestions", []),
                logic_suggestions=raw_suggestions.get("logic_suggestions", []),
                section_suggestions=section_suggestions,
                priority_order=raw_suggestions.get("priority_order", []),
                overall_assessment_zh=raw_suggestions.get("overall_assessment_zh", ""),
                overall_assessment_en=raw_suggestions.get("overall_assessment_en", "")
            )

        return SmartStructureResponse(
            sections=sections,
            total_paragraphs=result.get("total_paragraphs", len(paragraphs)),
            total_sections=result.get("total_sections", len(sections)),
            structure_score=result.get("structure_score", 0),
            risk_level=result.get("risk_level", "low"),
            issues=issues,
            score_breakdown=score_breakdown,
            recommendation=result.get("recommendation", ""),
            recommendation_zh=result.get("recommendation_zh", ""),
            detailed_suggestions=detailed_suggestions,
            explicit_connectors=explicit_connectors,
            logic_breaks=logic_breaks,
            paragraphs=paragraphs,
            has_linear_flow=score_breakdown.get("linear_flow", 0) > 0,
            has_repetitive_pattern=score_breakdown.get("repetitive_pattern", 0) > 0,
            has_uniform_length=score_breakdown.get("uniform_length", 0) > 0,
            has_predictable_order=score_breakdown.get("predictable_order", 0) > 0,
            options=[]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Step 1-1 and Step 1-2: Two-Phase Analysis Endpoints
# æ­¥éª¤ 1-1 å’Œ 1-2ï¼šä¸¤é˜¶æ®µåˆ†æç«¯ç‚¹
# =============================================================================

@router.post("/document/step1-1")
async def analyze_document_structure_step1(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Step 1-1: Analyze document STRUCTURE only (global patterns)
    æ­¥éª¤ 1-1ï¼šä»…åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆå…¨å±€æ¨¡å¼ï¼‰

    This endpoint performs the first phase of analysis:
    - Section structure identification
    - Paragraph identification
    - Global structural patterns (linear flow, symmetry, etc.)
    - Structure score calculation
    - Style/formality analysis with mismatch detection

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Structure analysis result ç»“æ„åˆ†æç»“æœ
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Get colloquialism_level from session if provided
        # å¦‚æœæä¾›äº† session_idï¼Œä» session è·å– colloquialism_level
        target_colloquialism = None
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session:
                target_colloquialism = session.colloquialism_level
                logger.info(f"Using colloquialism_level={target_colloquialism} from session {request.session_id}")

        # Check if we have cached Step 1-1 result with same colloquialism level
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ­¥éª¤ 1-1 ç»“æœï¼ˆä¸”å£è¯­åŒ–çº§åˆ«ç›¸åŒï¼‰
        cache_key = "step1_1_cache"
        if hasattr(document, 'structure_analysis_cache') and document.structure_analysis_cache:
            cached = document.structure_analysis_cache
            if cache_key in cached:
                # Check if cached result was analyzed with same colloquialism level
                # æ£€æŸ¥ç¼“å­˜ç»“æœæ˜¯å¦ä½¿ç”¨ç›¸åŒçš„å£è¯­åŒ–çº§åˆ«åˆ†æ
                cached_style = cached[cache_key].get("style_analysis", {})
                cached_target = cached_style.get("target_colloquialism")
                if cached_target == target_colloquialism or (cached_target is None and target_colloquialism is None):
                    logger.info(f"Using cached Step 1-1 result for document {request.document_id}")
                    return cached[cache_key]
                else:
                    logger.info(f"Cached result has different colloquialism level, re-analyzing")

        # Perform Step 1-1 analysis with colloquialism level
        # ä½¿ç”¨å£è¯­åŒ–çº§åˆ«æ‰§è¡Œæ­¥éª¤ 1-1 åˆ†æ
        logger.info(f"Starting Step 1-1 structure analysis for document {request.document_id} (target_colloquialism={target_colloquialism})")
        result = await smart_analyzer.analyze_structure(document.original_text, target_colloquialism=target_colloquialism)

        # Cache the result to SQLite
        # ç¼“å­˜ç»“æœåˆ° SQLite
        if not document.structure_analysis_cache:
            document.structure_analysis_cache = {}
        document.structure_analysis_cache[cache_key] = result
        flag_modified(document, 'structure_analysis_cache')
        await db.commit()
        logger.info(f"Step 1-1 cache saved to SQLite for document {request.document_id}")

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Step 1-1 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document/step1-2")
async def analyze_document_relationships_step2(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Step 1-2: Analyze paragraph RELATIONSHIPS (connections, transitions)
    æ­¥éª¤ 1-2ï¼šåˆ†ææ®µè½å…³ç³»ï¼ˆè¿æ¥ã€è¿‡æ¸¡ï¼‰

    This endpoint performs the second phase of analysis:
    - Explicit connector word detection
    - Logic break points between paragraphs
    - AI risk assessment for each paragraph
    - Rewrite suggestions

    Requires Step 1-1 to be completed first.

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Relationship analysis result å…³ç³»åˆ†æç»“æœ
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if Step 1-1 has been completed
        # æ£€æŸ¥æ­¥éª¤ 1-1 æ˜¯å¦å·²å®Œæˆ
        step1_1_key = "step1_1_cache"
        step1_2_key = "step1_2_cache"

        if not document.structure_analysis_cache or step1_1_key not in document.structure_analysis_cache:
            raise HTTPException(
                status_code=400,
                detail="Step 1-1 (structure analysis) must be completed first"
            )

        # Check if we have cached Step 1-2 result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ­¥éª¤ 1-2 ç»“æœ
        if step1_2_key in document.structure_analysis_cache:
            logger.info(f"Using cached Step 1-2 result for document {request.document_id}")
            return document.structure_analysis_cache[step1_2_key]

        # Get Step 1-1 result
        # è·å–æ­¥éª¤ 1-1 ç»“æœ
        structure_result = document.structure_analysis_cache[step1_1_key]

        # Perform Step 1-2 analysis
        # æ‰§è¡Œæ­¥éª¤ 1-2 åˆ†æ
        logger.info(f"Starting Step 1-2 relationship analysis for document {request.document_id}")
        result = await smart_analyzer.analyze_relationships(
            document.original_text,
            structure_result
        )

        # Cache the result to SQLite
        # ç¼“å­˜ç»“æœåˆ° SQLite
        document.structure_analysis_cache[step1_2_key] = result
        flag_modified(document, 'structure_analysis_cache')
        await db.commit()
        logger.info(f"Step 1-2 cache saved to SQLite for document {request.document_id}")

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Step 1-2 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/document/{document_id}/cache")
async def clear_analysis_cache(
    document_id: str,
    db: AsyncSession = Depends(get_db)
):
    """
    Clear analysis cache for a document (allows re-analysis)
    æ¸…é™¤æ–‡æ¡£çš„åˆ†æç¼“å­˜ï¼ˆå…è®¸é‡æ–°åˆ†æï¼‰

    Args:
        document_id: Document ID æ–‡æ¡£ID
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Success message æˆåŠŸæ¶ˆæ¯
    """
    try:
        document = await db.get(Document, document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        document.structure_analysis_cache = None
        flag_modified(document, 'structure_analysis_cache')
        await db.commit()
        logger.info(f"Cache cleared for document {document_id}")

        return {"message": "Cache cleared successfully", "document_id": document_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Clear cache error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/strategies")
async def get_structure_strategies():
    """
    Get available structure strategies
    è·å–å¯ç”¨çš„ç»“æ„ç­–ç•¥

    Returns:
        List of strategy descriptions ç­–ç•¥æè¿°åˆ—è¡¨
    """
    return {
        "strategies": [
            {
                "id": "optimize_connection",
                "name": "Optimize Connection",
                "name_zh": "ä¼˜åŒ–è¿æ¥",
                "description": "Improve paragraph connections without changing content order",
                "description_zh": "åœ¨ä¸æ”¹å˜å†…å®¹é¡ºåºçš„æƒ…å†µä¸‹æ”¹å–„æ®µè½è¿æ¥"
            },
            {
                "id": "deep_restructure",
                "name": "Deep Restructure",
                "name_zh": "æ·±åº¦é‡ç»„",
                "description": "Reorder and reorganize content for maximum naturalness",
                "description_zh": "é‡æ–°æ’åºå’Œç»„ç»‡å†…å®¹ä»¥è·å¾—æœ€å¤§è‡ªç„¶åº¦"
            }
        ]
    }


# =============================================================================
# Enhanced Structure Analysis Endpoints (Level 1 Enhancement)
# å¢å¼ºç»“æ„åˆ†æç«¯ç‚¹ï¼ˆLevel 1å¢å¼ºï¼‰
# =============================================================================

@router.post("/predictability", response_model=PredictabilityAnalysisResponse)
async def analyze_predictability(request: PredictabilityAnalysisRequest):
    """
    Analyze document structure predictability
    åˆ†ææ–‡æ¡£ç»“æ„é¢„æµ‹æ€§

    This endpoint provides detailed analysis of:
    - Progression type (monotonic vs non-monotonic)
    - Function distribution (uniform vs asymmetric)
    - Closure pattern (strong vs weak/open)
    - Lexical echo (explicit connectors vs semantic bridges)

    Args:
        request: Predictability analysis request é¢„æµ‹æ€§åˆ†æè¯·æ±‚

    Returns:
        PredictabilityAnalysisResponse with detailed analysis åŒ…å«è¯¦ç»†åˆ†æçš„å“åº”
    """
    try:
        # Perform full structure analysis
        # æ‰§è¡Œå®Œæ•´ç»“æ„åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=True
        )

        # Convert progression analysis
        # è½¬æ¢æ¨è¿›åˆ†æ
        progression = None
        if result.progression_analysis:
            progression = ProgressionAnalysisResult(
                progression_type=result.progression_analysis.progression_type,
                progression_type_zh=result.progression_analysis.progression_type_zh,
                forward_transitions=result.progression_analysis.forward_transitions,
                backward_references=result.progression_analysis.backward_references,
                conditional_statements=result.progression_analysis.conditional_statements,
                score=result.progression_analysis.score
            )

        # Convert function distribution
        # è½¬æ¢åŠŸèƒ½åˆ†å¸ƒ
        distribution = None
        if result.function_distribution:
            distribution = FunctionDistributionResult(
                distribution_type=result.function_distribution.distribution_type,
                distribution_type_zh=result.function_distribution.distribution_type_zh,
                function_counts=result.function_distribution.function_counts,
                depth_variance=result.function_distribution.depth_variance,
                longest_section_ratio=result.function_distribution.longest_section_ratio,
                score=result.function_distribution.score,
                asymmetry_opportunities=result.function_distribution.asymmetry_opportunities
            )

        # Convert closure analysis
        # è½¬æ¢é—­åˆåˆ†æ
        closure = None
        if result.closure_analysis:
            closure = ClosureAnalysisResult(
                closure_type=result.closure_analysis.closure_type,
                closure_type_zh=result.closure_analysis.closure_type_zh,
                has_formulaic_ending=result.closure_analysis.has_formulaic_ending,
                has_complete_resolution=result.closure_analysis.has_complete_resolution,
                open_questions=result.closure_analysis.open_questions,
                hedging_in_conclusion=result.closure_analysis.hedging_in_conclusion,
                score=result.closure_analysis.score,
                detected_patterns=result.closure_analysis.detected_patterns
            )

        # Convert lexical echo analysis
        # è½¬æ¢è¯æ±‡å›å£°åˆ†æ
        lexical_echo = None
        if result.lexical_echo_analysis:
            lexical_echo = LexicalEchoResult(
                total_transitions=result.lexical_echo_analysis.total_transitions,
                echo_transitions=result.lexical_echo_analysis.echo_transitions,
                explicit_connector_transitions=result.lexical_echo_analysis.explicit_connector_transitions,
                echo_ratio=result.lexical_echo_analysis.echo_ratio,
                score=result.lexical_echo_analysis.score,
                transition_details=result.lexical_echo_analysis.transition_details
            )

        # Determine recommended disruption level based on score
        # æ ¹æ®åˆ†æ•°ç¡®å®šæ¨èçš„æ‰°åŠ¨ç­‰çº§
        if result.structure_score >= 60:
            recommended_level = "strong"
            recommended_strategies = ["inversion", "conflict_injection", "weak_closure", "asymmetry"]
        elif result.structure_score >= 35:
            recommended_level = "medium"
            recommended_strategies = ["lexical_echo", "asymmetry", "local_reorder"]
        else:
            recommended_level = "light"
            recommended_strategies = ["rewrite_opening", "remove_connector", "lexical_echo"]

        return PredictabilityAnalysisResponse(
            total_score=result.structure_score,
            risk_level=result.risk_level,
            progression_analysis=progression,
            function_distribution=distribution,
            closure_analysis=closure,
            lexical_echo_analysis=lexical_echo,
            recommended_disruption_level=recommended_level,
            recommended_strategies=recommended_strategies,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Predictability analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/disruption-levels")
async def get_disruption_levels():
    """
    Get available disruption levels and their configurations
    è·å–å¯ç”¨çš„æ‰°åŠ¨ç­‰çº§åŠå…¶é…ç½®

    Returns:
        Dictionary of disruption levels æ‰°åŠ¨ç­‰çº§å­—å…¸
    """
    return {
        "levels": DISRUPTION_LEVELS,
        "strategies": DISRUPTION_STRATEGIES
    }


@router.get("/disruption-strategies")
async def get_disruption_strategies():
    """
    Get available disruption strategies
    è·å–å¯ç”¨çš„æ‰°åŠ¨ç­–ç•¥

    Returns:
        List of strategy descriptions ç­–ç•¥æè¿°åˆ—è¡¨
    """
    strategies = []
    for key, value in DISRUPTION_STRATEGIES.items():
        strategies.append({
            "id": key,
            "name": value["name"],
            "name_zh": value["name_zh"],
            "description": value["description"],
            "description_zh": value["description_zh"],
            "prompt_instruction": value["prompt_instruction"]
        })
    return {"strategies": strategies}


# =============================================================================
# 7-Indicator Structural Risk Card Endpoint
# 7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç«¯ç‚¹
# =============================================================================

@router.post("/risk-card", response_model=StructuralRiskCardResponse)
async def get_structural_risk_card(request: RiskCardRequest):
    """
    Get 7-indicator structural risk card for user visualization
    è·å–7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç”¨äºç”¨æˆ·å¯è§†åŒ–

    Returns a visual risk card showing:
    - 7 AI structural indicators with emoji and color
    - Whether each indicator is triggered
    - Overall risk level and summary

    The 7 indicators are:
    1. âš–ï¸ Perfect Symmetry (é€»è¾‘æ¨è¿›å¯¹ç§°) - â˜…â˜…â˜…
    2. ğŸ“Š Uniform Function (æ®µè½åŠŸèƒ½å‡åŒ€) - â˜…â˜…â˜†
    3. ğŸ”— Over-signaled Transitions (è¿æ¥è¯ä¾èµ–) - â˜…â˜…â˜…
    4. ğŸ“ Linear Enumeration (å•ä¸€çº¿æ€§æ¨è¿›) - â˜…â˜…â˜…
    5. ğŸ“ Rhythmic Regularity (æ®µè½èŠ‚å¥å‡è¡¡) - â˜…â˜…â˜†
    6. ğŸ”’ Over-conclusive Ending (ç»“å°¾è¿‡åº¦é—­åˆ) - â˜…â˜…â˜†
    7. ğŸ”„ No Cross-References (ç¼ºä¹å›æŒ‡ç»“æ„) - â˜…â˜…â˜†

    Args:
        request: Risk card analysis request é£é™©å¡ç‰‡åˆ†æè¯·æ±‚

    Returns:
        StructuralRiskCardResponse with 7 indicators åŒ…å«7ä¸ªæŒ‡å¾çš„å“åº”
    """
    try:
        # Perform full structure analysis
        # æ‰§è¡Œå®Œæ•´ç»“æ„åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=True
        )

        # Get the risk card from the result
        # ä»ç»“æœä¸­è·å–é£é™©å¡ç‰‡
        if not result.risk_card:
            raise HTTPException(status_code=500, detail="Failed to generate risk card")

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        indicators = [
            StructuralIndicatorResponse(
                id=ind.id,
                name=ind.name,
                name_zh=ind.name_zh,
                triggered=ind.triggered,
                risk_level=ind.risk_level,
                emoji=ind.emoji,
                color=ind.color,
                description=ind.description,
                description_zh=ind.description_zh,
                details=ind.details,
                details_zh=ind.details_zh
            )
            for ind in result.risk_card.indicators
        ]

        return StructuralRiskCardResponse(
            indicators=indicators,
            triggered_count=result.risk_card.triggered_count,
            overall_risk=result.risk_card.overall_risk,
            overall_risk_zh=result.risk_card.overall_risk_zh,
            summary=result.risk_card.summary,
            summary_zh=result.risk_card.summary_zh,
            total_score=result.risk_card.total_score
        )

    except Exception as e:
        logger.error(f"Risk card generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/indicator-config")
async def get_indicator_config():
    """
    Get the 7-indicator configuration for UI display
    è·å–7æŒ‡å¾é…ç½®ç”¨äºUIæ˜¾ç¤º

    Returns:
        Configuration for all 7 indicators with emoji, colors, and descriptions
    """
    from src.core.analyzer.structure import StructureAnalyzer

    return {
        "indicators": StructureAnalyzer.INDICATOR_CONFIG,
        "summary_text": "AIå†™ä½œçš„æœ€å¤§ç‰¹å¾ä¸æ˜¯è¯­æ³•å®Œç¾ï¼Œè€Œæ˜¯ç»“æ„å¤ªå®Œç¾ã€‚",
        "summary_text_en": "The biggest feature of AI writing is not perfect grammar, but perfect structure."
    }


# =============================================================================
# Single Paragraph Suggestion Endpoint
# å•ä¸ªæ®µè½å»ºè®®ç«¯ç‚¹
# =============================================================================

# Prompt for generating single paragraph rewrite suggestions
# ç”Ÿæˆå•ä¸ªæ®µè½æ”¹å†™å»ºè®®çš„æç¤ºè¯
PARAGRAPH_SUGGESTION_PROMPT = """You are an expert De-AIGC consultant. Analyze this paragraph and provide specific rewriting advice to remove AI-writing patterns.

## PARAGRAPH TEXT:
{paragraph_text}

## PARAGRAPH POSITION: {paragraph_position}

## KNOWN AI RISK: {ai_risk} - {ai_risk_reason}

## CONTEXT (if available):
{context_hint}

## YOUR TASK:
Generate a SPECIFIC, ACTIONABLE Chinese rewriting suggestion for this paragraph. Focus on:
1. Identifying specific AI writing patterns (explicit connectors, formulaic structure, etc.)
2. Providing concrete strategies to humanize the text
3. Giving an example of how to rewrite key sentences

## OUTPUT FORMAT (JSON):
{{
  "rewrite_suggestion_zh": "ã€é—®é¢˜è¯Šæ–­ã€‘æ®µé¦–ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯'Furthermore'ï¼Œå±äºå…¸å‹AIå†™ä½œç—•è¿¹ã€‚æ®µè½ç»“æ„é‡‡ç”¨'é—®é¢˜-åˆ†æ-ç»“è®º'å…¬å¼åŒ–æ¨¡å¼ã€‚\\nã€ä¿®æ”¹ç­–ç•¥ã€‘1. åˆ é™¤æ®µé¦–è¿æ¥è¯ï¼Œæ”¹ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥ä¸Šæ®µå…³é”®æ¦‚å¿µï¼›2. æ‰“æ•£å…¬å¼åŒ–ç»“æ„ï¼Œå°†ç»“è®ºæå‰æˆ–èå…¥è®ºè¿°ä¸­ã€‚\\nã€æ”¹å†™æç¤ºã€‘å¯å°†å¼€å¤´æ”¹ä¸ºç›´æ¥æ‰¿æ¥ä¸Šæ®µå†…å®¹ï¼Œå¦‚'åœŸå£¤ç›åˆ†ç´¯ç§¯çš„è¿™ä¸€è¶‹åŠ¿åœ¨...'",
  "rewrite_example": "The escalating trend of soil salinization poses new challenges to traditional agriculture. In the North China Plain, monitoring data from the past decade reveals...",
  "ai_risk": "high",
  "ai_risk_reason": "æ®µé¦–ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯'Furthermore'ï¼Œé‡‡ç”¨å…¬å¼åŒ–ç»“æ„"
}}

CRITICAL RULES:
- The rewrite_suggestion_zh MUST be in Chinese
- The rewrite_suggestion_zh MUST includeã€é—®é¢˜è¯Šæ–­ã€‘ã€ä¿®æ”¹ç­–ç•¥ã€‘ã€æ”¹å†™æç¤ºã€‘sections
- Quote specific text from the paragraph in the diagnosis
- Provide concrete examples, not generic advice
- The rewrite_example should be in ENGLISH showing a better version of the first 1-2 sentences
- The ai_risk_reason should be in CHINESE (ä¸­æ–‡æè¿°ï¼Œå¼•ç”¨åŸæ–‡æ—¶ä¿ç•™åŸè¯­è¨€)
"""


@router.post("/paragraph-suggestion", response_model=ParagraphSuggestionResponse)
async def get_paragraph_suggestion(request: ParagraphSuggestionRequest):
    """
    Get rewrite suggestion for a single paragraph
    è·å–å•ä¸ªæ®µè½çš„æ”¹å†™å»ºè®®

    This endpoint generates specific, actionable rewriting advice
    for a single paragraph using LLM analysis.
    æ­¤ç«¯ç‚¹ä½¿ç”¨ LLM åˆ†æä¸ºå•ä¸ªæ®µè½ç”Ÿæˆå…·ä½“å¯è¡Œçš„æ”¹å†™å»ºè®®ã€‚

    Args:
        request: Paragraph suggestion request æ®µè½å»ºè®®è¯·æ±‚

    Returns:
        ParagraphSuggestionResponse with rewrite suggestions åŒ…å«æ”¹å†™å»ºè®®çš„å“åº”
    """
    try:
        import httpx
        import json
        from src.config import get_settings

        settings = get_settings()

        # Build prompt
        # æ„å»ºæç¤ºè¯
        prompt = PARAGRAPH_SUGGESTION_PROMPT.format(
            paragraph_text=request.paragraph_text[:2000],  # Limit length
            paragraph_position=request.paragraph_position,
            ai_risk=request.ai_risk or "unknown",
            ai_risk_reason=request.ai_risk_reason or "Not yet analyzed",
            context_hint=request.context_hint or "No context provided"
        )

        # Call LLM API
        # è°ƒç”¨ LLM API
        response_text = await _call_llm_for_suggestion(prompt, settings)

        # Parse response
        # è§£æå“åº”
        result = _parse_suggestion_response(response_text)

        return ParagraphSuggestionResponse(
            paragraph_position=request.paragraph_position,
            rewrite_suggestion_zh=result.get("rewrite_suggestion_zh", "ã€é—®é¢˜è¯Šæ–­ã€‘åˆ†æå¤±è´¥\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·ç¨åé‡è¯•\nã€æ”¹å†™æç¤ºã€‘æ— "),
            rewrite_example=result.get("rewrite_example"),
            ai_risk=result.get("ai_risk", request.ai_risk or "unknown"),
            ai_risk_reason=result.get("ai_risk_reason", request.ai_risk_reason or "")
        )

    except Exception as e:
        logger.error(f"Paragraph suggestion error: {e}")
        # Return a fallback response instead of error
        # è¿”å›åå¤‡å“åº”è€Œä¸æ˜¯é”™è¯¯
        return ParagraphSuggestionResponse(
            paragraph_position=request.paragraph_position,
            rewrite_suggestion_zh=f"ã€é—®é¢˜è¯Šæ–­ã€‘åˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·ç¨åé‡è¯•\nã€æ”¹å†™æç¤ºã€‘å»ºè®®åˆ é™¤æ®µé¦–æ˜¾æ€§è¿æ¥è¯ï¼Œæ”¹ç”¨è¯­ä¹‰æ‰¿æ¥",
            rewrite_example=None,
            ai_risk=request.ai_risk or "unknown",
            ai_risk_reason=request.ai_risk_reason or ""
        )


async def _call_llm_for_suggestion(prompt: str, settings) -> str:
    """
    Call LLM API for paragraph suggestion
    è°ƒç”¨ LLM API è·å–æ®µè½å»ºè®®
    """
    import httpx

    # Use Volcengine (preferred)
    # ä½¿ç”¨ç«å±±å¼•æ“ï¼ˆé¦–é€‰ï¼‰
    if settings.llm_provider == "volcengine" and settings.volcengine_api_key:
        async with httpx.AsyncClient(
            base_url=settings.volcengine_base_url,
            headers={
                "Authorization": f"Bearer {settings.volcengine_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.volcengine_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use DeepSeek
    # ä½¿ç”¨ DeepSeek
    elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
        async with httpx.AsyncClient(
            base_url=settings.deepseek_base_url,
            headers={
                "Authorization": f"Bearer {settings.deepseek_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.llm_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use Gemini
    # ä½¿ç”¨ Gemini
    elif settings.llm_provider == "gemini" and settings.gemini_api_key:
        from google import genai
        client = genai.Client(api_key=settings.gemini_api_key)
        response = await client.aio.models.generate_content(
            model=settings.llm_model,
            contents=prompt,
            config={"max_output_tokens": 2048, "temperature": 0.2}
        )
        return response.text

    else:
        raise ValueError("No LLM API configured")


def _parse_suggestion_response(response: str) -> dict:
    """
    Parse LLM response to JSON
    è§£æ LLM å“åº”ä¸º JSON
    """
    import json

    # Clean response (remove markdown code blocks if present)
    # æ¸…ç†å“åº”
    response = response.strip()
    if response.startswith("```"):
        lines = response.split("\n")
        lines = [l for l in lines if not l.strip().startswith("```")]
        response = "\n".join(lines)
    response = response.strip()

    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # Try to extract JSON from response
        # å°è¯•ä»å“åº”ä¸­æå– JSON
        import re
        match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except:
                pass
        # Return default
        return {
            "rewrite_suggestion_zh": "ã€é—®é¢˜è¯Šæ–­ã€‘æ— æ³•è§£æåˆ†æç»“æœ\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·é‡è¯•\nã€æ”¹å†™æç¤ºã€‘å»ºè®®æ£€æŸ¥æ®µè½ç»“æ„",
            "rewrite_example": None,
            "ai_risk": "unknown",
            "ai_risk_reason": "åˆ†æå¤±è´¥"
        }


# =============================================================================
# Issue-Specific Suggestion Endpoint (Step 1-1 Click-to-Expand)
# é’ˆå¯¹ç‰¹å®šé—®é¢˜çš„å»ºè®®ç«¯ç‚¹ï¼ˆStep 1-1 ç‚¹å‡»å±•å¼€ï¼‰
# =============================================================================

@router.post("/issue-suggestion")
async def get_issue_suggestion(
    request: dict,
    db: AsyncSession = Depends(get_db)
):
    """
    Get detailed suggestion for a specific structure issue
    è·å–é’ˆå¯¹ç‰¹å®šç»“æ„é—®é¢˜çš„è¯¦ç»†å»ºè®®

    This endpoint uses LLM with comprehensive De-AIGC knowledge to generate:
    - Detailed diagnosis of the issue
    - Multiple modification strategies
    - A complete modification prompt for other AI tools
    - Priority tips and cautions

    Args:
        request: Issue suggestion request dict é—®é¢˜å»ºè®®è¯·æ±‚å­—å…¸
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Dict with detailed suggestions åŒ…å«è¯¦ç»†å»ºè®®çš„å­—å…¸
    """
    import json
    import httpx
    import re
    from src.config import get_settings
    from src.prompts.structure_deaigc import format_issue_prompt

    try:
        settings = get_settings()

        # Extract request fields
        # æå–è¯·æ±‚å­—æ®µ
        document_id = request.get("documentId", "")
        issue_type = request.get("issueType", "unknown")
        issue_description = request.get("issueDescription", "")
        issue_description_zh = request.get("issueDescriptionZh", "")
        severity = request.get("severity", "medium")
        affected_positions = request.get("affectedPositions", [])
        quick_mode = request.get("quickMode", False)

        # Get document for context
        # è·å–æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
        document = await db.get(Document, document_id) if document_id else None
        document_excerpt = ""
        total_sections = 0
        total_paragraphs = 0
        structure_score = 50
        risk_level = "medium"

        if document:
            document_excerpt = document.original_text[:3000] if document.original_text else ""
            # Get cached analysis if available
            # è·å–ç¼“å­˜çš„åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            if document.structure_analysis_cache:
                cache = document.structure_analysis_cache
                if "step1_1_cache" in cache:
                    step1_cache = cache["step1_1_cache"]
                    total_sections = step1_cache.get("totalSections", len(step1_cache.get("sections", [])))
                    total_paragraphs = step1_cache.get("totalParagraphs", 0)
                    structure_score = step1_cache.get("structureScore", 50)
                    risk_level = step1_cache.get("riskLevel", "medium")

        # Build prompt
        # æ„å»ºæç¤ºè¯
        prompt = format_issue_prompt(
            issue_type=issue_type,
            issue_description=issue_description,
            issue_description_zh=issue_description_zh,
            severity=severity,
            affected_positions=affected_positions,
            total_sections=total_sections,
            total_paragraphs=total_paragraphs,
            structure_score=structure_score,
            risk_level=risk_level,
            document_excerpt=document_excerpt,
            use_quick_mode=quick_mode
        )

        # Call LLM API
        # è°ƒç”¨ LLM API
        response_text = ""

        # Use Volcengine (preferred)
        # ä½¿ç”¨ç«å±±å¼•æ“ï¼ˆé¦–é€‰ï¼‰
        if settings.llm_provider == "volcengine" and settings.volcengine_api_key:
            async with httpx.AsyncClient(
                base_url=settings.volcengine_base_url,
                headers={
                    "Authorization": f"Bearer {settings.volcengine_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=90.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.volcengine_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4096,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        # Use DeepSeek
        # ä½¿ç”¨ DeepSeek
        elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
            async with httpx.AsyncClient(
                base_url=settings.deepseek_base_url,
                headers={
                    "Authorization": f"Bearer {settings.deepseek_api_key}",
                    "Content-Type": "application/json"
                },
                timeout=90.0,
                trust_env=False
            ) as client:
                response = await client.post("/chat/completions", json={
                    "model": settings.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4096,
                    "temperature": 0.3
                })
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"]

        # Use Gemini
        # ä½¿ç”¨ Gemini
        elif settings.llm_provider == "gemini" and settings.gemini_api_key:
            from google import genai
            client = genai.Client(api_key=settings.gemini_api_key)
            gen_response = await client.aio.models.generate_content(
                model=settings.llm_model,
                contents=prompt,
                config={"max_output_tokens": 4096, "temperature": 0.3}
            )
            response_text = gen_response.text

        else:
            raise ValueError("No LLM API configured")

        # Parse response
        # è§£æå“åº”
        response_text = response_text.strip()
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            lines = [l for l in lines if not l.strip().startswith("```")]
            response_text = "\n".join(lines)
        response_text = response_text.strip()

        result = {}
        try:
            result = json.loads(response_text)
        except json.JSONDecodeError:
            # Try to extract JSON
            # å°è¯•æå– JSON
            match = re.search(r'\{[\s\S]*\}', response_text)
            if match:
                try:
                    result = json.loads(match.group())
                except:
                    pass

        # Return response
        # è¿”å›å“åº”
        if quick_mode:
            return {
                "diagnosisZh": result.get("diagnosis_zh", "åˆ†æç»“æœè§£æå¤±è´¥"),
                "quickFixZh": result.get("quick_fix_zh", "å»ºè®®ç§»é™¤æ˜¾æ€§è¿æ¥è¯"),
                "detailedStrategyZh": result.get("detailed_strategy_zh", ""),
                "promptSnippet": result.get("prompt_snippet", ""),
                "estimatedImprovement": result.get("estimated_improvement", 10)
            }

        return {
            "diagnosisZh": result.get("diagnosis_zh", "åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"),
            "strategies": result.get("strategies", []),
            "modificationPrompt": result.get("modification_prompt", ""),
            "priorityTipsZh": result.get("priority_tips_zh", ""),
            "cautionZh": result.get("caution_zh", "è¯·ç¡®ä¿ä¿®æ”¹åä»ç¬¦åˆå­¦æœ¯è§„èŒƒ")
        }

    except Exception as e:
        logger.error(f"Issue suggestion error: {e}")
        return {
            "diagnosisZh": f"ã€åˆ†æå¤±è´¥ã€‘æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
            "strategies": [],
            "modificationPrompt": "",
            "priorityTipsZh": "å»ºè®®ï¼šåˆ é™¤æ®µé¦–æ˜¾æ€§è¿æ¥è¯ï¼Œä½¿ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥",
            "cautionZh": "è¯·ç¡®ä¿ä¿®æ”¹åä»ç¬¦åˆå­¦æœ¯è§„èŒƒ"
        }


# =============================================================================
# Merge Modify Endpoints (Step 1-1 Combined Issue Modification)
# åˆå¹¶ä¿®æ”¹ç«¯ç‚¹ï¼ˆStep 1-1 å¤šé—®é¢˜åˆå¹¶ä¿®æ”¹ï¼‰
# =============================================================================

# Prompt template for generating merge modification prompt
# ç”Ÿæˆåˆå¹¶ä¿®æ”¹æç¤ºè¯çš„æ¨¡æ¿
MERGE_MODIFY_PROMPT_TEMPLATE = """You are a professional academic writing editor. Generate a modification prompt that can be used to fix the following issues in a document.

## TARGET STYLE LEVEL: {colloquialism_level}/10
(0 = Most Academic/Formal, 10 = Most Casual/Conversational)
{style_description}

{previous_improvements}

{semantic_echo_context}

## ISSUES TO ADDRESS:
{issues_list}

## USER'S ADDITIONAL NOTES:
{user_notes}

## YOUR TASK:
Generate a comprehensive, ACTIONABLE prompt that another AI can use to modify the document.
The prompt should:
1. Address ALL selected issues
2. Maintain the target style level ({colloquialism_level}/10)
3. Preserve the original meaning and content
4. Be specific about what to change (remove connectors, restructure sentences, etc.)
5. Be written in the SAME LANGUAGE as the document
6. IMPORTANT: Preserve all previous improvements from Step 1-1 (if any)
7. **For connector issues: Include the specific semantic echo replacements provided above**

## OUTPUT FORMAT (JSON):
{{
  "prompt": "Your detailed modification prompt here...",
  "prompt_zh": "ç®€è¦è¯´æ˜è¿™ä¸ªæç¤ºè¯çš„ä½œç”¨",
  "issues_summary_zh": "å·²é€‰é—®é¢˜æ‘˜è¦ï¼š...",
  "estimated_changes": 5
}}

CRITICAL: The prompt must be actionable and specific. Include examples of patterns to remove/change.
CRITICAL: The generated prompt MUST explicitly mention preserving previous improvements to avoid reverting changes.
CRITICAL: If semantic echo replacements are provided, the generated prompt MUST include these specific replacements.
"""

# Prompt template for direct modification
# ç›´æ¥ä¿®æ”¹çš„æç¤ºè¯æ¨¡æ¿
MERGE_MODIFY_APPLY_TEMPLATE = """You are a professional academic writing editor specializing in De-AIGC (removing AI-writing patterns).

## DOCUMENT TO MODIFY:
{document_text}

## TARGET STYLE LEVEL: {colloquialism_level}/10
(0 = Most Academic/Formal, 10 = Most Casual/Conversational)
{style_description}

{previous_improvements}

{semantic_echo_context}

## ISSUES TO FIX:
{issues_list}

## USER'S ADDITIONAL NOTES:
{user_notes}

## YOUR TASK:
Modify the document to address ALL the listed issues while:
1. Maintaining the target style level ({colloquialism_level}/10)
2. Preserving the original meaning and structure
3. Keeping the output in the SAME LANGUAGE as the input
4. Making natural-sounding changes that a human would write
5. **CRITICAL: Preserve all improvements from previous steps (Step 1-1) - DO NOT revert any changes already made**
6. **For connector issues: USE the specific semantic echo replacements provided above**

## MODIFICATION GUIDELINES:
- Remove explicit connector words (Furthermore, Moreover, Additionally, æ­¤å¤–, å¦å¤–, etc.)
- Use semantic echo (repeat key concepts from previous paragraph) instead of connectors
- **When semantic echo replacements are provided above, USE THEM DIRECTLY**
- Break up formulaic sentence patterns
- Vary sentence length and structure
- Avoid AI-typical patterns like "First... Second... Third..."
- **Keep all previous improvements intact - only add new improvements, never revert**

## OUTPUT FORMAT (JSON):
{{
  "modified_text": "Your complete modified document here...",
  "changes_summary_zh": "ä¿®æ”¹æ‘˜è¦ï¼š1. ...; 2. ...; 3. ...",
  "changes_count": 5,
  "issues_addressed": ["connector_overuse", "linear_flow"]
}}

CRITICAL: Output the COMPLETE modified document, not just the changed parts. Preserve all content not related to the issues.
CRITICAL: If previous improvements exist, you MUST maintain them. Only make additional improvements, never revert to original patterns.
CRITICAL: If semantic echo replacements are provided, you MUST use these exact replacements in the modified text.
"""

# Style level descriptions
# é£æ ¼çº§åˆ«æè¿°
STYLE_LEVEL_DESCRIPTIONS = {
    0: "Extremely formal academic writing. Use precise terminology, complex sentence structures, passive voice, and formal transitions.",
    1: "Very formal academic style. Maintain scholarly tone with occasional active voice.",
    2: "Formal academic writing. Standard academic conventions with clear, professional language.",
    3: "Academic with moderate formality. Clear and professional but not overly stiff.",
    4: "Semi-formal academic. Accessible academic writing with some conversational elements.",
    5: "Balanced style. Mix of academic precision and readable prose.",
    6: "Semi-casual professional. Clear, direct language with minimal jargon.",
    7: "Casual professional. Conversational but still professional.",
    8: "Casual writing. Friendly, conversational tone.",
    9: "Very casual. Informal, personal writing style.",
    10: "Most casual. Highly conversational, like talking to a friend."
}


@router.post("/merge-modify/prompt", response_model=MergeModifyPromptResponse)
async def generate_merge_modify_prompt(
    request: MergeModifyRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Generate a modification prompt for selected issues
    ä¸ºé€‰å®šçš„é—®é¢˜ç”Ÿæˆä¿®æ”¹æç¤ºè¯

    User can copy this prompt to use with other AI tools.
    ç”¨æˆ·å¯ä»¥å¤åˆ¶æ­¤æç¤ºè¯ç”¨äºå…¶ä»–AIå·¥å…·ã€‚

    Args:
        request: Merge modify request åˆå¹¶ä¿®æ”¹è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        MergeModifyPromptResponse with generated prompt åŒ…å«ç”Ÿæˆæç¤ºè¯çš„å“åº”
    """
    import json
    import httpx
    import re
    from src.config import get_settings

    try:
        settings = get_settings()

        # Get document to access Step 1-1 cache
        # è·å–æ–‡æ¡£ä»¥è®¿é—® Step 1-1 ç¼“å­˜
        document = await db.get(Document, request.document_id)

        # Get colloquialism level from session
        # ä»ä¼šè¯è·å–å£è¯­åŒ–çº§åˆ«
        colloquialism_level = 3  # Default to semi-formal
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session and session.colloquialism_level is not None:
                colloquialism_level = session.colloquialism_level

        style_description = STYLE_LEVEL_DESCRIPTIONS.get(colloquialism_level, STYLE_LEVEL_DESCRIPTIONS[3])

        # Build previous improvements context from Step 1-1 cache
        # ä» Step 1-1 ç¼“å­˜æ„å»ºä¹‹å‰çš„æ”¹è¿›ä¸Šä¸‹æ–‡
        previous_improvements = _build_previous_improvements_context(document)

        # Build semantic echo context from Step 1-2 cache
        # ä» Step 1-2 ç¼“å­˜æ„å»ºè¯­ä¹‰å›å£°ä¸Šä¸‹æ–‡
        semantic_echo_context = _build_semantic_echo_context(document)

        # Build issues list
        # æ„å»ºé—®é¢˜åˆ—è¡¨
        issues_list = ""
        for i, issue in enumerate(request.selected_issues, 1):
            issues_list += f"{i}. [{issue.severity.upper()}] {issue.description_zh}\n"
            if issue.affected_positions:
                issues_list += f"   Affected positions: {', '.join(issue.affected_positions)}\n"

        # Build prompt for LLM
        # æ„å»º LLM æç¤ºè¯
        prompt = MERGE_MODIFY_PROMPT_TEMPLATE.format(
            colloquialism_level=colloquialism_level,
            style_description=style_description,
            previous_improvements=previous_improvements,
            semantic_echo_context=semantic_echo_context,
            issues_list=issues_list,
            user_notes=request.user_notes or "No additional notes"
        )

        # Call LLM
        # è°ƒç”¨ LLM
        response_text = await _call_llm_for_merge_modify(prompt, settings, max_tokens=2048)

        # Parse response
        # è§£æå“åº”
        result = _parse_json_response(response_text)

        return MergeModifyPromptResponse(
            prompt=result.get("prompt", "ç”Ÿæˆæç¤ºè¯å¤±è´¥ï¼Œè¯·é‡è¯•"),
            prompt_zh=result.get("prompt_zh", "ä¿®æ”¹æç¤ºè¯"),
            issues_summary_zh=result.get("issues_summary_zh", f"å·²é€‰æ‹© {len(request.selected_issues)} ä¸ªé—®é¢˜"),
            colloquialism_level=colloquialism_level,
            estimated_changes=result.get("estimated_changes", len(request.selected_issues))
        )

    except Exception as e:
        logger.error(f"Generate merge modify prompt error: {e}")
        # Return a fallback prompt
        # è¿”å›åå¤‡æç¤ºè¯
        fallback_prompt = _generate_fallback_prompt(request.selected_issues, request.user_notes)
        return MergeModifyPromptResponse(
            prompt=fallback_prompt,
            prompt_zh="å·²ç”ŸæˆåŸºç¡€ä¿®æ”¹æç¤ºè¯",
            issues_summary_zh=f"å·²é€‰æ‹© {len(request.selected_issues)} ä¸ªé—®é¢˜",
            colloquialism_level=3,
            estimated_changes=len(request.selected_issues)
        )


@router.post("/merge-modify/apply", response_model=MergeModifyApplyResponse)
async def apply_merge_modify(
    request: MergeModifyRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Apply AI modification to document directly
    ç›´æ¥åº”ç”¨AIä¿®æ”¹åˆ°æ–‡æ¡£

    Args:
        request: Merge modify request åˆå¹¶ä¿®æ”¹è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        MergeModifyApplyResponse with modified document åŒ…å«ä¿®æ”¹åæ–‡æ¡£çš„å“åº”
    """
    import json
    import httpx
    import re
    from src.config import get_settings

    try:
        settings = get_settings()

        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Get colloquialism level from session
        # ä»ä¼šè¯è·å–å£è¯­åŒ–çº§åˆ«
        colloquialism_level = 3  # Default to semi-formal
        if request.session_id:
            session = await db.get(Session, request.session_id)
            if session and session.colloquialism_level is not None:
                colloquialism_level = session.colloquialism_level

        style_description = STYLE_LEVEL_DESCRIPTIONS.get(colloquialism_level, STYLE_LEVEL_DESCRIPTIONS[3])

        # Build previous improvements context from Step 1-1 cache
        # ä» Step 1-1 ç¼“å­˜æ„å»ºä¹‹å‰çš„æ”¹è¿›ä¸Šä¸‹æ–‡
        previous_improvements = _build_previous_improvements_context(document)

        # Build semantic echo context from Step 1-2 cache
        # ä» Step 1-2 ç¼“å­˜æ„å»ºè¯­ä¹‰å›å£°ä¸Šä¸‹æ–‡
        semantic_echo_context = _build_semantic_echo_context(document)

        # Build issues list
        # æ„å»ºé—®é¢˜åˆ—è¡¨
        issues_list = ""
        for i, issue in enumerate(request.selected_issues, 1):
            issues_list += f"{i}. [{issue.severity.upper()}] {issue.description_zh}\n"
            if issue.affected_positions:
                issues_list += f"   Affected: {', '.join(issue.affected_positions)}\n"

        # Build prompt for LLM
        # æ„å»º LLM æç¤ºè¯
        prompt = MERGE_MODIFY_APPLY_TEMPLATE.format(
            document_text=document.original_text[:15000],  # Limit to avoid token overflow
            colloquialism_level=colloquialism_level,
            style_description=style_description,
            previous_improvements=previous_improvements,
            semantic_echo_context=semantic_echo_context,
            issues_list=issues_list,
            user_notes=request.user_notes or "No additional notes"
        )

        # Call LLM with longer timeout for document modification
        # è°ƒç”¨ LLMï¼Œæ–‡æ¡£ä¿®æ”¹éœ€è¦æ›´é•¿è¶…æ—¶
        response_text = await _call_llm_for_merge_modify(prompt, settings, max_tokens=8192, timeout=120.0)

        # Parse response
        # è§£æå“åº”
        result = _parse_json_response(response_text)

        return MergeModifyApplyResponse(
            modified_text=result.get("modified_text", document.original_text),
            changes_summary_zh=result.get("changes_summary_zh", "ä¿®æ”¹å·²å®Œæˆ"),
            changes_count=result.get("changes_count", 0),
            issues_addressed=result.get("issues_addressed", []),
            remaining_attempts=2,  # 3 total, 1 used
            colloquialism_level=colloquialism_level
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Apply merge modify error: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿®æ”¹å¤±è´¥: {str(e)}")


async def _call_llm_for_merge_modify(prompt: str, settings, max_tokens: int = 4096, timeout: float = 90.0) -> str:
    """
    Call LLM API for merge modification
    è°ƒç”¨ LLM API è¿›è¡Œåˆå¹¶ä¿®æ”¹
    """
    import httpx

    # Use Volcengine (preferred)
    # ä½¿ç”¨ç«å±±å¼•æ“ï¼ˆé¦–é€‰ï¼‰
    if settings.llm_provider == "volcengine" and settings.volcengine_api_key:
        async with httpx.AsyncClient(
            base_url=settings.volcengine_base_url,
            headers={
                "Authorization": f"Bearer {settings.volcengine_api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.volcengine_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.3
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use DeepSeek
    # ä½¿ç”¨ DeepSeek
    elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
        async with httpx.AsyncClient(
            base_url=settings.deepseek_base_url,
            headers={
                "Authorization": f"Bearer {settings.deepseek_api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.llm_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.3
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use Gemini
    # ä½¿ç”¨ Gemini
    elif settings.llm_provider == "gemini" and settings.gemini_api_key:
        from google import genai
        client = genai.Client(api_key=settings.gemini_api_key)
        response = await client.aio.models.generate_content(
            model=settings.llm_model,
            contents=prompt,
            config={"max_output_tokens": max_tokens, "temperature": 0.3}
        )
        return response.text

    else:
        raise ValueError("No LLM API configured")


def _parse_json_response(response: str) -> dict:
    """
    Parse LLM response to JSON
    è§£æ LLM å“åº”ä¸º JSON
    """
    import json
    import re

    # Clean response
    # æ¸…ç†å“åº”
    response = response.strip()
    if response.startswith("```"):
        lines = response.split("\n")
        lines = [l for l in lines if not l.strip().startswith("```")]
        response = "\n".join(lines)
    response = response.strip()

    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # Try to extract JSON from response
        # å°è¯•ä»å“åº”ä¸­æå– JSON
        match = re.search(r'\{[\s\S]*\}', response)
        if match:
            try:
                return json.loads(match.group())
            except:
                pass
        return {}


def _build_previous_improvements_context(document) -> str:
    """
    Build context about previous improvements from Step 1-1 analysis.
    ä» Step 1-1 åˆ†æç»“æœæ„å»ºä¹‹å‰æ”¹è¿›çš„ä¸Šä¸‹æ–‡ã€‚

    This helps LLM understand what changes were already suggested/made
    so it doesn't revert them in subsequent modifications.
    è¿™å¸®åŠ© LLM äº†è§£å·²ç»å»ºè®®/å®Œæˆçš„æ”¹è¿›ï¼Œé¿å…åœ¨åç»­ä¿®æ”¹ä¸­æ’¤é”€å®ƒä»¬ã€‚

    Args:
        document: Document model with structure_analysis_cache

    Returns:
        Formatted string describing previous improvements
    """
    if not document or not document.structure_analysis_cache:
        return ""

    cache = document.structure_analysis_cache
    improvements = []

    # Extract Step 1-1 issues that were identified
    # æå– Step 1-1 ä¸­è¯†åˆ«çš„é—®é¢˜
    step1_1_cache = cache.get("step1_1_cache", {})
    if step1_1_cache:
        structure_issues = step1_1_cache.get("structureIssues") or step1_1_cache.get("structure_issues", [])
        if structure_issues:
            for issue in structure_issues[:5]:  # Limit to top 5 to avoid prompt bloat
                desc = issue.get("descriptionZh") or issue.get("description_zh") or issue.get("description", "")
                if desc:
                    improvements.append(f"- {desc}")

        # Include style analysis context
        # åŒ…å«é£æ ¼åˆ†æä¸Šä¸‹æ–‡
        style_analysis = step1_1_cache.get("styleAnalysis") or step1_1_cache.get("style_analysis", {})
        if style_analysis:
            style_name = style_analysis.get("styleNameZh") or style_analysis.get("style_name_zh", "")
            if style_name:
                improvements.append(f"- æ–‡æ¡£åŸå§‹é£æ ¼: {style_name}")

    # Extract Step 1-2 issues if available
    # æå– Step 1-2 çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
    step1_2_cache = cache.get("step1_2_cache", {})
    if step1_2_cache:
        # Include relationship issues context
        # åŒ…å«å…³ç³»é—®é¢˜ä¸Šä¸‹æ–‡
        relationship_issues = step1_2_cache.get("relationshipIssues") or step1_2_cache.get("relationship_issues", [])
        if relationship_issues:
            for issue in relationship_issues[:3]:  # Limit to top 3
                desc = issue.get("descriptionZh") or issue.get("description_zh") or issue.get("description", "")
                if desc:
                    improvements.append(f"- {desc}")

    if not improvements:
        return ""

    # Build the context block
    # æ„å»ºä¸Šä¸‹æ–‡å—
    improvements_text = "\n".join(improvements)
    return f"""## âš ï¸ PREVIOUS ANALYSIS CONTEXT (MUST PRESERVE):
The document has been analyzed in previous steps. The following issues/improvements were identified:
åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­å·²å¯¹æ–‡æ¡£è¿›è¡Œäº†åˆ†æï¼Œè¯†åˆ«å‡ºä»¥ä¸‹é—®é¢˜/æ”¹è¿›ç‚¹ï¼š

{improvements_text}

**CRITICAL INSTRUCTION å…³é”®æŒ‡ä»¤:**
- You MUST preserve any improvements already made based on these issues
- DO NOT revert the document to patterns that were flagged as problematic
- Only make NEW improvements for the current issues, while keeping previous changes intact
- å¿…é¡»ä¿ç•™å·²æ ¹æ®è¿™äº›é—®é¢˜æ‰€åšçš„æ”¹è¿›
- ä¸è¦å°†æ–‡æ¡£æ¢å¤åˆ°è¢«æ ‡è®°ä¸ºæœ‰é—®é¢˜çš„æ¨¡å¼
- ä»…å¯¹å½“å‰é—®é¢˜è¿›è¡Œæ–°çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒä¹‹å‰çš„æ›´æ”¹ä¸å˜
"""


def _build_semantic_echo_context(document) -> str:
    """
    Build semantic echo replacement context from Step 1-2 analysis.
    ä» Step 1-2 åˆ†æç»“æœæ„å»ºè¯­ä¹‰å›å£°æ›¿æ¢ä¸Šä¸‹æ–‡ã€‚

    This provides LLM with specific replacement examples for explicit connectors.
    è¿™ä¸º LLM æä¾›æ˜¾æ€§è¿æ¥è¯çš„å…·ä½“æ›¿æ¢ç¤ºä¾‹ã€‚

    Args:
        document: Document model with structure_analysis_cache

    Returns:
        Formatted string with semantic echo replacements
    """
    if not document or not document.structure_analysis_cache:
        return ""

    cache = document.structure_analysis_cache
    replacements = []

    # Extract semantic echo replacements from Step 1-2 cache
    # ä» Step 1-2 ç¼“å­˜æå–è¯­ä¹‰å›å£°æ›¿æ¢
    step1_2_cache = cache.get("step1_2_cache", {})
    if step1_2_cache:
        # Get explicit connectors with replacements
        # è·å–å¸¦æœ‰æ›¿æ¢çš„æ˜¾æ€§è¿æ¥è¯
        explicit_connectors = step1_2_cache.get("explicit_connectors") or step1_2_cache.get("explicitConnectors", [])
        for conn in explicit_connectors:
            word = conn.get("word", "")
            position = conn.get("position", "")
            current_opening = conn.get("current_opening") or conn.get("currentOpening", "")
            replacement = conn.get("semantic_echo_replacement") or conn.get("semanticEchoReplacement", "")
            prev_concepts = conn.get("prev_key_concepts") or conn.get("prevKeyConcepts", [])
            explanation = conn.get("replacement_explanation_zh") or conn.get("replacementExplanationZh", "")

            if current_opening and replacement:
                concepts_str = ", ".join(prev_concepts) if prev_concepts else "N/A"
                replacements.append(f"""
### ä½ç½® {position}: "{word}"
- **åŸæ–‡**: {current_opening}
- **å‰æ®µå…³é”®æ¦‚å¿µ**: {concepts_str}
- **è¯­ä¹‰å›å£°æ›¿æ¢**: {replacement}
- **è¯´æ˜**: {explanation}""")

    # Also check Step 1-1 for any connector issues with replacements
    # åŒæ—¶æ£€æŸ¥ Step 1-1 æ˜¯å¦æœ‰å¸¦æ›¿æ¢çš„è¿æ¥è¯é—®é¢˜
    step1_1_cache = cache.get("step1_1_cache", {})
    if step1_1_cache:
        structure_issues = step1_1_cache.get("structureIssues") or step1_1_cache.get("structure_issues", [])
        for issue in structure_issues:
            issue_type = issue.get("type", "")
            if issue_type == "explicit_connector":
                original = issue.get("originalText") or issue.get("original_text", "")
                replacement = issue.get("semanticEchoReplacement") or issue.get("semantic_echo_replacement", "")
                if original and replacement and len(replacements) < 10:  # Limit total
                    replacements.append(f"""
### è¿æ¥è¯é—®é¢˜
- **åŸæ–‡**: {original}
- **è¯­ä¹‰å›å£°æ›¿æ¢**: {replacement}""")

    if not replacements:
        return ""

    # Build the context block
    # æ„å»ºä¸Šä¸‹æ–‡å—
    replacements_text = "\n".join(replacements)
    return f"""## ğŸ”„ SEMANTIC ECHO REPLACEMENTS (è¯­ä¹‰å›å£°æ›¿æ¢ - å¿…é¡»ä½¿ç”¨):
The following specific replacements have been generated for explicit connector words.
**YOU MUST use these exact replacements in the modified text.**
ä»¥ä¸‹æ˜¯é’ˆå¯¹æ˜¾æ€§è¿æ¥è¯ç”Ÿæˆçš„å…·ä½“æ›¿æ¢æ–¹æ¡ˆã€‚**æ‚¨å¿…é¡»åœ¨ä¿®æ”¹åçš„æ–‡æœ¬ä¸­ä½¿ç”¨è¿™äº›æ›¿æ¢ã€‚**

{replacements_text}

**HOW TO USE ä½¿ç”¨æ–¹æ³•:**
1. Find each original text in the document
2. Replace it with the semantic echo replacement
3. The replacement uses key concepts from the previous paragraph to create natural flow
4. Do NOT add back any explicit connectors

1. åœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°æ¯ä¸ªåŸæ–‡
2. ç”¨è¯­ä¹‰å›å£°æ›¿æ¢è¿›è¡Œæ›¿æ¢
3. æ›¿æ¢ä½¿ç”¨å‰ä¸€æ®µçš„å…³é”®æ¦‚å¿µæ¥åˆ›å»ºè‡ªç„¶çš„è¡”æ¥
4. ä¸è¦å†æ·»åŠ ä»»ä½•æ˜¾æ€§è¿æ¥è¯
"""


def _generate_fallback_prompt(selected_issues: list, user_notes: str = None) -> str:
    """
    Generate a fallback prompt when LLM fails
    å½“ LLM å¤±è´¥æ—¶ç”Ÿæˆåå¤‡æç¤ºè¯
    """
    issues_text = "\n".join([
        f"- {issue.description_zh}" for issue in selected_issues
    ])

    prompt = f"""è¯·ä¿®æ”¹ä»¥ä¸‹æ–‡æ¡£ï¼Œè§£å†³è¿™äº›é—®é¢˜ï¼š

{issues_text}

ä¿®æ”¹è¦æ±‚ï¼š
1. åˆ é™¤æ˜¾æ€§è¿æ¥è¯ï¼ˆå¦‚ï¼šFurthermore, Moreover, æ­¤å¤–, å¦å¤–ç­‰ï¼‰
2. ä½¿ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥ä¸Šä¸‹æ–‡
3. æ‰“ç ´å…¬å¼åŒ–çš„å¥å­ç»“æ„
4. ä¿æŒåŸæ–‡çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
5. è¾“å‡ºè¯­è¨€ä¸åŸæ–‡ä¿æŒä¸€è‡´

"""
    if user_notes:
        prompt += f"ç”¨æˆ·æ³¨æ„äº‹é¡¹ï¼š{user_notes}\n"

    return prompt
