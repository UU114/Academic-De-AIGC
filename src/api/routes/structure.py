"""
Structure Analysis API Routes (Level 1 De-AIGC)
ç»“æ„åˆ†æ API è·¯ç”±ï¼ˆLevel 1 De-AIGCï¼‰

Phase 4: Document structure analysis and restructuring endpoints
ç¬¬4é˜¶æ®µï¼šæ–‡æ¡£ç»“æ„åˆ†æå’Œé‡ç»„ç«¯ç‚¹
"""

from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional
import logging

from src.api.schemas import (
    StructureAnalysisRequest,
    StructureAnalysisResponse,
    StructureStrategy,
    StructureOption,
    LogicDiagnosisResponse,
    DocumentStructureRequest,
    ParagraphInfo,
    StructureIssue,
    BreakPoint,
    FlowRelation,
    RiskArea,
    StructureModification,
    StructureChange,
    SmartStructureResponse,
    SmartStructureIssue,
    SectionInfo,
    SmartParagraphInfo,
    ExplicitConnector,
    LogicBreak,
    # Enhanced schemas (Level 1 Enhancement)
    PredictabilityAnalysisRequest,
    PredictabilityAnalysisResponse,
    ProgressionAnalysisResult,
    FunctionDistributionResult,
    ClosureAnalysisResult,
    LexicalEchoResult,
    DisruptionRestructureRequest,
    DisruptionRestructureResponse,
    DisruptionLevel,
    # 7-Indicator Risk Card schemas
    StructuralIndicatorResponse,
    StructuralRiskCardResponse,
    CrossReferenceResult,
    RiskCardRequest,
    # Single paragraph suggestion schemas
    ParagraphSuggestionRequest,
    ParagraphSuggestionResponse,
)
from src.core.analyzer.structure import StructureAnalyzer
from src.core.analyzer.smart_structure import SmartStructureAnalyzer
from src.prompts.structure import DISRUPTION_LEVELS, DISRUPTION_STRATEGIES
from src.db.database import get_db
from src.db.models import Document

logger = logging.getLogger(__name__)

router = APIRouter()

# Initialize analyzers
# åˆå§‹åŒ–åˆ†æå™¨
structure_analyzer = StructureAnalyzer()
smart_analyzer = SmartStructureAnalyzer()


@router.post("/", response_model=StructureAnalysisResponse)
async def analyze_structure(request: StructureAnalysisRequest):
    """
    Analyze document structure for AI patterns
    åˆ†ææ–‡æ¡£ç»“æ„çš„AIæ¨¡å¼

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureAnalysisResponse with analysis results åŒ…å«åˆ†æç»“æœçš„å“åº”
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Convert paragraph info to API format
        # å°†æ®µè½ä¿¡æ¯è½¬æ¢ä¸ºAPIæ ¼å¼
        paragraphs = [
            ParagraphInfo(
                index=p.index,
                first_sentence=p.first_sentence[:200] if len(p.first_sentence) > 200 else p.first_sentence,
                last_sentence=p.last_sentence[:200] if len(p.last_sentence) > 200 else p.last_sentence,
                word_count=p.word_count,
                sentence_count=p.sentence_count,
                has_topic_sentence=p.has_topic_sentence,
                has_summary_ending=p.has_summary_ending,
                connector_words=p.connector_words,
                function_type=p.function_type
            )
            for p in result.paragraphs
        ]

        # Convert issues
        # è½¬æ¢é—®é¢˜
        issues = [
            StructureIssue(
                type=i.type,
                description=i.description,
                description_zh=i.description_zh,
                severity=i.severity,
                affected_paragraphs=i.affected_paragraphs,
                suggestion=i.suggestion,
                suggestion_zh=i.suggestion_zh
            )
            for i in result.issues
        ]

        # Convert break points
        # è½¬æ¢æ–­ç‚¹
        break_points = [
            BreakPoint(
                position=bp.position,
                type=bp.type,
                description=bp.description,
                description_zh=bp.description_zh
            )
            for bp in result.break_points
        ]

        return StructureAnalysisResponse(
            total_paragraphs=result.total_paragraphs,
            total_sentences=result.total_sentences,
            total_words=result.total_words,
            avg_paragraph_length=result.avg_paragraph_length,
            paragraph_length_variance=result.paragraph_length_variance,
            structure_score=result.structure_score,
            risk_level=result.risk_level,
            paragraphs=paragraphs,
            issues=issues,
            break_points=break_points,
            core_thesis=result.core_thesis,
            key_arguments=result.key_arguments,
            has_linear_flow=result.has_linear_flow,
            has_repetitive_pattern=result.has_repetitive_pattern,
            has_uniform_length=result.has_uniform_length,
            has_predictable_order=result.has_predictable_order,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/with-suggestions", response_model=StructureAnalysisResponse)
async def analyze_structure_with_suggestions(request: StructureAnalysisRequest):
    """
    Analyze document structure and generate restructuring suggestions
    åˆ†ææ–‡æ¡£ç»“æ„å¹¶ç”Ÿæˆé‡ç»„å»ºè®®

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureAnalysisResponse with analysis and suggestions åŒ…å«åˆ†æå’Œå»ºè®®çš„å“åº”
    """
    try:
        # Perform base analysis
        # æ‰§è¡ŒåŸºç¡€åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Generate suggestions based on issues
        # æ ¹æ®é—®é¢˜ç”Ÿæˆå»ºè®®
        options = []

        # Only generate suggestions if there are issues
        # ä»…åœ¨å­˜åœ¨é—®é¢˜æ—¶ç”Ÿæˆå»ºè®®
        if result.issues:
            # Option 1: Optimize Connection (gentle approach)
            # é€‰é¡¹1ï¼šä¼˜åŒ–è¿æ¥ï¼ˆæ¸©å’Œæ–¹æ³•ï¼‰
            optimize_modifications = []
            for p in result.paragraphs:
                if p.has_topic_sentence or p.connector_words:
                    optimize_modifications.append(
                        StructureModification(
                            paragraph_index=p.index,
                            change_type="rewrite_opening",
                            original=p.first_sentence[:100],
                            modified=None,  # Would be filled by LLM
                            explanation_zh="ç§»é™¤æ˜¾å¼è¿æ¥è¯ï¼Œåˆ›å»ºéšå¼é€»è¾‘æµ"
                        )
                    )

            options.append(StructureOption(
                strategy=StructureStrategy.OPTIMIZE_CONNECTION,
                strategy_name_zh="ä¼˜åŒ–è¿æ¥",
                modifications=optimize_modifications[:5],  # Limit to top 5
                outline=[f"æ®µè½{i+1}: {p.function_type}" for i, p in enumerate(result.paragraphs)],
                predicted_improvement=15,
                explanation_zh="ä¿æŒæ®µè½é¡ºåºï¼Œä¼˜åŒ–æ®µè½ä¹‹é—´çš„è¡”æ¥æ–¹å¼"
            ))

            # Option 2: Deep Restructure (aggressive approach)
            # é€‰é¡¹2ï¼šæ·±åº¦é‡ç»„ï¼ˆæ¿€è¿›æ–¹æ³•ï¼‰
            if result.structure_score >= 40:
                # Suggest reordering based on function types
                # æ ¹æ®åŠŸèƒ½ç±»å‹å»ºè®®é‡æ–°æ’åº
                new_order = list(range(len(result.paragraphs)))

                # Example: Move a body paragraph to front as hook
                # ç¤ºä¾‹ï¼šå°†æ­£æ–‡æ®µè½ç§»åˆ°å‰é¢ä½œä¸ºé’©å­
                body_indices = [
                    i for i, p in enumerate(result.paragraphs)
                    if p.function_type in ["evidence", "body"]
                ]
                if body_indices and len(result.paragraphs) > 3:
                    # Move evidence paragraph after intro as hook
                    # å°†è¯æ®æ®µè½ç§»åˆ°å¼•è¨€åä½œä¸ºé’©å­
                    hook_idx = body_indices[0]
                    new_order = [0, hook_idx] + [
                        i for i in range(1, len(result.paragraphs))
                        if i != hook_idx
                    ]

                options.append(StructureOption(
                    strategy=StructureStrategy.DEEP_RESTRUCTURE,
                    strategy_name_zh="æ·±åº¦é‡ç»„",
                    new_order=new_order,
                    restructure_type="hook_cycle",
                    restructure_type_zh="é’©å­å¾ªç¯",
                    changes=[
                        StructureChange(
                            type="reorder",
                            affected_paragraphs=new_order,
                            description="Reorganize to break linear flow",
                            description_zh="é‡æ–°ç»„ç»‡ä»¥æ‰“ç ´çº¿æ€§æµç¨‹"
                        )
                    ],
                    outline=[f"æ–°ä½ç½®{i+1}: åŸæ®µè½{idx+1}" for i, idx in enumerate(new_order)],
                    predicted_improvement=25,
                    explanation_zh="é‡æ–°æ’åºæ®µè½ï¼Œæ‰“ç ´å¯é¢„æµ‹çš„ç»“æ„æ¨¡å¼"
                ))

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        paragraphs = [
            ParagraphInfo(
                index=p.index,
                first_sentence=p.first_sentence[:200] if len(p.first_sentence) > 200 else p.first_sentence,
                last_sentence=p.last_sentence[:200] if len(p.last_sentence) > 200 else p.last_sentence,
                word_count=p.word_count,
                sentence_count=p.sentence_count,
                has_topic_sentence=p.has_topic_sentence,
                has_summary_ending=p.has_summary_ending,
                connector_words=p.connector_words,
                function_type=p.function_type
            )
            for p in result.paragraphs
        ]

        issues = [
            StructureIssue(
                type=i.type,
                description=i.description,
                description_zh=i.description_zh,
                severity=i.severity,
                affected_paragraphs=i.affected_paragraphs,
                suggestion=i.suggestion,
                suggestion_zh=i.suggestion_zh
            )
            for i in result.issues
        ]

        break_points = [
            BreakPoint(
                position=bp.position,
                type=bp.type,
                description=bp.description,
                description_zh=bp.description_zh
            )
            for bp in result.break_points
        ]

        return StructureAnalysisResponse(
            total_paragraphs=result.total_paragraphs,
            total_sentences=result.total_sentences,
            total_words=result.total_words,
            avg_paragraph_length=result.avg_paragraph_length,
            paragraph_length_variance=result.paragraph_length_variance,
            structure_score=result.structure_score,
            risk_level=result.risk_level,
            paragraphs=paragraphs,
            issues=issues,
            break_points=break_points,
            core_thesis=result.core_thesis,
            key_arguments=result.key_arguments,
            has_linear_flow=result.has_linear_flow,
            has_repetitive_pattern=result.has_repetitive_pattern,
            has_uniform_length=result.has_uniform_length,
            has_predictable_order=result.has_predictable_order,
            options=options,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Structure analysis with suggestions error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/suggest/{strategy}", response_model=StructureOption)
async def get_structure_suggestion(
    strategy: StructureStrategy,
    request: StructureAnalysisRequest
):
    """
    Get suggestion for a specific restructuring strategy
    è·å–ç‰¹å®šé‡ç»„ç­–ç•¥çš„å»ºè®®

    Args:
        strategy: Restructuring strategy é‡ç»„ç­–ç•¥
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        StructureOption with specific suggestion åŒ…å«ç‰¹å®šå»ºè®®çš„é€‰é¡¹
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        if strategy == StructureStrategy.OPTIMIZE_CONNECTION:
            # Generate optimize connection suggestion
            # ç”Ÿæˆä¼˜åŒ–è¿æ¥å»ºè®®
            modifications = []
            for p in result.paragraphs:
                if p.has_topic_sentence or p.connector_words:
                    modifications.append(
                        StructureModification(
                            paragraph_index=p.index,
                            change_type="rewrite_opening",
                            original=p.first_sentence[:100],
                            modified=None,
                            explanation_zh="å»ºè®®ç§»é™¤æ˜¾å¼è¿æ¥è¯"
                        )
                    )

            return StructureOption(
                strategy=StructureStrategy.OPTIMIZE_CONNECTION,
                strategy_name_zh="ä¼˜åŒ–è¿æ¥",
                modifications=modifications[:5],
                outline=[f"æ®µè½{i+1}: {p.function_type}" for i, p in enumerate(result.paragraphs)],
                predicted_improvement=15,
                explanation_zh="ä¿æŒé¡ºåºï¼Œä¼˜åŒ–è¡”æ¥"
            )

        elif strategy == StructureStrategy.DEEP_RESTRUCTURE:
            # Generate deep restructure suggestion
            # ç”Ÿæˆæ·±åº¦é‡ç»„å»ºè®®
            new_order = list(range(len(result.paragraphs)))

            # Suggest moving evidence to front
            # å»ºè®®å°†è¯æ®ç§»åˆ°å‰é¢
            body_indices = [
                i for i, p in enumerate(result.paragraphs)
                if p.function_type in ["evidence", "body"]
            ]
            if body_indices and len(result.paragraphs) > 3:
                hook_idx = body_indices[0]
                new_order = [0, hook_idx] + [
                    i for i in range(1, len(result.paragraphs))
                    if i != hook_idx
                ]

            return StructureOption(
                strategy=StructureStrategy.DEEP_RESTRUCTURE,
                strategy_name_zh="æ·±åº¦é‡ç»„",
                new_order=new_order,
                restructure_type="hook_cycle",
                restructure_type_zh="é’©å­å¾ªç¯",
                changes=[
                    StructureChange(
                        type="reorder",
                        affected_paragraphs=new_order,
                        description="Reorganize paragraph order",
                        description_zh="é‡æ–°ç»„ç»‡æ®µè½é¡ºåº"
                    )
                ],
                outline=[f"æ–°ä½ç½®{i+1}: åŸæ®µè½{idx+1}" for i, idx in enumerate(new_order)],
                predicted_improvement=25,
                explanation_zh="æ‰“ç ´çº¿æ€§ç»“æ„ï¼Œåˆ›å»ºé’©å­å¾ªç¯æ¨¡å¼"
            )

    except Exception as e:
        logger.error(f"Get structure suggestion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/diagnosis", response_model=LogicDiagnosisResponse)
async def get_logic_diagnosis(request: StructureAnalysisRequest):
    """
    Get logic diagnosis card for document
    è·å–æ–‡æ¡£çš„é€»è¾‘è¯Šæ–­å¡

    Args:
        request: Structure analysis request ç»“æ„åˆ†æè¯·æ±‚

    Returns:
        LogicDiagnosisResponse with diagnosis card data åŒ…å«è¯Šæ–­å¡æ•°æ®çš„å“åº”
    """
    try:
        # Perform analysis
        # æ‰§è¡Œåˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=request.extract_thesis
        )

        # Generate flow map
        # ç”Ÿæˆæµç¨‹å›¾
        flow_map = []
        for i in range(len(result.paragraphs) - 1):
            curr = result.paragraphs[i]
            next_para = result.paragraphs[i + 1]

            # Determine relationship
            # ç¡®å®šå…³ç³»
            if next_para.connector_words:
                relation = "continuation"
                symbol = "â†’"
            elif curr.function_type == "evidence" and next_para.function_type == "analysis":
                relation = "evidence"
                symbol = "â¤µ"
            elif curr.function_type == next_para.function_type:
                relation = "comparison"
                symbol = "â†”"
            else:
                relation = "continuation"
                symbol = "â†’"

            # Check for gaps
            # æ£€æŸ¥é—´éš™
            for bp in result.break_points:
                if bp.position == i + 1:
                    relation = "gap"
                    symbol = "âœ—"
                    break

            flow_map.append(FlowRelation(
                **{"from": i, "to": i + 1},
                relation=relation,
                symbol=symbol
            ))

        # Determine structure pattern
        # ç¡®å®šç»“æ„æ¨¡å¼
        if result.has_linear_flow:
            pattern = "linear"
            pattern_zh = "çº¿æ€§"
        elif result.has_repetitive_pattern:
            pattern = "parallel"
            pattern_zh = "å¹¶åˆ—"
        else:
            pattern = "nested"
            pattern_zh = "åµŒå¥—"

        # Generate risk areas
        # ç”Ÿæˆé£é™©åŒºåŸŸ
        risk_areas = []
        for p in result.paragraphs:
            risk_level = "low"
            reason = ""
            reason_zh = ""

            if p.has_topic_sentence and p.connector_words:
                risk_level = "high"
                reason = "Topic sentence with explicit connectors"
                reason_zh = "ä¸»é¢˜å¥é…åˆæ˜¾å¼è¿æ¥è¯"
            elif p.has_topic_sentence:
                risk_level = "medium"
                reason = "Topic sentence pattern detected"
                reason_zh = "æ£€æµ‹åˆ°ä¸»é¢˜å¥æ¨¡å¼"
            elif p.connector_words:
                risk_level = "medium"
                reason = "Explicit connectors detected"
                reason_zh = "æ£€æµ‹åˆ°æ˜¾å¼è¿æ¥è¯"

            if risk_level != "low":
                risk_areas.append(RiskArea(
                    paragraph=p.index,
                    risk_level=risk_level,
                    reason=reason,
                    reason_zh=reason_zh
                ))

        # Determine recommended strategy
        # ç¡®å®šæ¨èç­–ç•¥
        if result.structure_score >= 40:
            recommended = StructureStrategy.DEEP_RESTRUCTURE
            rec_reason = "High structure score requires significant reorganization"
            rec_reason_zh = "é«˜ç»“æ„åˆ†æ•°éœ€è¦æ˜¾è‘—é‡ç»„"
        else:
            recommended = StructureStrategy.OPTIMIZE_CONNECTION
            rec_reason = "Moderate issues can be fixed with connection optimization"
            rec_reason_zh = "ä¸­ç­‰é—®é¢˜å¯ä»¥é€šè¿‡ä¼˜åŒ–è¿æ¥ä¿®å¤"

        return LogicDiagnosisResponse(
            flow_map=flow_map,
            structure_pattern=pattern,
            structure_pattern_zh=pattern_zh,
            pattern_description=f"Document follows a {pattern} structure pattern",
            pattern_description_zh=f"æ–‡æ¡£éµå¾ª{pattern_zh}ç»“æ„æ¨¡å¼",
            risk_areas=risk_areas,
            recommended_strategy=recommended,
            recommendation_reason=rec_reason,
            recommendation_reason_zh=rec_reason_zh
        )

    except Exception as e:
        logger.error(f"Logic diagnosis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document", response_model=SmartStructureResponse)
async def analyze_document_structure(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Analyze structure of a document by ID using smart LLM analysis
    ä½¿ç”¨æ™ºèƒ½ LLM åˆ†ææŒ‰ ID åˆ†ææ–‡æ¡£ç»“æ„

    Features:
    - Filters non-paragraph content (titles, tables, figures)
    - Uses paper section numbering (1, 1.1, 2.3.1)
    - Generates meaningful content summaries
    - Labels each paragraph with position like "3.2(1)"
    - Caches results to avoid repeated LLM calls
    - ç¼“å­˜ç»“æœä»¥é¿å…é‡å¤çš„ LLM è°ƒç”¨

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        SmartStructureResponse with analysis results åŒ…å«åˆ†æç»“æœçš„å“åº”
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if we have cached analysis result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
        if document.structure_analysis_cache:
            logger.info(f"Using cached structure analysis for document {request.document_id}")
            result = document.structure_analysis_cache
        else:
            # Use smart LLM-based analysis
            # ä½¿ç”¨æ™ºèƒ½ LLM åˆ†æ
            logger.info(f"Starting smart structure analysis for document {request.document_id}")
            result = await smart_analyzer.analyze(document.original_text)
            logger.info(f"Smart analysis completed: {result.get('total_paragraphs', 0)} paragraphs found")

            # Cache the result in database
            # å°†ç»“æœç¼“å­˜åˆ°æ•°æ®åº“
            document.structure_analysis_cache = result
            await db.commit()
            logger.info(f"Cached structure analysis for document {request.document_id}")

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        sections = [
            SectionInfo(
                number=s.get("number", "?"),
                title=s.get("title", ""),
                paragraphs=[
                    SmartParagraphInfo(
                        position=p.get("position", "?"),
                        summary=p.get("summary", ""),
                        summary_zh=p.get("summary_zh", ""),
                        first_sentence=p.get("first_sentence", "")[:200],
                        last_sentence=p.get("last_sentence", "")[:200],
                        word_count=p.get("word_count", 0),
                        ai_risk=p.get("ai_risk", "unknown"),
                        ai_risk_reason=p.get("ai_risk_reason", ""),
                        # New fields for detailed rewrite suggestions
                        # æ–°å­—æ®µï¼šè¯¦ç»†ä¿®æ”¹å»ºè®®
                        rewrite_suggestion_zh=p.get("rewrite_suggestion_zh"),
                        rewrite_example=p.get("rewrite_example")
                    )
                    for p in s.get("paragraphs", [])
                ]
            )
            for s in result.get("sections", [])
        ]

        # Convert issues
        # è½¬æ¢é—®é¢˜
        # Handle case where LLM returns "All" instead of a list
        # å¤„ç†LLMè¿”å›"All"è€Œéåˆ—è¡¨çš„æƒ…å†µ
        def ensure_list(val):
            if isinstance(val, list):
                return val
            elif isinstance(val, str):
                return [val] if val else []
            return []

        issues = [
            SmartStructureIssue(
                type=i.get("type", "unknown"),
                description=i.get("description", ""),
                description_zh=i.get("description_zh", ""),
                severity=i.get("severity", "low"),
                affected_positions=ensure_list(i.get("affected_positions", []))
            )
            for i in result.get("issues", [])
        ]

        # Build compatible paragraphs list for existing frontend
        # æ„å»ºä¸ç°æœ‰å‰ç«¯å…¼å®¹çš„æ®µè½åˆ—è¡¨
        paragraphs = []
        idx = 0
        for section in sections:
            for p in section.paragraphs:
                paragraphs.append(ParagraphInfo(
                    index=idx,
                    first_sentence=p.first_sentence,
                    last_sentence=p.last_sentence,
                    word_count=p.word_count,
                    sentence_count=0,
                    has_topic_sentence=False,
                    has_summary_ending=False,
                    connector_words=[],
                    function_type="body",
                    position=p.position,
                    summary=p.summary,
                    summary_zh=p.summary_zh,
                    ai_risk=p.ai_risk,
                    ai_risk_reason=p.ai_risk_reason,
                    # New rewrite suggestion fields
                    # æ–°çš„ä¿®æ”¹å»ºè®®å­—æ®µ
                    rewrite_suggestion_zh=p.rewrite_suggestion_zh,
                    rewrite_example=p.rewrite_example
                ))
                idx += 1

        # Extract pattern flags from score breakdown
        # ä»åˆ†æ•°åˆ†è§£ä¸­æå–æ¨¡å¼æ ‡å¿—
        score_breakdown = result.get("score_breakdown", {})

        # Convert explicit connectors
        # è½¬æ¢æ˜¾æ€§è¿æ¥è¯
        explicit_connectors = [
            ExplicitConnector(
                word=c.get("word", ""),
                position=c.get("position", ""),
                location=c.get("location", "paragraph_start"),
                severity=c.get("severity", "high")
            )
            for c in result.get("explicit_connectors", [])
        ]

        # Convert logic breaks
        # è½¬æ¢é€»è¾‘æ–­è£‚ç‚¹
        # Note: lb.get("key", "") returns None if key exists with null value
        # Pydantic v2 treats explicit None differently - always convert to empty string
        # æ³¨æ„ï¼šå½“keyå­˜åœ¨ä½†å€¼ä¸ºnullæ—¶ï¼Œlb.get("key", "")è¿”å›None
        # Pydantic v2 å¯¹æ˜¾å¼ None å¤„ç†ä¸åŒ - å§‹ç»ˆè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        logic_breaks = [
            LogicBreak(
                from_position=lb.get("from_position") or "",
                to_position=lb.get("to_position") or "",
                transition_type=lb.get("transition_type") or "abrupt",
                issue=lb.get("issue") or "",
                issue_zh=lb.get("issue_zh") or "",
                suggestion=lb.get("suggestion") or "",  # Empty string if None/null
                suggestion_zh=lb.get("suggestion_zh") or ""  # Empty string if None/null
            )
            for lb in result.get("logic_breaks", [])
        ]

        return SmartStructureResponse(
            sections=sections,
            total_paragraphs=result.get("total_paragraphs", len(paragraphs)),
            total_sections=result.get("total_sections", len(sections)),
            structure_score=result.get("structure_score", 0),
            risk_level=result.get("risk_level", "low"),
            issues=issues,
            score_breakdown=score_breakdown,
            recommendation=result.get("recommendation", ""),
            recommendation_zh=result.get("recommendation_zh", ""),
            explicit_connectors=explicit_connectors,
            logic_breaks=logic_breaks,
            paragraphs=paragraphs,
            has_linear_flow=score_breakdown.get("linear_flow", 0) > 0,
            has_repetitive_pattern=score_breakdown.get("repetitive_pattern", 0) > 0,
            has_uniform_length=score_breakdown.get("uniform_length", 0) > 0,
            has_predictable_order=score_breakdown.get("predictable_order", 0) > 0,
            options=[]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Step 1-1 and Step 1-2: Two-Phase Analysis Endpoints
# æ­¥éª¤ 1-1 å’Œ 1-2ï¼šä¸¤é˜¶æ®µåˆ†æç«¯ç‚¹
# =============================================================================

@router.post("/document/step1-1")
async def analyze_document_structure_step1(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Step 1-1: Analyze document STRUCTURE only (global patterns)
    æ­¥éª¤ 1-1ï¼šä»…åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆå…¨å±€æ¨¡å¼ï¼‰

    This endpoint performs the first phase of analysis:
    - Section structure identification
    - Paragraph identification
    - Global structural patterns (linear flow, symmetry, etc.)
    - Structure score calculation

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Structure analysis result ç»“æ„åˆ†æç»“æœ
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if we have cached Step 1-1 result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ­¥éª¤ 1-1 ç»“æœ
        cache_key = "step1_1_cache"
        if hasattr(document, 'structure_analysis_cache') and document.structure_analysis_cache:
            cached = document.structure_analysis_cache
            if cache_key in cached:
                logger.info(f"Using cached Step 1-1 result for document {request.document_id}")
                return cached[cache_key]

        # Perform Step 1-1 analysis
        # æ‰§è¡Œæ­¥éª¤ 1-1 åˆ†æ
        logger.info(f"Starting Step 1-1 structure analysis for document {request.document_id}")
        result = await smart_analyzer.analyze_structure(document.original_text)

        # Cache the result
        # ç¼“å­˜ç»“æœ
        if not document.structure_analysis_cache:
            document.structure_analysis_cache = {}
        document.structure_analysis_cache[cache_key] = result
        await db.commit()

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Step 1-1 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document/step1-2")
async def analyze_document_relationships_step2(
    request: DocumentStructureRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Step 1-2: Analyze paragraph RELATIONSHIPS (connections, transitions)
    æ­¥éª¤ 1-2ï¼šåˆ†ææ®µè½å…³ç³»ï¼ˆè¿æ¥ã€è¿‡æ¸¡ï¼‰

    This endpoint performs the second phase of analysis:
    - Explicit connector word detection
    - Logic break points between paragraphs
    - AI risk assessment for each paragraph
    - Rewrite suggestions

    Requires Step 1-1 to be completed first.

    Args:
        request: Document structure request æ–‡æ¡£ç»“æ„è¯·æ±‚
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Relationship analysis result å…³ç³»åˆ†æç»“æœ
    """
    try:
        # Get document
        # è·å–æ–‡æ¡£
        document = await db.get(Document, request.document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if Step 1-1 has been completed
        # æ£€æŸ¥æ­¥éª¤ 1-1 æ˜¯å¦å·²å®Œæˆ
        step1_1_key = "step1_1_cache"
        step1_2_key = "step1_2_cache"

        if not document.structure_analysis_cache or step1_1_key not in document.structure_analysis_cache:
            raise HTTPException(
                status_code=400,
                detail="Step 1-1 (structure analysis) must be completed first"
            )

        # Check if we have cached Step 1-2 result
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ­¥éª¤ 1-2 ç»“æœ
        if step1_2_key in document.structure_analysis_cache:
            logger.info(f"Using cached Step 1-2 result for document {request.document_id}")
            return document.structure_analysis_cache[step1_2_key]

        # Get Step 1-1 result
        # è·å–æ­¥éª¤ 1-1 ç»“æœ
        structure_result = document.structure_analysis_cache[step1_1_key]

        # Perform Step 1-2 analysis
        # æ‰§è¡Œæ­¥éª¤ 1-2 åˆ†æ
        logger.info(f"Starting Step 1-2 relationship analysis for document {request.document_id}")
        result = await smart_analyzer.analyze_relationships(
            document.original_text,
            structure_result
        )

        # Cache the result
        # ç¼“å­˜ç»“æœ
        document.structure_analysis_cache[step1_2_key] = result
        await db.commit()

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Step 1-2 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/document/{document_id}/cache")
async def clear_analysis_cache(
    document_id: str,
    db: AsyncSession = Depends(get_db)
):
    """
    Clear analysis cache for a document (allows re-analysis)
    æ¸…é™¤æ–‡æ¡£çš„åˆ†æç¼“å­˜ï¼ˆå…è®¸é‡æ–°åˆ†æï¼‰

    Args:
        document_id: Document ID æ–‡æ¡£ID
        db: Database session æ•°æ®åº“ä¼šè¯

    Returns:
        Success message æˆåŠŸæ¶ˆæ¯
    """
    try:
        document = await db.get(Document, document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        document.structure_analysis_cache = None
        await db.commit()

        return {"message": "Cache cleared successfully", "document_id": document_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Clear cache error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/strategies")
async def get_structure_strategies():
    """
    Get available structure strategies
    è·å–å¯ç”¨çš„ç»“æ„ç­–ç•¥

    Returns:
        List of strategy descriptions ç­–ç•¥æè¿°åˆ—è¡¨
    """
    return {
        "strategies": [
            {
                "id": "optimize_connection",
                "name": "Optimize Connection",
                "name_zh": "ä¼˜åŒ–è¿æ¥",
                "description": "Improve paragraph connections without changing content order",
                "description_zh": "åœ¨ä¸æ”¹å˜å†…å®¹é¡ºåºçš„æƒ…å†µä¸‹æ”¹å–„æ®µè½è¿æ¥"
            },
            {
                "id": "deep_restructure",
                "name": "Deep Restructure",
                "name_zh": "æ·±åº¦é‡ç»„",
                "description": "Reorder and reorganize content for maximum naturalness",
                "description_zh": "é‡æ–°æ’åºå’Œç»„ç»‡å†…å®¹ä»¥è·å¾—æœ€å¤§è‡ªç„¶åº¦"
            }
        ]
    }


# =============================================================================
# Enhanced Structure Analysis Endpoints (Level 1 Enhancement)
# å¢å¼ºç»“æ„åˆ†æç«¯ç‚¹ï¼ˆLevel 1å¢å¼ºï¼‰
# =============================================================================

@router.post("/predictability", response_model=PredictabilityAnalysisResponse)
async def analyze_predictability(request: PredictabilityAnalysisRequest):
    """
    Analyze document structure predictability
    åˆ†ææ–‡æ¡£ç»“æ„é¢„æµ‹æ€§

    This endpoint provides detailed analysis of:
    - Progression type (monotonic vs non-monotonic)
    - Function distribution (uniform vs asymmetric)
    - Closure pattern (strong vs weak/open)
    - Lexical echo (explicit connectors vs semantic bridges)

    Args:
        request: Predictability analysis request é¢„æµ‹æ€§åˆ†æè¯·æ±‚

    Returns:
        PredictabilityAnalysisResponse with detailed analysis åŒ…å«è¯¦ç»†åˆ†æçš„å“åº”
    """
    try:
        # Perform full structure analysis
        # æ‰§è¡Œå®Œæ•´ç»“æ„åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=True
        )

        # Convert progression analysis
        # è½¬æ¢æ¨è¿›åˆ†æ
        progression = None
        if result.progression_analysis:
            progression = ProgressionAnalysisResult(
                progression_type=result.progression_analysis.progression_type,
                progression_type_zh=result.progression_analysis.progression_type_zh,
                forward_transitions=result.progression_analysis.forward_transitions,
                backward_references=result.progression_analysis.backward_references,
                conditional_statements=result.progression_analysis.conditional_statements,
                score=result.progression_analysis.score
            )

        # Convert function distribution
        # è½¬æ¢åŠŸèƒ½åˆ†å¸ƒ
        distribution = None
        if result.function_distribution:
            distribution = FunctionDistributionResult(
                distribution_type=result.function_distribution.distribution_type,
                distribution_type_zh=result.function_distribution.distribution_type_zh,
                function_counts=result.function_distribution.function_counts,
                depth_variance=result.function_distribution.depth_variance,
                longest_section_ratio=result.function_distribution.longest_section_ratio,
                score=result.function_distribution.score,
                asymmetry_opportunities=result.function_distribution.asymmetry_opportunities
            )

        # Convert closure analysis
        # è½¬æ¢é—­åˆåˆ†æ
        closure = None
        if result.closure_analysis:
            closure = ClosureAnalysisResult(
                closure_type=result.closure_analysis.closure_type,
                closure_type_zh=result.closure_analysis.closure_type_zh,
                has_formulaic_ending=result.closure_analysis.has_formulaic_ending,
                has_complete_resolution=result.closure_analysis.has_complete_resolution,
                open_questions=result.closure_analysis.open_questions,
                hedging_in_conclusion=result.closure_analysis.hedging_in_conclusion,
                score=result.closure_analysis.score,
                detected_patterns=result.closure_analysis.detected_patterns
            )

        # Convert lexical echo analysis
        # è½¬æ¢è¯æ±‡å›å£°åˆ†æ
        lexical_echo = None
        if result.lexical_echo_analysis:
            lexical_echo = LexicalEchoResult(
                total_transitions=result.lexical_echo_analysis.total_transitions,
                echo_transitions=result.lexical_echo_analysis.echo_transitions,
                explicit_connector_transitions=result.lexical_echo_analysis.explicit_connector_transitions,
                echo_ratio=result.lexical_echo_analysis.echo_ratio,
                score=result.lexical_echo_analysis.score,
                transition_details=result.lexical_echo_analysis.transition_details
            )

        # Determine recommended disruption level based on score
        # æ ¹æ®åˆ†æ•°ç¡®å®šæ¨èçš„æ‰°åŠ¨ç­‰çº§
        if result.structure_score >= 60:
            recommended_level = "strong"
            recommended_strategies = ["inversion", "conflict_injection", "weak_closure", "asymmetry"]
        elif result.structure_score >= 35:
            recommended_level = "medium"
            recommended_strategies = ["lexical_echo", "asymmetry", "local_reorder"]
        else:
            recommended_level = "light"
            recommended_strategies = ["rewrite_opening", "remove_connector", "lexical_echo"]

        return PredictabilityAnalysisResponse(
            total_score=result.structure_score,
            risk_level=result.risk_level,
            progression_analysis=progression,
            function_distribution=distribution,
            closure_analysis=closure,
            lexical_echo_analysis=lexical_echo,
            recommended_disruption_level=recommended_level,
            recommended_strategies=recommended_strategies,
            message=result.message,
            message_zh=result.message_zh
        )

    except Exception as e:
        logger.error(f"Predictability analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/disruption-levels")
async def get_disruption_levels():
    """
    Get available disruption levels and their configurations
    è·å–å¯ç”¨çš„æ‰°åŠ¨ç­‰çº§åŠå…¶é…ç½®

    Returns:
        Dictionary of disruption levels æ‰°åŠ¨ç­‰çº§å­—å…¸
    """
    return {
        "levels": DISRUPTION_LEVELS,
        "strategies": DISRUPTION_STRATEGIES
    }


@router.get("/disruption-strategies")
async def get_disruption_strategies():
    """
    Get available disruption strategies
    è·å–å¯ç”¨çš„æ‰°åŠ¨ç­–ç•¥

    Returns:
        List of strategy descriptions ç­–ç•¥æè¿°åˆ—è¡¨
    """
    strategies = []
    for key, value in DISRUPTION_STRATEGIES.items():
        strategies.append({
            "id": key,
            "name": value["name"],
            "name_zh": value["name_zh"],
            "description": value["description"],
            "description_zh": value["description_zh"],
            "prompt_instruction": value["prompt_instruction"]
        })
    return {"strategies": strategies}


# =============================================================================
# 7-Indicator Structural Risk Card Endpoint
# 7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç«¯ç‚¹
# =============================================================================

@router.post("/risk-card", response_model=StructuralRiskCardResponse)
async def get_structural_risk_card(request: RiskCardRequest):
    """
    Get 7-indicator structural risk card for user visualization
    è·å–7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç”¨äºç”¨æˆ·å¯è§†åŒ–

    Returns a visual risk card showing:
    - 7 AI structural indicators with emoji and color
    - Whether each indicator is triggered
    - Overall risk level and summary

    The 7 indicators are:
    1. âš–ï¸ Perfect Symmetry (é€»è¾‘æ¨è¿›å¯¹ç§°) - â˜…â˜…â˜…
    2. ğŸ“Š Uniform Function (æ®µè½åŠŸèƒ½å‡åŒ€) - â˜…â˜…â˜†
    3. ğŸ”— Over-signaled Transitions (è¿æ¥è¯ä¾èµ–) - â˜…â˜…â˜…
    4. ğŸ“ Linear Enumeration (å•ä¸€çº¿æ€§æ¨è¿›) - â˜…â˜…â˜…
    5. ğŸ“ Rhythmic Regularity (æ®µè½èŠ‚å¥å‡è¡¡) - â˜…â˜…â˜†
    6. ğŸ”’ Over-conclusive Ending (ç»“å°¾è¿‡åº¦é—­åˆ) - â˜…â˜…â˜†
    7. ğŸ”„ No Cross-References (ç¼ºä¹å›æŒ‡ç»“æ„) - â˜…â˜…â˜†

    Args:
        request: Risk card analysis request é£é™©å¡ç‰‡åˆ†æè¯·æ±‚

    Returns:
        StructuralRiskCardResponse with 7 indicators åŒ…å«7ä¸ªæŒ‡å¾çš„å“åº”
    """
    try:
        # Perform full structure analysis
        # æ‰§è¡Œå®Œæ•´ç»“æ„åˆ†æ
        result = structure_analyzer.analyze(
            text=request.text,
            extract_thesis=True
        )

        # Get the risk card from the result
        # ä»ç»“æœä¸­è·å–é£é™©å¡ç‰‡
        if not result.risk_card:
            raise HTTPException(status_code=500, detail="Failed to generate risk card")

        # Convert to response format
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        indicators = [
            StructuralIndicatorResponse(
                id=ind.id,
                name=ind.name,
                name_zh=ind.name_zh,
                triggered=ind.triggered,
                risk_level=ind.risk_level,
                emoji=ind.emoji,
                color=ind.color,
                description=ind.description,
                description_zh=ind.description_zh,
                details=ind.details,
                details_zh=ind.details_zh
            )
            for ind in result.risk_card.indicators
        ]

        return StructuralRiskCardResponse(
            indicators=indicators,
            triggered_count=result.risk_card.triggered_count,
            overall_risk=result.risk_card.overall_risk,
            overall_risk_zh=result.risk_card.overall_risk_zh,
            summary=result.risk_card.summary,
            summary_zh=result.risk_card.summary_zh,
            total_score=result.risk_card.total_score
        )

    except Exception as e:
        logger.error(f"Risk card generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/indicator-config")
async def get_indicator_config():
    """
    Get the 7-indicator configuration for UI display
    è·å–7æŒ‡å¾é…ç½®ç”¨äºUIæ˜¾ç¤º

    Returns:
        Configuration for all 7 indicators with emoji, colors, and descriptions
    """
    from src.core.analyzer.structure import StructureAnalyzer

    return {
        "indicators": StructureAnalyzer.INDICATOR_CONFIG,
        "summary_text": "AIå†™ä½œçš„æœ€å¤§ç‰¹å¾ä¸æ˜¯è¯­æ³•å®Œç¾ï¼Œè€Œæ˜¯ç»“æ„å¤ªå®Œç¾ã€‚",
        "summary_text_en": "The biggest feature of AI writing is not perfect grammar, but perfect structure."
    }


# =============================================================================
# Single Paragraph Suggestion Endpoint
# å•ä¸ªæ®µè½å»ºè®®ç«¯ç‚¹
# =============================================================================

# Prompt for generating single paragraph rewrite suggestions
# ç”Ÿæˆå•ä¸ªæ®µè½æ”¹å†™å»ºè®®çš„æç¤ºè¯
PARAGRAPH_SUGGESTION_PROMPT = """You are an expert De-AIGC consultant. Analyze this paragraph and provide specific rewriting advice to remove AI-writing patterns.

## PARAGRAPH TEXT:
{paragraph_text}

## PARAGRAPH POSITION: {paragraph_position}

## KNOWN AI RISK: {ai_risk} - {ai_risk_reason}

## CONTEXT (if available):
{context_hint}

## YOUR TASK:
Generate a SPECIFIC, ACTIONABLE Chinese rewriting suggestion for this paragraph. Focus on:
1. Identifying specific AI writing patterns (explicit connectors, formulaic structure, etc.)
2. Providing concrete strategies to humanize the text
3. Giving an example of how to rewrite key sentences

## OUTPUT FORMAT (JSON):
{{
  "rewrite_suggestion_zh": "ã€é—®é¢˜è¯Šæ–­ã€‘æ®µé¦–ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯'Furthermore'ï¼Œå±äºå…¸å‹AIå†™ä½œç—•è¿¹ã€‚æ®µè½ç»“æ„é‡‡ç”¨'é—®é¢˜-åˆ†æ-ç»“è®º'å…¬å¼åŒ–æ¨¡å¼ã€‚\\nã€ä¿®æ”¹ç­–ç•¥ã€‘1. åˆ é™¤æ®µé¦–è¿æ¥è¯ï¼Œæ”¹ç”¨è¯­ä¹‰å›å£°æ‰¿æ¥ä¸Šæ®µå…³é”®æ¦‚å¿µï¼›2. æ‰“æ•£å…¬å¼åŒ–ç»“æ„ï¼Œå°†ç»“è®ºæå‰æˆ–èå…¥è®ºè¿°ä¸­ã€‚\\nã€æ”¹å†™æç¤ºã€‘å¯å°†å¼€å¤´æ”¹ä¸ºç›´æ¥æ‰¿æ¥ä¸Šæ®µå†…å®¹ï¼Œå¦‚'åœŸå£¤ç›åˆ†ç´¯ç§¯çš„è¿™ä¸€è¶‹åŠ¿åœ¨...'",
  "rewrite_example": "The escalating trend of soil salinization poses new challenges to traditional agriculture. In the North China Plain, monitoring data from the past decade reveals...",
  "ai_risk": "high",
  "ai_risk_reason": "æ®µé¦–ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯'Furthermore'ï¼Œé‡‡ç”¨å…¬å¼åŒ–ç»“æ„"
}}

CRITICAL RULES:
- The rewrite_suggestion_zh MUST be in Chinese
- The rewrite_suggestion_zh MUST includeã€é—®é¢˜è¯Šæ–­ã€‘ã€ä¿®æ”¹ç­–ç•¥ã€‘ã€æ”¹å†™æç¤ºã€‘sections
- Quote specific text from the paragraph in the diagnosis
- Provide concrete examples, not generic advice
- The rewrite_example should be in ENGLISH showing a better version of the first 1-2 sentences
- The ai_risk_reason should be in CHINESE (ä¸­æ–‡æè¿°ï¼Œå¼•ç”¨åŸæ–‡æ—¶ä¿ç•™åŸè¯­è¨€)
"""


@router.post("/paragraph-suggestion", response_model=ParagraphSuggestionResponse)
async def get_paragraph_suggestion(request: ParagraphSuggestionRequest):
    """
    Get rewrite suggestion for a single paragraph
    è·å–å•ä¸ªæ®µè½çš„æ”¹å†™å»ºè®®

    This endpoint generates specific, actionable rewriting advice
    for a single paragraph using LLM analysis.
    æ­¤ç«¯ç‚¹ä½¿ç”¨ LLM åˆ†æä¸ºå•ä¸ªæ®µè½ç”Ÿæˆå…·ä½“å¯è¡Œçš„æ”¹å†™å»ºè®®ã€‚

    Args:
        request: Paragraph suggestion request æ®µè½å»ºè®®è¯·æ±‚

    Returns:
        ParagraphSuggestionResponse with rewrite suggestions åŒ…å«æ”¹å†™å»ºè®®çš„å“åº”
    """
    try:
        import httpx
        import json
        from src.config import get_settings

        settings = get_settings()

        # Build prompt
        # æ„å»ºæç¤ºè¯
        prompt = PARAGRAPH_SUGGESTION_PROMPT.format(
            paragraph_text=request.paragraph_text[:2000],  # Limit length
            paragraph_position=request.paragraph_position,
            ai_risk=request.ai_risk or "unknown",
            ai_risk_reason=request.ai_risk_reason or "Not yet analyzed",
            context_hint=request.context_hint or "No context provided"
        )

        # Call LLM API
        # è°ƒç”¨ LLM API
        response_text = await _call_llm_for_suggestion(prompt, settings)

        # Parse response
        # è§£æå“åº”
        result = _parse_suggestion_response(response_text)

        return ParagraphSuggestionResponse(
            paragraph_position=request.paragraph_position,
            rewrite_suggestion_zh=result.get("rewrite_suggestion_zh", "ã€é—®é¢˜è¯Šæ–­ã€‘åˆ†æå¤±è´¥\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·ç¨åé‡è¯•\nã€æ”¹å†™æç¤ºã€‘æ— "),
            rewrite_example=result.get("rewrite_example"),
            ai_risk=result.get("ai_risk", request.ai_risk or "unknown"),
            ai_risk_reason=result.get("ai_risk_reason", request.ai_risk_reason or "")
        )

    except Exception as e:
        logger.error(f"Paragraph suggestion error: {e}")
        # Return a fallback response instead of error
        # è¿”å›åå¤‡å“åº”è€Œä¸æ˜¯é”™è¯¯
        return ParagraphSuggestionResponse(
            paragraph_position=request.paragraph_position,
            rewrite_suggestion_zh=f"ã€é—®é¢˜è¯Šæ–­ã€‘åˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·ç¨åé‡è¯•\nã€æ”¹å†™æç¤ºã€‘å»ºè®®åˆ é™¤æ®µé¦–æ˜¾æ€§è¿æ¥è¯ï¼Œæ”¹ç”¨è¯­ä¹‰æ‰¿æ¥",
            rewrite_example=None,
            ai_risk=request.ai_risk or "unknown",
            ai_risk_reason=request.ai_risk_reason or ""
        )


async def _call_llm_for_suggestion(prompt: str, settings) -> str:
    """
    Call LLM API for paragraph suggestion
    è°ƒç”¨ LLM API è·å–æ®µè½å»ºè®®
    """
    import httpx

    # Use Volcengine (preferred)
    # ä½¿ç”¨ç«å±±å¼•æ“ï¼ˆé¦–é€‰ï¼‰
    if settings.llm_provider == "volcengine" and settings.volcengine_api_key:
        async with httpx.AsyncClient(
            base_url=settings.volcengine_base_url,
            headers={
                "Authorization": f"Bearer {settings.volcengine_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.volcengine_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use DeepSeek
    # ä½¿ç”¨ DeepSeek
    elif settings.llm_provider == "deepseek" and settings.deepseek_api_key:
        async with httpx.AsyncClient(
            base_url=settings.deepseek_base_url,
            headers={
                "Authorization": f"Bearer {settings.deepseek_api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0,
            trust_env=False
        ) as client:
            response = await client.post("/chat/completions", json={
                "model": settings.llm_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.2
            })
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # Use Gemini
    # ä½¿ç”¨ Gemini
    elif settings.llm_provider == "gemini" and settings.gemini_api_key:
        from google import genai
        client = genai.Client(api_key=settings.gemini_api_key)
        response = await client.aio.models.generate_content(
            model=settings.llm_model,
            contents=prompt,
            config={"max_output_tokens": 2048, "temperature": 0.2}
        )
        return response.text

    else:
        raise ValueError("No LLM API configured")


def _parse_suggestion_response(response: str) -> dict:
    """
    Parse LLM response to JSON
    è§£æ LLM å“åº”ä¸º JSON
    """
    import json

    # Clean response (remove markdown code blocks if present)
    # æ¸…ç†å“åº”
    response = response.strip()
    if response.startswith("```"):
        lines = response.split("\n")
        lines = [l for l in lines if not l.strip().startswith("```")]
        response = "\n".join(lines)
    response = response.strip()

    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # Try to extract JSON from response
        # å°è¯•ä»å“åº”ä¸­æå– JSON
        import re
        match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except:
                pass
        # Return default
        return {
            "rewrite_suggestion_zh": "ã€é—®é¢˜è¯Šæ–­ã€‘æ— æ³•è§£æåˆ†æç»“æœ\nã€ä¿®æ”¹ç­–ç•¥ã€‘è¯·é‡è¯•\nã€æ”¹å†™æç¤ºã€‘å»ºè®®æ£€æŸ¥æ®µè½ç»“æ„",
            "rewrite_example": None,
            "ai_risk": "unknown",
            "ai_risk_reason": "åˆ†æå¤±è´¥"
        }
