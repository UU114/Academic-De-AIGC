"""
Document Structure Analyzer (Level 1 De-AIGC)
æ–‡æ¡£ç»“æ„åˆ†æå™¨ï¼ˆLevel 1 De-AIGCï¼‰

Phase 4: Analyzes full document structure for AI-like linear patterns
and suggests two restructuring strategies: optimize connection, deep restructure

Phase 4ï¼šåˆ†æå…¨æ–‡ç»“æ„çš„AIé£æ ¼çº¿æ€§æ¨¡å¼ï¼Œ
æä¾›ä¸¤ç§é‡æ„ç­–ç•¥ï¼šä¼˜åŒ–è¿æ¥ã€æ·±åº¦é‡ç»„
"""

import re
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
from enum import Enum


class StructureStrategy(str, Enum):
    """Structure strategy types ç»“æ„ç­–ç•¥ç±»å‹"""
    OPTIMIZE_CONNECTION = "optimize_connection"  # Optimize transitions without changing order
    DEEP_RESTRUCTURE = "deep_restructure"        # Reorder and restructure content


@dataclass
class ParagraphInfo:
    """
    Information about a paragraph
    æ®µè½ä¿¡æ¯
    """
    index: int
    text: str
    first_sentence: str
    last_sentence: str
    word_count: int
    sentence_count: int
    has_topic_sentence: bool
    has_summary_ending: bool
    connector_words: List[str]
    function_type: str  # introduction, body, conclusion, transition, evidence, analysis


@dataclass
class StructureIssue:
    """
    Detected structure issue
    æ£€æµ‹åˆ°çš„ç»“æ„é—®é¢˜
    """
    type: str  # linear_flow, repetitive_pattern, uniform_length, predictable_order
    description: str
    description_zh: str
    severity: str  # high, medium, low
    affected_paragraphs: List[int]
    suggestion: str
    suggestion_zh: str


@dataclass
class BreakPoint:
    """
    Logic break point in document structure
    æ–‡æ¡£ç»“æ„ä¸­çš„é€»è¾‘æ–­ç‚¹
    """
    position: int  # Paragraph index
    type: str  # topic_shift, argument_gap, repetition, abrupt_transition
    description: str
    description_zh: str


@dataclass
class StructureOption:
    """
    A structure repair option
    ç»“æ„ä¿®å¤é€‰é¡¹
    """
    strategy: StructureStrategy
    strategy_name_zh: str
    outline: List[str]  # New paragraph order/structure
    changes: List[Dict]  # List of changes to make
    explanation: str
    explanation_zh: str
    predicted_improvement: int  # Expected structure score reduction


@dataclass
class ProgressionAnalysis:
    """
    Analysis of document progression type (P1 Enhancement)
    æ–‡æ¡£æ¨è¿›ç±»å‹åˆ†æï¼ˆP1å¢å¼ºï¼‰
    """
    progression_type: str  # "monotonic" | "non_monotonic" | "mixed"
    progression_type_zh: str
    forward_transitions: int  # Count of forward-only transitions
    backward_references: int  # Count of references to earlier content
    conditional_statements: int  # Count of conditional "if X then Y" patterns
    score: int  # 0-100, higher = more AI-like (monotonic)
    details: List[Dict] = field(default_factory=list)


@dataclass
class FunctionDistribution:
    """
    Analysis of paragraph function distribution (P1 Enhancement)
    æ®µè½åŠŸèƒ½åˆ†å¸ƒåˆ†æï¼ˆP1å¢å¼ºï¼‰
    """
    distribution_type: str  # "uniform" | "asymmetric" | "balanced"
    distribution_type_zh: str
    function_counts: Dict[str, int] = field(default_factory=dict)
    depth_variance: float = 0.0  # Variance in treatment depth
    longest_section_ratio: float = 0.0  # Ratio of longest to average
    score: int = 0  # 0-100, higher = more uniform (AI-like)
    asymmetry_opportunities: List[Dict] = field(default_factory=list)


@dataclass
class ClosureAnalysis:
    """
    Analysis of document closure pattern (P2 Enhancement)
    æ–‡æ¡£é—­åˆæ¨¡å¼åˆ†æï¼ˆP2å¢å¼ºï¼‰
    """
    closure_type: str  # "strong" | "moderate" | "weak" | "open"
    closure_type_zh: str
    has_formulaic_ending: bool  # "In conclusion..." patterns
    has_complete_resolution: bool  # All tensions resolved
    open_questions: int  # Count of unresolved questions
    hedging_in_conclusion: int  # Count of hedging words in conclusion
    score: int = 0  # 0-100, higher = stronger closure (AI-like)
    detected_patterns: List[str] = field(default_factory=list)


@dataclass
class LexicalEchoAnalysis:
    """
    Analysis of lexical echo between paragraphs (P2 Enhancement)
    æ®µè½é—´è¯æ±‡å›å£°åˆ†æï¼ˆP2å¢å¼ºï¼‰
    """
    total_transitions: int
    echo_transitions: int  # Transitions with lexical echo
    explicit_connector_transitions: int  # Transitions with explicit connectors
    echo_ratio: float  # Ratio of echo to explicit
    score: int = 0  # 0-100, lower = more echo (human-like)
    transition_details: List[Dict] = field(default_factory=list)


@dataclass
class CrossReferenceAnalysis:
    """
    Analysis of cross-referential links in document (7th AI Indicator Enhancement)
    æ–‡æ¡£äº¤å‰å¼•ç”¨åˆ†æï¼ˆç¬¬7é¡¹AIæŒ‡å¾å¢å¼ºï¼‰

    Detects:
    - Cross-paragraph concept references
    - Core concept callbacks
    - Non-linear structural links
    """
    has_cross_references: bool  # Whether document has cross-references
    cross_reference_count: int  # Total count of cross-references
    concept_callbacks: int  # References back to core concepts
    forward_only_ratio: float  # Ratio of forward-only progression (0-1, higher = AI-like)
    score: int = 0  # 0-100, higher = more AI-like (lacking cross-refs)
    detected_references: List[Dict] = field(default_factory=list)
    core_concepts: List[str] = field(default_factory=list)


@dataclass
class StructuralIndicator:
    """
    Single structural AI indicator for risk card (7-Indicator System)
    å•ä¸ªç»“æ„AIæŒ‡å¾ç”¨äºé£é™©å¡ç‰‡ï¼ˆ7æŒ‡å¾ç³»ç»Ÿï¼‰
    """
    id: str  # Indicator ID
    name: str  # English name
    name_zh: str  # Chinese name
    triggered: bool  # Whether this indicator is triggered
    risk_level: int  # 1-3 stars
    emoji: str  # Visual emoji
    color: str  # hex color code
    description: str  # Brief description
    description_zh: str
    details: str = ""  # Specific details for this document
    details_zh: str = ""


@dataclass
class StructuralRiskCard:
    """
    7-Indicator Structural Risk Card for user visualization
    7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç”¨äºç”¨æˆ·å¯è§†åŒ–
    """
    indicators: List[StructuralIndicator]
    triggered_count: int  # How many indicators are triggered
    overall_risk: str  # low, medium, high
    overall_risk_zh: str
    summary: str  # One-line summary
    summary_zh: str
    total_score: int  # Combined structure score


@dataclass
class StructureAnalysisResult:
    """
    Result of document structure analysis
    æ–‡æ¡£ç»“æ„åˆ†æç»“æœ
    """
    # Basic info
    # åŸºæœ¬ä¿¡æ¯
    total_paragraphs: int
    total_sentences: int
    total_words: int
    avg_paragraph_length: float
    paragraph_length_variance: float

    # Scores and levels
    # åˆ†æ•°å’Œç­‰çº§
    structure_score: int  # 0-100, higher = more AI-like
    risk_level: str  # "low", "medium", "high"

    # Detected patterns
    # æ£€æµ‹åˆ°çš„æ¨¡å¼
    paragraphs: List[ParagraphInfo] = field(default_factory=list)
    issues: List[StructureIssue] = field(default_factory=list)
    break_points: List[BreakPoint] = field(default_factory=list)

    # Extracted thesis
    # æå–çš„è®ºç‚¹
    core_thesis: Optional[str] = None
    key_arguments: List[str] = field(default_factory=list)

    # Pattern detection
    # æ¨¡å¼æ£€æµ‹
    has_linear_flow: bool = False  # 1-2-3 linear progression
    has_repetitive_pattern: bool = False  # Similar paragraph structures
    has_uniform_length: bool = False  # Similar paragraph lengths
    has_predictable_order: bool = False  # Introduction-body-conclusion

    # P1 Enhancement: Advanced pattern detection
    # P1å¢å¼ºï¼šé«˜çº§æ¨¡å¼æ£€æµ‹
    progression_analysis: Optional[ProgressionAnalysis] = None
    function_distribution: Optional[FunctionDistribution] = None

    # P2 Enhancement: Closure and lexical echo analysis
    # P2å¢å¼ºï¼šé—­åˆå’Œè¯æ±‡å›å£°åˆ†æ
    closure_analysis: Optional[ClosureAnalysis] = None
    lexical_echo_analysis: Optional[LexicalEchoAnalysis] = None

    # 7th Indicator Enhancement: Cross-reference analysis
    # ç¬¬7æŒ‡å¾å¢å¼ºï¼šäº¤å‰å¼•ç”¨åˆ†æ
    cross_reference_analysis: Optional[CrossReferenceAnalysis] = None

    # 7-Indicator Risk Card for user visualization
    # 7æŒ‡å¾é£é™©å¡ç‰‡ç”¨äºç”¨æˆ·å¯è§†åŒ–
    risk_card: Optional[StructuralRiskCard] = None

    # Repair options (populated when suggestions are generated)
    # ä¿®å¤é€‰é¡¹ï¼ˆç”Ÿæˆå»ºè®®æ—¶å¡«å……ï¼‰
    options: List[StructureOption] = field(default_factory=list)

    # Messages
    # æ¶ˆæ¯
    message: str = ""
    message_zh: str = ""


class StructureAnalyzer:
    """
    Analyzes document structure for AI-like patterns
    åˆ†ææ–‡æ¡£ç»“æ„çš„AIé£æ ¼æ¨¡å¼
    """

    # Topic sentence patterns (AI tendency)
    # ä¸»é¢˜å¥æ¨¡å¼ï¼ˆAIå€¾å‘ï¼‰
    TOPIC_SENTENCE_PATTERNS = [
        r"^This (paper|study|research|analysis|section|paragraph) (examines|explores|investigates|discusses|presents)",
        r"^The (purpose|aim|goal|objective) of this",
        r"^In this (paper|study|section|paragraph)",
        r"^(First|Second|Third|Finally|Additionally|Moreover|Furthermore),",
        r"^One (key|crucial|important|significant) (aspect|factor|consideration)",
        r"^(An|The) important (aspect|factor|consideration) (is|of)",
    ]

    # Summary ending patterns
    # æ€»ç»“ç»“å°¾æ¨¡å¼
    SUMMARY_PATTERNS = [
        r"(In summary|To summarize|In conclusion|Overall|Thus|Therefore|Hence),?.*\.$",
        r"(clearly|evidently|significantly) (demonstrates?|shows?|indicates?).*\.$",
        r"(This|These) (findings?|results?|observations?) (suggest|indicate|demonstrate).*\.$",
    ]

    # Linear transition patterns
    # çº¿æ€§è¿‡æ¸¡æ¨¡å¼
    LINEAR_TRANSITIONS = [
        "First", "Second", "Third", "Fourth", "Fifth",
        "Firstly", "Secondly", "Thirdly",
        "To begin with", "Next", "Then", "Finally", "Lastly",
        "In the first place", "In the second place",
    ]

    # Paragraph function keywords
    # æ®µè½åŠŸèƒ½å…³é”®è¯
    FUNCTION_KEYWORDS = {
        "introduction": ["introduce", "overview", "background", "context", "aim", "purpose", "objective"],
        "conclusion": ["conclude", "summary", "findings", "implications", "future", "recommend"],
        "evidence": ["data", "results", "findings", "experiment", "survey", "analysis shows"],
        "analysis": ["analyze", "examine", "investigate", "explore", "discuss", "interpret"],
        "transition": ["turning to", "moving on", "having discussed", "building on"],
    }

    # P1 Enhancement: Backward reference patterns (non-monotonic indicators)
    # P1å¢å¼ºï¼šå›æŒ‡æ¨¡å¼ï¼ˆéå•è°ƒæŒ‡ç¤ºå™¨ï¼‰
    BACKWARD_REFERENCE_PATTERNS = [
        r"as (mentioned|noted|discussed|stated|shown|demonstrated) (earlier|above|previously|before)",
        r"returning to (the|our|this)",
        r"recall(ing)? (that|the)",
        r"(this|these) earlier (point|observation|finding|argument)",
        r"revisit(ing)? (the|our)",
        r"as we (saw|noted|observed)",
        r"the (previous|earlier|above) (section|paragraph|discussion)",
    ]

    # P1 Enhancement: Conditional statement patterns (human-like)
    # P1å¢å¼ºï¼šæ¡ä»¶é™ˆè¿°æ¨¡å¼ï¼ˆäººç±»ç‰¹å¾ï¼‰
    CONDITIONAL_PATTERNS = [
        r"\bif\b.*\bthen\b",
        r"\bwhen\b.*,.*\b(this|it|they)\b",
        r"\bassuming\b.*,",
        r"\bgiven\b.*,",
        r"\bprovided\b.*,",
        r"\bunless\b.*,",
        r"\bwhile\b.*\balso\b",
    ]

    # P1 Enhancement: Forward-only transition patterns (monotonic indicators)
    # P1å¢å¼ºï¼šå•å‘æ¨è¿›æ¨¡å¼ï¼ˆå•è°ƒæŒ‡ç¤ºå™¨ï¼‰
    FORWARD_ONLY_PATTERNS = [
        r"^(furthermore|moreover|additionally|in addition),",
        r"^(building on this|extending this|taking this further)",
        r"^(the next|another|a further) (point|aspect|consideration)",
        r"^having (established|shown|demonstrated).*,.*now",
    ]

    # P2 Enhancement: Formulaic conclusion patterns (AI-like strong closure)
    # P2å¢å¼ºï¼šå…¬å¼åŒ–ç»“è®ºæ¨¡å¼ï¼ˆAIé£æ ¼å¼ºé—­åˆï¼‰
    FORMULAIC_CONCLUSION_PATTERNS = [
        r"^in conclusion,",
        r"^to (summarize|conclude|sum up),",
        r"^in summary,",
        r"^this (paper|study|research|analysis) (has |)(demonstrated|shown|established)",
        r"^overall,.*findings",
        r"^the (results|findings|evidence) (clearly |)(demonstrate|show|indicate)",
        r"^taken together,",
    ]

    # P2 Enhancement: Open ending patterns (human-like weak closure)
    # P2å¢å¼ºï¼šå¼€æ”¾å¼ç»“å°¾æ¨¡å¼ï¼ˆäººç±»é£æ ¼å¼±é—­åˆï¼‰
    OPEN_ENDING_PATTERNS = [
        r"(remains|remain) (unclear|to be seen|open)",
        r"further (research|investigation|study) (is|are) (needed|required|warranted)",
        r"(what|whether|how|why).*\?$",
        r"(may|might|could) (warrant|require|benefit from)",
        r"the (extent|degree|nature) of.*remains",
        r"(opens|raises|suggests) (new )?(questions|possibilities)",
    ]

    # P2 Enhancement: Hedging words for conclusion analysis
    # P2å¢å¼ºï¼šç»“è®ºåˆ†æä¸­çš„å¼±åŒ–è¯
    HEDGING_WORDS = [
        "may", "might", "could", "possibly", "perhaps", "likely",
        "appears", "seems", "suggests", "indicates", "potentially",
        "to some extent", "in part", "somewhat", "arguably",
    ]

    # P2 Enhancement: Explicit connectors for lexical echo analysis
    # P2å¢å¼ºï¼šè¯æ±‡å›å£°åˆ†æçš„æ˜¾æ€§è¿æ¥è¯
    EXPLICIT_CONNECTORS = [
        "furthermore", "moreover", "additionally", "in addition",
        "however", "nevertheless", "nonetheless", "conversely",
        "therefore", "thus", "hence", "consequently", "accordingly",
        "similarly", "likewise", "in contrast", "on the other hand",
        "first", "second", "third", "finally", "lastly",
        "for example", "for instance", "specifically", "namely",
    ]

    # 7th Indicator Enhancement: Cross-reference patterns
    # ç¬¬7æŒ‡å¾å¢å¼ºï¼šäº¤å‰å¼•ç”¨æ¨¡å¼
    CROSS_REFERENCE_PATTERNS = [
        r"as (mentioned|discussed|noted|shown|demonstrated|described) (earlier|above|previously|before|in section)",
        r"(this|these|the) (mechanism|phenomenon|pattern|approach|method|finding|result|observation)s? (again |)(appear|emerge|manifest|recur)",
        r"returning to (the|our|this)",
        r"recall(ing)? (that |)(the |our |this )",
        r"(revisiting|revisit) (the|our|this)",
        r"as we (saw|noted|observed|discussed|mentioned)",
        r"(the|this) (earlier|previous|above|aforementioned) (discussion|analysis|section|point|argument)",
        r"building (on|upon) (the|this|our) (earlier|previous)",
        r"(this|these) (connect|relate|link|tie)s? (back )?(to|with)",
        r"(consistent|in line|in keeping) with (the|our|this) (earlier|previous)",
    ]

    # 7th Indicator: Core concept callback patterns
    # ç¬¬7æŒ‡å¾ï¼šæ ¸å¿ƒæ¦‚å¿µå›è°ƒæ¨¡å¼
    CONCEPT_CALLBACK_PATTERNS = [
        r"(this|the) (central|core|key|main|primary|fundamental) (concept|idea|theme|argument|thesis)",
        r"(the|this) (recurring|underlying|overarching) (theme|pattern|principle)",
        r"once again",
        r"(return|come back) to",
        r"(echo|mirror|reflect)s? (the|this|our) (earlier|initial|original)",
    ]

    # 7-Indicator Configuration
    # 7æŒ‡å¾é…ç½®
    INDICATOR_CONFIG = {
        "symmetry": {
            "name": "Perfect Symmetry",
            "name_zh": "é€»è¾‘æ¨è¿›å¯¹ç§°",
            "risk_level": 3,
            "emoji": "âš–ï¸",
            "color_triggered": "#ef4444",  # red
            "color_safe": "#22c55e",  # green
            "description": "Parallel structure: First/Second/Third pattern",
            "description_zh": "é¦–å…ˆ/å…¶æ¬¡/æœ€å çš„å¹³è¡Œç»“æ„"
        },
        "uniform_function": {
            "name": "Uniform Paragraph Function",
            "name_zh": "æ®µè½åŠŸèƒ½å‡åŒ€",
            "risk_level": 2,
            "emoji": "ğŸ“Š",
            "color_triggered": "#f97316",  # orange
            "color_safe": "#22c55e",
            "description": "Every paragraph has complete claim-explain-conclude",
            "description_zh": "æ¯æ®µéƒ½å®Œæ•´'æå‡º-è§£é‡Š-æ€»ç»“'"
        },
        "explicit_connectors": {
            "name": "Over-signaled Transitions",
            "name_zh": "è¿æ¥è¯ä¾èµ–",
            "risk_level": 3,
            "emoji": "ğŸ”—",
            "color_triggered": "#ef4444",
            "color_safe": "#22c55e",
            "description": "Heavy use of Furthermore/Moreover/Additionally",
            "description_zh": "æ®µé¦–å¯†é›†ä½¿ç”¨æ˜¾æ€§è¿æ¥è¯"
        },
        "linear_progression": {
            "name": "Linear Enumeration",
            "name_zh": "å•ä¸€çº¿æ€§æ¨è¿›",
            "risk_level": 3,
            "emoji": "ğŸ“",
            "color_triggered": "#ef4444",
            "color_safe": "#22c55e",
            "description": "Point 1 â†’ Point 2 â†’ Point 3 â†’ Conclusion",
            "description_zh": "è§‚ç‚¹1â†’è§‚ç‚¹2â†’è§‚ç‚¹3â†’æ€»ç»“"
        },
        "rhythmic_regularity": {
            "name": "Rhythmic Regularity",
            "name_zh": "æ®µè½èŠ‚å¥å‡è¡¡",
            "risk_level": 2,
            "emoji": "ğŸ“",
            "color_triggered": "#f97316",
            "color_safe": "#22c55e",
            "description": "All paragraphs have similar length",
            "description_zh": "å„æ®µé•¿åº¦é«˜åº¦ä¸€è‡´"
        },
        "over_conclusive": {
            "name": "Over-conclusive Ending",
            "name_zh": "ç»“å°¾è¿‡åº¦é—­åˆ",
            "risk_level": 2,
            "emoji": "ğŸ”’",
            "color_triggered": "#f97316",
            "color_safe": "#22c55e",
            "description": "Strong 'In conclusion...' pattern with no open questions",
            "description_zh": "'In conclusion...'å¼ç»“å°¾ï¼Œæ— å¼€æ”¾é—®é¢˜"
        },
        "no_cross_reference": {
            "name": "No Cross-References",
            "name_zh": "ç¼ºä¹å›æŒ‡ç»“æ„",
            "risk_level": 2,
            "emoji": "ğŸ”„",
            "color_triggered": "#f97316",
            "color_safe": "#22c55e",
            "description": "Paragraphs are independent modules without callbacks",
            "description_zh": "æ®µè½å¦‚ç‹¬ç«‹æ¨¡å—ï¼Œæ— äº¤å‰å‘¼åº”"
        },
    }

    def __init__(self):
        """Initialize the structure analyzer åˆå§‹åŒ–ç»“æ„åˆ†æå™¨"""
        # Compile patterns for efficiency
        # é¢„ç¼–è¯‘æ¨¡å¼ä»¥æé«˜æ•ˆç‡
        self.topic_patterns = [re.compile(p, re.IGNORECASE) for p in self.TOPIC_SENTENCE_PATTERNS]
        self.summary_patterns = [re.compile(p, re.IGNORECASE) for p in self.SUMMARY_PATTERNS]

        # P1 Enhancement: Compile progression detection patterns
        # P1å¢å¼ºï¼šç¼–è¯‘æ¨è¿›æ£€æµ‹æ¨¡å¼
        self.backward_ref_patterns = [re.compile(p, re.IGNORECASE) for p in self.BACKWARD_REFERENCE_PATTERNS]
        self.conditional_patterns = [re.compile(p, re.IGNORECASE) for p in self.CONDITIONAL_PATTERNS]
        self.forward_only_patterns = [re.compile(p, re.IGNORECASE) for p in self.FORWARD_ONLY_PATTERNS]

        # P2 Enhancement: Compile closure and lexical echo patterns
        # P2å¢å¼ºï¼šç¼–è¯‘é—­åˆå’Œè¯æ±‡å›å£°æ¨¡å¼
        self.formulaic_conclusion_patterns = [re.compile(p, re.IGNORECASE) for p in self.FORMULAIC_CONCLUSION_PATTERNS]
        self.open_ending_patterns = [re.compile(p, re.IGNORECASE) for p in self.OPEN_ENDING_PATTERNS]

        # 7th Indicator Enhancement: Compile cross-reference patterns
        # ç¬¬7æŒ‡å¾å¢å¼ºï¼šç¼–è¯‘äº¤å‰å¼•ç”¨æ¨¡å¼
        self.cross_reference_patterns = [re.compile(p, re.IGNORECASE) for p in self.CROSS_REFERENCE_PATTERNS]
        self.concept_callback_patterns = [re.compile(p, re.IGNORECASE) for p in self.CONCEPT_CALLBACK_PATTERNS]

    def analyze(
        self,
        text: str,
        extract_thesis: bool = True
    ) -> StructureAnalysisResult:
        """
        Analyze document structure
        åˆ†ææ–‡æ¡£ç»“æ„

        Args:
            text: Full document text å®Œæ•´æ–‡æ¡£æ–‡æœ¬
            extract_thesis: Whether to extract core thesis æ˜¯å¦æå–æ ¸å¿ƒè®ºç‚¹

        Returns:
            StructureAnalysisResult with issues and scores
            åŒ…å«é—®é¢˜å’Œåˆ†æ•°çš„StructureAnalysisResult
        """
        # Split into paragraphs
        # åˆ†å‰²ä¸ºæ®µè½
        paragraphs = self._split_paragraphs(text)

        if len(paragraphs) < 2:
            return StructureAnalysisResult(
                total_paragraphs=len(paragraphs),
                total_sentences=0,
                total_words=0,
                avg_paragraph_length=0,
                paragraph_length_variance=0,
                structure_score=0,
                risk_level="low",
                message="Document too short for structure analysis.",
                message_zh="æ–‡æ¡£å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œç»“æ„åˆ†æã€‚"
            )

        # Analyze each paragraph
        # åˆ†ææ¯ä¸ªæ®µè½
        paragraph_infos = [self._analyze_paragraph(i, p) for i, p in enumerate(paragraphs)]

        # Calculate basic statistics
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        total_words = sum(p.word_count for p in paragraph_infos)
        total_sentences = sum(p.sentence_count for p in paragraph_infos)
        avg_length = total_words / len(paragraphs)
        lengths = [p.word_count for p in paragraph_infos]
        variance = sum((l - avg_length) ** 2 for l in lengths) / len(lengths)

        # Initialize result
        # åˆå§‹åŒ–ç»“æœ
        result = StructureAnalysisResult(
            total_paragraphs=len(paragraphs),
            total_sentences=total_sentences,
            total_words=total_words,
            avg_paragraph_length=avg_length,
            paragraph_length_variance=variance,
            structure_score=0,
            risk_level="low",
            paragraphs=paragraph_infos
        )

        issues: List[StructureIssue] = []
        break_points: List[BreakPoint] = []
        score = 0

        # 1. Check for linear flow (1-2-3 pattern)
        # 1. æ£€æŸ¥çº¿æ€§æµç¨‹ï¼ˆ1-2-3æ¨¡å¼ï¼‰
        linear_result = self._check_linear_flow(paragraph_infos)
        if linear_result["detected"]:
            result.has_linear_flow = True
            issues.append(StructureIssue(
                type="linear_flow",
                description=linear_result["description"],
                description_zh=linear_result["description_zh"],
                severity="high",
                affected_paragraphs=linear_result["affected"],
                suggestion="Consider reorganizing to break the predictable sequence.",
                suggestion_zh="è€ƒè™‘é‡æ–°ç»„ç»‡ä»¥æ‰“ç ´å¯é¢„æµ‹çš„é¡ºåºã€‚"
            ))
            score += 25

        # 2. Check for repetitive paragraph structure
        # 2. æ£€æŸ¥é‡å¤çš„æ®µè½ç»“æ„
        repetitive_result = self._check_repetitive_pattern(paragraph_infos)
        if repetitive_result["detected"]:
            result.has_repetitive_pattern = True
            issues.append(StructureIssue(
                type="repetitive_pattern",
                description=repetitive_result["description"],
                description_zh=repetitive_result["description_zh"],
                severity="medium",
                affected_paragraphs=repetitive_result["affected"],
                suggestion="Vary paragraph openings and structures.",
                suggestion_zh="å˜åŒ–æ®µè½å¼€å¤´å’Œç»“æ„ã€‚"
            ))
            score += 20

        # 3. Check for uniform paragraph lengths
        # 3. æ£€æŸ¥å‡åŒ€çš„æ®µè½é•¿åº¦
        uniform_result = self._check_uniform_length(paragraph_infos)
        if uniform_result["detected"]:
            result.has_uniform_length = True
            issues.append(StructureIssue(
                type="uniform_length",
                description=uniform_result["description"],
                description_zh=uniform_result["description_zh"],
                severity="medium",
                affected_paragraphs=uniform_result["affected"],
                suggestion="Vary paragraph lengths for natural rhythm.",
                suggestion_zh="å˜åŒ–æ®µè½é•¿åº¦ä»¥è·å¾—è‡ªç„¶èŠ‚å¥ã€‚"
            ))
            score += 15

        # 4. Check for predictable introduction-body-conclusion
        # 4. æ£€æŸ¥å¯é¢„æµ‹çš„å¼•è¨€-æ­£æ–‡-ç»“è®ºç»“æ„
        predictable_result = self._check_predictable_order(paragraph_infos)
        if predictable_result["detected"]:
            result.has_predictable_order = True
            issues.append(StructureIssue(
                type="predictable_order",
                description=predictable_result["description"],
                description_zh=predictable_result["description_zh"],
                severity="low",
                affected_paragraphs=predictable_result["affected"],
                suggestion="Consider interspersing evidence and analysis.",
                suggestion_zh="è€ƒè™‘ç©¿æ’è¯æ®å’Œåˆ†æã€‚"
            ))
            score += 10

        # 5. Find break points
        # 5. æ‰¾å‡ºæ–­ç‚¹
        break_points = self._find_break_points(paragraph_infos)
        result.break_points = break_points

        # 6. Extract thesis if requested
        # 6. å¦‚æœéœ€è¦ï¼Œæå–è®ºç‚¹
        if extract_thesis:
            thesis_result = self._extract_thesis(paragraph_infos)
            result.core_thesis = thesis_result.get("thesis")
            result.key_arguments = thesis_result.get("arguments", [])

        # 7. P1 Enhancement: Analyze progression type
        # 7. P1å¢å¼ºï¼šåˆ†ææ¨è¿›ç±»å‹
        result.progression_analysis = self.analyze_progression_type(paragraph_infos)
        # Add to score if monotonic (AI-like)
        # å¦‚æœæ˜¯å•è°ƒçš„ï¼ˆAIé£æ ¼ï¼‰ï¼ŒåŠ å…¥åˆ†æ•°
        if result.progression_analysis.progression_type == "monotonic":
            score += 15

        # 8. P1 Enhancement: Analyze function distribution
        # 8. P1å¢å¼ºï¼šåˆ†æåŠŸèƒ½åˆ†å¸ƒ
        result.function_distribution = self.analyze_function_distribution(paragraph_infos)
        # Add to score if uniform (AI-like)
        # å¦‚æœæ˜¯å‡åŒ€çš„ï¼ˆAIé£æ ¼ï¼‰ï¼ŒåŠ å…¥åˆ†æ•°
        if result.function_distribution.distribution_type == "uniform":
            score += 10

        # 9. P2 Enhancement: Analyze closure pattern
        # 9. P2å¢å¼ºï¼šåˆ†æé—­åˆæ¨¡å¼
        result.closure_analysis = self.analyze_closure(paragraph_infos)
        # Add to score if strong closure (AI-like)
        # å¦‚æœæ˜¯å¼ºé—­åˆï¼ˆAIé£æ ¼ï¼‰ï¼ŒåŠ å…¥åˆ†æ•°
        if result.closure_analysis.closure_type == "strong":
            score += 10

        # 10. P2 Enhancement: Analyze lexical echo
        # 10. P2å¢å¼ºï¼šåˆ†æè¯æ±‡å›å£°
        result.lexical_echo_analysis = self.analyze_lexical_echo(paragraph_infos)
        # Add to score if high explicit connector ratio (AI-like)
        # å¦‚æœæ˜¾æ€§è¿æ¥è¯æ¯”ä¾‹é«˜ï¼ˆAIé£æ ¼ï¼‰ï¼ŒåŠ å…¥åˆ†æ•°
        if result.lexical_echo_analysis.score >= 60:
            score += 5

        # 11. 7th Indicator Enhancement: Analyze cross-references
        # 11. ç¬¬7æŒ‡å¾å¢å¼ºï¼šåˆ†æäº¤å‰å¼•ç”¨
        result.cross_reference_analysis = self.analyze_cross_references(paragraph_infos)
        # Add to score if lacking cross-references (AI-like)
        # å¦‚æœç¼ºå°‘äº¤å‰å¼•ç”¨ï¼ˆAIé£æ ¼ï¼‰ï¼ŒåŠ å…¥åˆ†æ•°
        if not result.cross_reference_analysis.has_cross_references:
            score += 5

        # Calculate final score and level
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°å’Œç­‰çº§
        result.structure_score = min(score, 100)
        result.risk_level = self._score_to_level(result.structure_score)
        result.issues = issues

        # Generate messages
        # ç”Ÿæˆæ¶ˆæ¯
        result.message, result.message_zh = self._generate_messages(result)

        # 12. Generate 7-Indicator Risk Card
        # 12. ç”Ÿæˆ7æŒ‡å¾é£é™©å¡ç‰‡
        result.risk_card = self.generate_risk_card(result)

        return result

    def _split_paragraphs(self, text: str) -> List[str]:
        """
        Split text into paragraphs
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ®µè½
        """
        # Split by double newlines or more
        # æŒ‰åŒæ¢è¡Œç¬¦æˆ–æ›´å¤šåˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text.strip())
        # Filter out empty paragraphs and very short ones
        # è¿‡æ»¤ç©ºæ®µè½å’Œéå¸¸çŸ­çš„æ®µè½
        return [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 20]

    def _analyze_paragraph(self, index: int, text: str) -> ParagraphInfo:
        """
        Analyze a single paragraph
        åˆ†æå•ä¸ªæ®µè½
        """
        sentences = self._split_sentences(text)
        words = text.split()

        first_sentence = sentences[0] if sentences else ""
        last_sentence = sentences[-1] if sentences else ""

        # Check for topic sentence pattern
        # æ£€æŸ¥ä¸»é¢˜å¥æ¨¡å¼
        has_topic = any(p.search(first_sentence) for p in self.topic_patterns)

        # Check for summary ending
        # æ£€æŸ¥æ€»ç»“ç»“å°¾
        has_summary = any(p.search(last_sentence) for p in self.summary_patterns)

        # Find connector words
        # æŸ¥æ‰¾è¿æ¥è¯
        connectors = []
        for trans in self.LINEAR_TRANSITIONS:
            if text.lower().startswith(trans.lower()):
                connectors.append(trans)
            elif f" {trans.lower()}" in text.lower():
                connectors.append(trans)

        # Determine function type
        # ç¡®å®šåŠŸèƒ½ç±»å‹
        function_type = self._determine_function(text)

        return ParagraphInfo(
            index=index,
            text=text,
            first_sentence=first_sentence,
            last_sentence=last_sentence,
            word_count=len(words),
            sentence_count=len(sentences),
            has_topic_sentence=has_topic,
            has_summary_ending=has_summary,
            connector_words=connectors,
            function_type=function_type
        )

    def _split_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­
        """
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        return [s.strip() for s in sentences if s.strip()]

    def _determine_function(self, text: str) -> str:
        """
        Determine the function type of a paragraph
        ç¡®å®šæ®µè½çš„åŠŸèƒ½ç±»å‹
        """
        text_lower = text.lower()

        for func_type, keywords in self.FUNCTION_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return func_type

        return "body"  # Default to body paragraph

    def _check_linear_flow(self, paragraphs: List[ParagraphInfo]) -> Dict:
        """
        Check for linear 1-2-3 flow pattern
        æ£€æŸ¥çº¿æ€§1-2-3æµç¨‹æ¨¡å¼
        """
        linear_markers = []
        affected = []

        for p in paragraphs:
            if p.connector_words:
                for conn in p.connector_words:
                    if conn in self.LINEAR_TRANSITIONS[:10]:  # First, Second, Third, etc.
                        linear_markers.append(p.index)
                        affected.append(p.index)

        if len(linear_markers) >= 3:
            return {
                "detected": True,
                "description": f"Found {len(linear_markers)} linear transition markers (First, Second, etc.)",
                "description_zh": f"å‘ç° {len(linear_markers)} ä¸ªçº¿æ€§è¿‡æ¸¡æ ‡è®°ï¼ˆç¬¬ä¸€ã€ç¬¬äºŒç­‰ï¼‰",
                "affected": affected
            }

        return {"detected": False}

    def _check_repetitive_pattern(self, paragraphs: List[ParagraphInfo]) -> Dict:
        """
        Check for repetitive paragraph structure
        æ£€æŸ¥é‡å¤çš„æ®µè½ç»“æ„
        """
        topic_count = sum(1 for p in paragraphs if p.has_topic_sentence)
        total = len(paragraphs)

        if total >= 3 and topic_count / total > 0.7:
            affected = [p.index for p in paragraphs if p.has_topic_sentence]
            return {
                "detected": True,
                "description": f"{topic_count}/{total} paragraphs start with topic sentences",
                "description_zh": f"{topic_count}/{total} ä¸ªæ®µè½ä»¥ä¸»é¢˜å¥å¼€å¤´",
                "affected": affected
            }

        return {"detected": False}

    def _check_uniform_length(self, paragraphs: List[ParagraphInfo]) -> Dict:
        """
        Check for uniform paragraph lengths
        æ£€æŸ¥å‡åŒ€çš„æ®µè½é•¿åº¦
        """
        lengths = [p.word_count for p in paragraphs]
        if not lengths:
            return {"detected": False}

        avg = sum(lengths) / len(lengths)
        # Check if all lengths are within 30% of average
        # æ£€æŸ¥æ‰€æœ‰é•¿åº¦æ˜¯å¦åœ¨å¹³å‡å€¼çš„30%ä»¥å†…
        uniform_count = sum(1 for l in lengths if abs(l - avg) / avg < 0.3)

        if len(paragraphs) >= 4 and uniform_count / len(paragraphs) > 0.75:
            return {
                "detected": True,
                "description": f"Paragraphs have uniform length (avg: {avg:.0f} words, {uniform_count}/{len(paragraphs)} similar)",
                "description_zh": f"æ®µè½é•¿åº¦å‡åŒ€ï¼ˆå¹³å‡ï¼š{avg:.0f}è¯ï¼Œ{uniform_count}/{len(paragraphs)}ä¸ªç›¸ä¼¼ï¼‰",
                "affected": list(range(len(paragraphs)))
            }

        return {"detected": False}

    def _check_predictable_order(self, paragraphs: List[ParagraphInfo]) -> Dict:
        """
        Check for predictable introduction-body-conclusion pattern
        æ£€æŸ¥å¯é¢„æµ‹çš„å¼•è¨€-æ­£æ–‡-ç»“è®ºæ¨¡å¼
        """
        if len(paragraphs) < 3:
            return {"detected": False}

        functions = [p.function_type for p in paragraphs]

        # Check for classic pattern
        # æ£€æŸ¥ç»å…¸æ¨¡å¼
        if (functions[0] == "introduction" and
            functions[-1] == "conclusion" and
            all(f == "body" for f in functions[1:-1])):
            return {
                "detected": True,
                "description": "Classic introduction-body-conclusion structure detected",
                "description_zh": "æ£€æµ‹åˆ°ç»å…¸çš„å¼•è¨€-æ­£æ–‡-ç»“è®ºç»“æ„",
                "affected": [0, len(paragraphs) - 1]
            }

        return {"detected": False}

    def _find_break_points(self, paragraphs: List[ParagraphInfo]) -> List[BreakPoint]:
        """
        Find logic break points in document
        æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„é€»è¾‘æ–­ç‚¹
        """
        break_points = []

        for i in range(1, len(paragraphs)):
            prev = paragraphs[i - 1]
            curr = paragraphs[i]

            # Check for abrupt topic shift
            # æ£€æŸ¥çªç„¶çš„ä¸»é¢˜è½¬å˜
            if prev.function_type != curr.function_type:
                if prev.function_type == "evidence" and curr.function_type == "conclusion":
                    break_points.append(BreakPoint(
                        position=i,
                        type="abrupt_transition",
                        description="Jump from evidence to conclusion without analysis",
                        description_zh="ä»è¯æ®ç›´æ¥è·³åˆ°ç»“è®ºï¼Œç¼ºå°‘åˆ†æ"
                    ))

        return break_points

    def _extract_thesis(self, paragraphs: List[ParagraphInfo]) -> Dict:
        """
        Extract core thesis and key arguments
        æå–æ ¸å¿ƒè®ºç‚¹å’Œå…³é”®è®ºæ®
        """
        # Look in first few paragraphs for thesis
        # åœ¨å‰å‡ æ®µä¸­æŸ¥æ‰¾è®ºç‚¹
        thesis = None
        arguments = []

        for p in paragraphs[:3]:
            # Look for thesis indicators
            # æŸ¥æ‰¾è®ºç‚¹æŒ‡ç¤ºè¯
            if "argue" in p.text.lower() or "thesis" in p.text.lower() or "claim" in p.text.lower():
                thesis = p.first_sentence
                break
            elif p.function_type == "introduction":
                # Use last sentence of introduction as thesis
                # ä½¿ç”¨å¼•è¨€çš„æœ€åä¸€å¥ä½œä¸ºè®ºç‚¹
                thesis = p.last_sentence

        # Extract key arguments from body paragraphs
        # ä»æ­£æ–‡æ®µè½ä¸­æå–å…³é”®è®ºæ®
        for p in paragraphs:
            if p.has_topic_sentence and p.function_type in ["body", "analysis", "evidence"]:
                arguments.append(p.first_sentence)

        return {
            "thesis": thesis,
            "arguments": arguments[:5]  # Limit to top 5
        }

    def _score_to_level(self, score: int) -> str:
        """
        Convert structure score to risk level
        å°†ç»“æ„åˆ†æ•°è½¬æ¢ä¸ºé£é™©ç­‰çº§
        """
        if score >= 50:
            return "high"
        elif score >= 25:
            return "medium"
        return "low"

    def _generate_messages(self, result: StructureAnalysisResult) -> Tuple[str, str]:
        """
        Generate human-readable messages
        ç”Ÿæˆäººç±»å¯è¯»çš„æ¶ˆæ¯
        """
        if result.structure_score < 25:
            return (
                "Document structure appears natural with varied patterns.",
                "æ–‡æ¡£ç»“æ„çœ‹èµ·æ¥è‡ªç„¶ï¼Œæ¨¡å¼å¤šæ ·ã€‚"
            )
        elif result.structure_score < 50:
            return (
                f"Some structural patterns detected ({len(result.issues)} issues). Consider varying structure.",
                f"æ£€æµ‹åˆ°ä¸€äº›ç»“æ„æ¨¡å¼ï¼ˆ{len(result.issues)}ä¸ªé—®é¢˜ï¼‰ã€‚å»ºè®®å˜åŒ–ç»“æ„ã€‚"
            )
        else:
            return (
                f"Strong AI structural patterns detected ({len(result.issues)} issues). Structure needs significant revision.",
                f"æ£€æµ‹åˆ°å¼ºAIç»“æ„æ¨¡å¼ï¼ˆ{len(result.issues)}ä¸ªé—®é¢˜ï¼‰ã€‚ç»“æ„éœ€è¦å¤§å¹…ä¿®æ”¹ã€‚"
            )

    # =========================================================================
    # P1 Enhancement: Progression Type Detection
    # P1å¢å¼ºï¼šæ¨è¿›ç±»å‹æ£€æµ‹
    # =========================================================================

    def analyze_progression_type(self, paragraphs: List[ParagraphInfo]) -> ProgressionAnalysis:
        """
        Analyze the progression type of document structure
        åˆ†ææ–‡æ¡£ç»“æ„çš„æ¨è¿›ç±»å‹

        Detects:
        - Monotonic: Forward-only progression (AI-typical)
        - Non-monotonic: References back, conditional, recursive (human-typical)
        - Mixed: Combination of both

        Args:
            paragraphs: List of analyzed paragraphs åˆ†æåçš„æ®µè½åˆ—è¡¨

        Returns:
            ProgressionAnalysis with type and score åŒ…å«ç±»å‹å’Œåˆ†æ•°çš„æ¨è¿›åˆ†æ
        """
        forward_count = 0
        backward_count = 0
        conditional_count = 0
        details = []

        full_text = " ".join([p.text for p in paragraphs])

        # Count forward-only transitions
        # ç»Ÿè®¡å•å‘æ¨è¿›è¿‡æ¸¡
        for i, pattern in enumerate(self.forward_only_patterns):
            matches = pattern.findall(full_text)
            if matches:
                forward_count += len(matches)
                details.append({
                    "type": "forward",
                    "pattern_id": i,
                    "count": len(matches)
                })

        # Count backward references
        # ç»Ÿè®¡å›æŒ‡
        for i, pattern in enumerate(self.backward_ref_patterns):
            matches = pattern.findall(full_text)
            if matches:
                backward_count += len(matches)
                details.append({
                    "type": "backward",
                    "pattern_id": i,
                    "count": len(matches)
                })

        # Count conditional statements
        # ç»Ÿè®¡æ¡ä»¶é™ˆè¿°
        for i, pattern in enumerate(self.conditional_patterns):
            matches = pattern.findall(full_text)
            if matches:
                conditional_count += len(matches)
                details.append({
                    "type": "conditional",
                    "pattern_id": i,
                    "count": len(matches)
                })

        # Also count linear markers from existing detection
        # åŒæ—¶ç»Ÿè®¡ç°æœ‰æ£€æµ‹ä¸­çš„çº¿æ€§æ ‡è®°
        linear_marker_count = sum(len(p.connector_words) for p in paragraphs)
        forward_count += linear_marker_count

        # Determine progression type
        # ç¡®å®šæ¨è¿›ç±»å‹
        total_non_monotonic = backward_count + conditional_count
        total_forward = forward_count

        if total_non_monotonic == 0 and total_forward > 0:
            progression_type = "monotonic"
            progression_type_zh = "å•è°ƒæ¨è¿›"
            # High score = AI-like
            score = min(100, 50 + (total_forward * 10))
        elif total_non_monotonic > total_forward:
            progression_type = "non_monotonic"
            progression_type_zh = "éå•è°ƒæ¨è¿›"
            # Low score = human-like
            score = max(0, 50 - (total_non_monotonic * 10))
        else:
            progression_type = "mixed"
            progression_type_zh = "æ··åˆæ¨è¿›"
            # Medium score
            ratio = total_forward / max(1, total_forward + total_non_monotonic)
            score = int(ratio * 100)

        return ProgressionAnalysis(
            progression_type=progression_type,
            progression_type_zh=progression_type_zh,
            forward_transitions=forward_count,
            backward_references=backward_count,
            conditional_statements=conditional_count,
            score=score,
            details=details
        )

    # =========================================================================
    # P1 Enhancement: Function Distribution Detection
    # P1å¢å¼ºï¼šåŠŸèƒ½åˆ†å¸ƒæ£€æµ‹
    # =========================================================================

    def analyze_function_distribution(self, paragraphs: List[ParagraphInfo]) -> FunctionDistribution:
        """
        Analyze the function distribution across paragraphs
        åˆ†ææ®µè½é—´çš„åŠŸèƒ½åˆ†å¸ƒ

        Detects:
        - Uniform: All paragraphs have similar depth/length (AI-typical)
        - Asymmetric: Some topics get deep dives, others brief mention (human-typical)
        - Balanced: Reasonable variation

        Args:
            paragraphs: List of analyzed paragraphs åˆ†æåçš„æ®µè½åˆ—è¡¨

        Returns:
            FunctionDistribution with type and score åŒ…å«ç±»å‹å’Œåˆ†æ•°çš„åŠŸèƒ½åˆ†å¸ƒ
        """
        if not paragraphs:
            return FunctionDistribution(
                distribution_type="unknown",
                distribution_type_zh="æœªçŸ¥",
                function_counts={},
                depth_variance=0.0,
                longest_section_ratio=0.0,
                score=50,
                asymmetry_opportunities=[]
            )

        # Count functions
        # ç»Ÿè®¡åŠŸèƒ½
        function_counts: Dict[str, int] = {}
        for p in paragraphs:
            func = p.function_type
            function_counts[func] = function_counts.get(func, 0) + 1

        # Calculate depth variance using word counts
        # ä½¿ç”¨è¯æ•°è®¡ç®—æ·±åº¦æ–¹å·®
        word_counts = [p.word_count for p in paragraphs]
        avg_words = sum(word_counts) / len(word_counts)
        variance = sum((wc - avg_words) ** 2 for wc in word_counts) / len(word_counts)
        std_dev = variance ** 0.5
        coefficient_of_variation = std_dev / avg_words if avg_words > 0 else 0

        # Find longest section ratio
        # æ‰¾å‡ºæœ€é•¿éƒ¨åˆ†çš„æ¯”ä¾‹
        max_words = max(word_counts)
        longest_ratio = max_words / avg_words if avg_words > 0 else 1.0

        # Identify asymmetry opportunities (paragraphs that could be expanded/compressed)
        # è¯†åˆ«éå¯¹ç§°æœºä¼šï¼ˆå¯ä»¥æ‰©å±•/å‹ç¼©çš„æ®µè½ï¼‰
        asymmetry_opportunities = []

        # Find paragraphs significantly shorter than average (compression candidates)
        # æ‰¾å‡ºæ˜¾è‘—çŸ­äºå¹³å‡å€¼çš„æ®µè½ï¼ˆå‹ç¼©å€™é€‰ï¼‰
        short_threshold = avg_words * 0.6
        long_threshold = avg_words * 1.4

        for i, p in enumerate(paragraphs):
            if p.word_count < short_threshold:
                asymmetry_opportunities.append({
                    "index": i,
                    "type": "expand_candidate",
                    "current_words": p.word_count,
                    "reason": "Paragraph is significantly shorter than average",
                    "reason_zh": "æ®µè½æ˜¾è‘—çŸ­äºå¹³å‡å€¼"
                })
            elif p.word_count > long_threshold:
                asymmetry_opportunities.append({
                    "index": i,
                    "type": "already_expanded",
                    "current_words": p.word_count,
                    "reason": "Paragraph has good depth",
                    "reason_zh": "æ®µè½æ·±åº¦è‰¯å¥½"
                })

        # Determine distribution type based on coefficient of variation
        # æ ¹æ®å˜å¼‚ç³»æ•°ç¡®å®šåˆ†å¸ƒç±»å‹
        if coefficient_of_variation < 0.2:
            distribution_type = "uniform"
            distribution_type_zh = "å‡åŒ€åˆ†å¸ƒ"
            # High score = AI-like
            score = 80 + int((0.2 - coefficient_of_variation) * 100)
        elif coefficient_of_variation > 0.5:
            distribution_type = "asymmetric"
            distribution_type_zh = "éå¯¹ç§°åˆ†å¸ƒ"
            # Low score = human-like
            score = max(10, 50 - int((coefficient_of_variation - 0.5) * 60))
        else:
            distribution_type = "balanced"
            distribution_type_zh = "å¹³è¡¡åˆ†å¸ƒ"
            # Medium score
            score = 50

        return FunctionDistribution(
            distribution_type=distribution_type,
            distribution_type_zh=distribution_type_zh,
            function_counts=function_counts,
            depth_variance=variance,
            longest_section_ratio=longest_ratio,
            score=min(100, max(0, score)),
            asymmetry_opportunities=asymmetry_opportunities
        )

    # =========================================================================
    # P2 Enhancement: Closure Pattern Detection
    # P2å¢å¼ºï¼šé—­åˆæ¨¡å¼æ£€æµ‹
    # =========================================================================

    def analyze_closure(self, paragraphs: List[ParagraphInfo]) -> ClosureAnalysis:
        """
        Analyze the closure pattern of the document
        åˆ†ææ–‡æ¡£çš„é—­åˆæ¨¡å¼

        Detects:
        - Strong: Formulaic conclusion with complete resolution (AI-typical)
        - Moderate: Some closure but with open elements
        - Weak: Minimal closure, questions remain
        - Open: Ends with questions or unresolved tension (human-typical)

        Args:
            paragraphs: List of analyzed paragraphs åˆ†æåçš„æ®µè½åˆ—è¡¨

        Returns:
            ClosureAnalysis with type and score åŒ…å«ç±»å‹å’Œåˆ†æ•°çš„é—­åˆåˆ†æ
        """
        if not paragraphs:
            return ClosureAnalysis(
                closure_type="unknown",
                closure_type_zh="æœªçŸ¥",
                has_formulaic_ending=False,
                has_complete_resolution=False,
                open_questions=0,
                hedging_in_conclusion=0,
                score=50,
                detected_patterns=[]
            )

        # Focus on last 1-2 paragraphs for conclusion analysis
        # å…³æ³¨æœ€å1-2æ®µè¿›è¡Œç»“è®ºåˆ†æ
        conclusion_paras = paragraphs[-2:] if len(paragraphs) >= 2 else paragraphs[-1:]
        conclusion_text = " ".join([p.text for p in conclusion_paras])
        conclusion_text_lower = conclusion_text.lower()

        # Check for formulaic conclusion patterns
        # æ£€æŸ¥å…¬å¼åŒ–ç»“è®ºæ¨¡å¼
        detected_patterns = []
        has_formulaic = False
        for pattern in self.formulaic_conclusion_patterns:
            if pattern.search(conclusion_text):
                has_formulaic = True
                detected_patterns.append(f"formulaic: {pattern.pattern}")

        # Check for open ending patterns
        # æ£€æŸ¥å¼€æ”¾å¼ç»“å°¾æ¨¡å¼
        open_questions = 0
        for pattern in self.open_ending_patterns:
            matches = pattern.findall(conclusion_text)
            if matches:
                open_questions += len(matches)
                detected_patterns.append(f"open: {pattern.pattern}")

        # Count hedging words in conclusion
        # ç»Ÿè®¡ç»“è®ºä¸­çš„å¼±åŒ–è¯
        hedging_count = 0
        for hedge in self.HEDGING_WORDS:
            count = conclusion_text_lower.count(hedge.lower())
            hedging_count += count

        # Check for question marks (indicates open questions)
        # æ£€æŸ¥é—®å·ï¼ˆè¡¨ç¤ºå¼€æ”¾é—®é¢˜ï¼‰
        question_marks = conclusion_text.count("?")
        open_questions += question_marks

        # Determine if there's complete resolution
        # ç¡®å®šæ˜¯å¦æœ‰å®Œå…¨è§£å†³
        # Strong resolution indicators
        resolution_words = ["demonstrates", "proves", "confirms", "establishes", "clearly shows"]
        has_complete_resolution = any(word in conclusion_text_lower for word in resolution_words)

        # Calculate closure score and type
        # è®¡ç®—é—­åˆåˆ†æ•°å’Œç±»å‹
        score = 50  # Start at neutral

        if has_formulaic:
            score += 30
        if has_complete_resolution:
            score += 20

        if open_questions > 0:
            score -= 15 * min(open_questions, 3)
        if hedging_count > 2:
            score -= 10

        score = max(0, min(100, score))

        # Determine closure type
        # ç¡®å®šé—­åˆç±»å‹
        if score >= 70:
            closure_type = "strong"
            closure_type_zh = "å¼ºé—­åˆ"
        elif score >= 45:
            closure_type = "moderate"
            closure_type_zh = "ä¸­ç­‰é—­åˆ"
        elif score >= 25:
            closure_type = "weak"
            closure_type_zh = "å¼±é—­åˆ"
        else:
            closure_type = "open"
            closure_type_zh = "å¼€æ”¾å¼"

        return ClosureAnalysis(
            closure_type=closure_type,
            closure_type_zh=closure_type_zh,
            has_formulaic_ending=has_formulaic,
            has_complete_resolution=has_complete_resolution,
            open_questions=open_questions,
            hedging_in_conclusion=hedging_count,
            score=score,
            detected_patterns=detected_patterns
        )

    # =========================================================================
    # P2 Enhancement: Lexical Echo Score
    # P2å¢å¼ºï¼šè¯æ±‡å›å£°åˆ†æ•°
    # =========================================================================

    def analyze_lexical_echo(self, paragraphs: List[ParagraphInfo]) -> LexicalEchoAnalysis:
        """
        Analyze lexical echo between paragraphs
        åˆ†ææ®µè½é—´çš„è¯æ±‡å›å£°

        Detects whether paragraph transitions use:
        - Explicit connectors (AI-typical): Furthermore, Moreover, However
        - Lexical echo (human-typical): Repeating key concepts from previous paragraph

        Args:
            paragraphs: List of analyzed paragraphs åˆ†æåçš„æ®µè½åˆ—è¡¨

        Returns:
            LexicalEchoAnalysis with scores and details åŒ…å«åˆ†æ•°å’Œç»†èŠ‚çš„è¯æ±‡å›å£°åˆ†æ
        """
        if len(paragraphs) < 2:
            return LexicalEchoAnalysis(
                total_transitions=0,
                echo_transitions=0,
                explicit_connector_transitions=0,
                echo_ratio=0.0,
                score=50,
                transition_details=[]
            )

        total_transitions = len(paragraphs) - 1
        echo_transitions = 0
        explicit_transitions = 0
        transition_details = []

        for i in range(1, len(paragraphs)):
            prev_para = paragraphs[i - 1]
            curr_para = paragraphs[i]

            # Get key content words from previous paragraph ending
            # ä»ä¸Šä¸€æ®µè½ç»“å°¾è·å–å…³é”®å†…å®¹è¯
            prev_ending = prev_para.last_sentence.lower()
            prev_content_words = self._extract_content_words(prev_ending)

            # Get words from current paragraph opening
            # è·å–å½“å‰æ®µè½å¼€å¤´çš„è¯
            curr_opening = curr_para.first_sentence.lower()

            # Check for explicit connectors
            # æ£€æŸ¥æ˜¾æ€§è¿æ¥è¯
            has_explicit = False
            for connector in self.EXPLICIT_CONNECTORS:
                if curr_opening.startswith(connector.lower()):
                    has_explicit = True
                    explicit_transitions += 1
                    break

            # Check for lexical echo (shared content words)
            # æ£€æŸ¥è¯æ±‡å›å£°ï¼ˆå…±äº«å†…å®¹è¯ï¼‰
            curr_content_words = self._extract_content_words(curr_opening)
            shared_words = prev_content_words.intersection(curr_content_words)
            has_echo = len(shared_words) > 0

            if has_echo:
                echo_transitions += 1

            transition_details.append({
                "from_paragraph": i - 1,
                "to_paragraph": i,
                "has_explicit_connector": has_explicit,
                "has_lexical_echo": has_echo,
                "shared_words": list(shared_words),
                "connector_found": next((c for c in self.EXPLICIT_CONNECTORS
                                        if curr_opening.startswith(c.lower())), None)
            })

        # Calculate score (higher = more explicit connectors = AI-like)
        # è®¡ç®—åˆ†æ•°ï¼ˆè¶Šé«˜ = è¶Šå¤šæ˜¾æ€§è¿æ¥è¯ = AIé£æ ¼ï¼‰
        explicit_ratio = explicit_transitions / total_transitions if total_transitions > 0 else 0
        echo_ratio = echo_transitions / total_transitions if total_transitions > 0 else 0

        # Score: high explicit ratio is AI-like
        score = int(explicit_ratio * 80) + 20 if explicit_ratio > 0 else 20
        # Reduce score if there's good lexical echo
        if echo_ratio > 0.5:
            score = max(10, score - 20)

        return LexicalEchoAnalysis(
            total_transitions=total_transitions,
            echo_transitions=echo_transitions,
            explicit_connector_transitions=explicit_transitions,
            echo_ratio=echo_ratio,
            score=min(100, max(0, score)),
            transition_details=transition_details
        )

    def _extract_content_words(self, text: str) -> set:
        """
        Extract content words (nouns, verbs, adjectives) from text
        ä»æ–‡æœ¬ä¸­æå–å†…å®¹è¯ï¼ˆåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ï¼‰

        Simple heuristic: words longer than 4 characters that aren't stopwords
        ç®€å•å¯å‘å¼ï¼šé•¿åº¦è¶…è¿‡4ä¸ªå­—ç¬¦ä¸”ä¸æ˜¯åœç”¨è¯çš„è¯
        """
        STOPWORDS = {
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "of", "with", "by", "from", "as", "is", "was", "are", "were", "been",
            "be", "have", "has", "had", "do", "does", "did", "will", "would",
            "could", "should", "may", "might", "must", "shall", "can", "need",
            "this", "that", "these", "those", "it", "its", "they", "them",
            "their", "which", "who", "whom", "what", "when", "where", "why",
            "how", "all", "each", "every", "both", "few", "more", "most",
            "other", "some", "such", "no", "not", "only", "same", "so",
            "than", "too", "very", "just", "also", "now", "here", "there",
        }

        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        return {w for w in words if w not in STOPWORDS}

    # =========================================================================
    # 7th Indicator Enhancement: Cross-Reference Analysis
    # ç¬¬7æŒ‡å¾å¢å¼ºï¼šäº¤å‰å¼•ç”¨åˆ†æ
    # =========================================================================

    def analyze_cross_references(self, paragraphs: List[ParagraphInfo]) -> CrossReferenceAnalysis:
        """
        Analyze cross-referential links in the document
        åˆ†ææ–‡æ¡£ä¸­çš„äº¤å‰å¼•ç”¨é“¾æ¥

        Detects:
        - Explicit cross-references (as mentioned earlier, returning to...)
        - Core concept callbacks
        - Non-linear structural links

        Args:
            paragraphs: List of analyzed paragraphs åˆ†æåçš„æ®µè½åˆ—è¡¨

        Returns:
            CrossReferenceAnalysis with detection results åŒ…å«æ£€æµ‹ç»“æœçš„äº¤å‰å¼•ç”¨åˆ†æ
        """
        if len(paragraphs) < 2:
            return CrossReferenceAnalysis(
                has_cross_references=False,
                cross_reference_count=0,
                concept_callbacks=0,
                forward_only_ratio=1.0,
                score=80,  # High score = AI-like (no cross-refs)
                detected_references=[],
                core_concepts=[]
            )

        full_text = " ".join([p.text for p in paragraphs])
        cross_ref_count = 0
        callback_count = 0
        detected_references = []

        # Detect explicit cross-references
        # æ£€æµ‹æ˜¾å¼äº¤å‰å¼•ç”¨
        for i, pattern in enumerate(self.cross_reference_patterns):
            matches = pattern.findall(full_text)
            if matches:
                cross_ref_count += len(matches)
                for match in matches:
                    match_str = match if isinstance(match, str) else " ".join(match)
                    detected_references.append({
                        "type": "cross_reference",
                        "pattern_id": i,
                        "match": match_str
                    })

        # Detect concept callbacks
        # æ£€æµ‹æ¦‚å¿µå›è°ƒ
        for i, pattern in enumerate(self.concept_callback_patterns):
            matches = pattern.findall(full_text)
            if matches:
                callback_count += len(matches)
                for match in matches:
                    match_str = match if isinstance(match, str) else " ".join(match)
                    detected_references.append({
                        "type": "concept_callback",
                        "pattern_id": i,
                        "match": match_str
                    })

        # Extract core concepts from first paragraph (likely thesis area)
        # ä»ç¬¬ä¸€æ®µæå–æ ¸å¿ƒæ¦‚å¿µï¼ˆå¯èƒ½æ˜¯è®ºç‚¹åŒºåŸŸï¼‰
        core_concepts = list(self._extract_content_words(paragraphs[0].text))[:5]

        # Check if core concepts appear in later paragraphs (concept echoing)
        # æ£€æŸ¥æ ¸å¿ƒæ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨åé¢çš„æ®µè½ä¸­ï¼ˆæ¦‚å¿µå›å£°ï¼‰
        concept_echo_count = 0
        for concept in core_concepts:
            for p in paragraphs[2:]:  # Skip first two paragraphs
                if concept.lower() in p.text.lower():
                    concept_echo_count += 1

        # Calculate forward-only ratio
        # è®¡ç®—å•å‘æ¨è¿›æ¯”ä¾‹
        total_references = cross_ref_count + callback_count + concept_echo_count
        if total_references > 0:
            forward_only_ratio = max(0, 1.0 - (total_references / (len(paragraphs) * 0.5)))
        else:
            forward_only_ratio = 1.0

        # Calculate score (higher = more AI-like, lacking cross-refs)
        # è®¡ç®—åˆ†æ•°ï¼ˆè¶Šé«˜ = è¶ŠåƒAIï¼Œç¼ºå°‘äº¤å‰å¼•ç”¨ï¼‰
        if total_references == 0:
            score = 85
        elif total_references < 2:
            score = 70
        elif total_references < 4:
            score = 50
        else:
            score = max(10, 50 - (total_references * 5))

        return CrossReferenceAnalysis(
            has_cross_references=total_references > 0,
            cross_reference_count=cross_ref_count + callback_count,
            concept_callbacks=concept_echo_count,
            forward_only_ratio=forward_only_ratio,
            score=score,
            detected_references=detected_references,
            core_concepts=core_concepts
        )

    # =========================================================================
    # 7-Indicator Risk Card Generation
    # 7æŒ‡å¾é£é™©å¡ç‰‡ç”Ÿæˆ
    # =========================================================================

    def generate_risk_card(self, result: 'StructureAnalysisResult') -> StructuralRiskCard:
        """
        Generate a 7-indicator structural risk card for user visualization
        ç”Ÿæˆ7æŒ‡å¾ç»“æ„é£é™©å¡ç‰‡ç”¨äºç”¨æˆ·å¯è§†åŒ–

        Args:
            result: Complete structure analysis result å®Œæ•´ç»“æ„åˆ†æç»“æœ

        Returns:
            StructuralRiskCard with all 7 indicators åŒ…å«7ä¸ªæŒ‡å¾çš„é£é™©å¡ç‰‡
        """
        indicators = []

        # 1. Perfect Symmetry (é€»è¾‘æ¨è¿›å¯¹ç§°)
        # Check if progression is monotonic
        symmetry_triggered = (
            result.progression_analysis and
            result.progression_analysis.progression_type == "monotonic"
        )
        config = self.INDICATOR_CONFIG["symmetry"]
        indicators.append(StructuralIndicator(
            id="symmetry",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=symmetry_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if symmetry_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details=f"Forward transitions: {result.progression_analysis.forward_transitions}" if result.progression_analysis else "",
            details_zh=f"å•å‘æ¨è¿›: {result.progression_analysis.forward_transitions}æ¬¡" if result.progression_analysis else ""
        ))

        # 2. Uniform Paragraph Function (æ®µè½åŠŸèƒ½å‡åŒ€)
        uniform_triggered = (
            result.function_distribution and
            result.function_distribution.distribution_type == "uniform"
        )
        config = self.INDICATOR_CONFIG["uniform_function"]
        indicators.append(StructuralIndicator(
            id="uniform_function",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=uniform_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if uniform_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details=f"Depth variance: {result.function_distribution.depth_variance:.1f}" if result.function_distribution else "",
            details_zh=f"æ·±åº¦æ–¹å·®: {result.function_distribution.depth_variance:.1f}" if result.function_distribution else ""
        ))

        # 3. Over-signaled Transitions (è¿æ¥è¯ä¾èµ–)
        connector_triggered = (
            result.lexical_echo_analysis and
            result.lexical_echo_analysis.explicit_connector_transitions >= 3
        )
        config = self.INDICATOR_CONFIG["explicit_connectors"]
        indicators.append(StructuralIndicator(
            id="explicit_connectors",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=connector_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if connector_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details=f"Explicit connectors: {result.lexical_echo_analysis.explicit_connector_transitions}" if result.lexical_echo_analysis else "",
            details_zh=f"æ˜¾æ€§è¿æ¥è¯: {result.lexical_echo_analysis.explicit_connector_transitions}ä¸ª" if result.lexical_echo_analysis else ""
        ))

        # 4. Linear Enumeration (å•ä¸€çº¿æ€§æ¨è¿›)
        linear_triggered = result.has_linear_flow
        config = self.INDICATOR_CONFIG["linear_progression"]
        indicators.append(StructuralIndicator(
            id="linear_progression",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=linear_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if linear_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details="First/Second/Third pattern detected" if linear_triggered else "",
            details_zh="æ£€æµ‹åˆ°é¦–å…ˆ/å…¶æ¬¡/æœ€åæ¨¡å¼" if linear_triggered else ""
        ))

        # 5. Rhythmic Regularity (æ®µè½èŠ‚å¥å‡è¡¡)
        rhythm_triggered = result.has_uniform_length
        config = self.INDICATOR_CONFIG["rhythmic_regularity"]
        indicators.append(StructuralIndicator(
            id="rhythmic_regularity",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=rhythm_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if rhythm_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details=f"Avg length: {result.avg_paragraph_length:.0f} words" if result.avg_paragraph_length else "",
            details_zh=f"å¹³å‡é•¿åº¦: {result.avg_paragraph_length:.0f}è¯" if result.avg_paragraph_length else ""
        ))

        # 6. Over-conclusive Ending (ç»“å°¾è¿‡åº¦é—­åˆ)
        conclusive_triggered = (
            result.closure_analysis and
            result.closure_analysis.closure_type == "strong"
        )
        config = self.INDICATOR_CONFIG["over_conclusive"]
        indicators.append(StructuralIndicator(
            id="over_conclusive",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=conclusive_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if conclusive_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details="Formulaic conclusion detected" if (result.closure_analysis and result.closure_analysis.has_formulaic_ending) else "",
            details_zh="æ£€æµ‹åˆ°å…¬å¼åŒ–ç»“è®º" if (result.closure_analysis and result.closure_analysis.has_formulaic_ending) else ""
        ))

        # 7. No Cross-References (ç¼ºä¹å›æŒ‡ç»“æ„)
        no_crossref_triggered = (
            result.cross_reference_analysis and
            not result.cross_reference_analysis.has_cross_references
        )
        config = self.INDICATOR_CONFIG["no_cross_reference"]
        indicators.append(StructuralIndicator(
            id="no_cross_reference",
            name=config["name"],
            name_zh=config["name_zh"],
            triggered=no_crossref_triggered,
            risk_level=config["risk_level"],
            emoji=config["emoji"],
            color=config["color_triggered"] if no_crossref_triggered else config["color_safe"],
            description=config["description"],
            description_zh=config["description_zh"],
            details=f"Cross-refs found: {result.cross_reference_analysis.cross_reference_count}" if result.cross_reference_analysis else "",
            details_zh=f"äº¤å‰å¼•ç”¨: {result.cross_reference_analysis.cross_reference_count}ä¸ª" if result.cross_reference_analysis else ""
        ))

        # Count triggered indicators
        # ç»Ÿè®¡è§¦å‘çš„æŒ‡å¾æ•°é‡
        triggered_count = sum(1 for ind in indicators if ind.triggered)

        # Determine overall risk
        # ç¡®å®šæ•´ä½“é£é™©
        high_risk_triggered = sum(1 for ind in indicators if ind.triggered and ind.risk_level == 3)

        if triggered_count >= 4 or high_risk_triggered >= 2:
            overall_risk = "high"
            overall_risk_zh = "é«˜é£é™©"
            summary = f"ğŸš¨ {triggered_count}/7 AI structural indicators triggered - significant revision needed"
            summary_zh = f"ğŸš¨ è§¦å‘ {triggered_count}/7 é¡¹AIç»“æ„æŒ‡å¾ - éœ€è¦æ˜¾è‘—ä¿®æ”¹"
        elif triggered_count >= 2:
            overall_risk = "medium"
            overall_risk_zh = "ä¸­é£é™©"
            summary = f"âš ï¸ {triggered_count}/7 AI structural indicators triggered - some revision recommended"
            summary_zh = f"âš ï¸ è§¦å‘ {triggered_count}/7 é¡¹AIç»“æ„æŒ‡å¾ - å»ºè®®éƒ¨åˆ†ä¿®æ”¹"
        else:
            overall_risk = "low"
            overall_risk_zh = "ä½é£é™©"
            summary = f"âœ… {triggered_count}/7 AI structural indicators - structure appears natural"
            summary_zh = f"âœ… ä»…è§¦å‘ {triggered_count}/7 é¡¹AIç»“æ„æŒ‡å¾ - ç»“æ„çœ‹èµ·æ¥è‡ªç„¶"

        return StructuralRiskCard(
            indicators=indicators,
            triggered_count=triggered_count,
            overall_risk=overall_risk,
            overall_risk_zh=overall_risk_zh,
            summary=summary,
            summary_zh=summary_zh,
            total_score=result.structure_score
        )
